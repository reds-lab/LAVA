{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\utils.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import lava\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as n\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('imbalanced_train.pkl', 'rb') as f:\n",
    "    X_train_imbalanced, y_train_imbalanced = pickle.load(f)\n",
    "with open('imbalanced_dev.pkl', 'rb') as f:\n",
    "    X_dev_imbalanced, y_dev_imbalanced = pickle.load(f)\n",
    "with open('imbalanced_test.pkl', 'rb') as f:\n",
    "    X_test_imbalanced, y_test_imbalanced = pickle.load(f)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28) (1000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_imbalanced.shape, y_train_imbalanced.shape)\n",
    "print(type(X_train_imbalanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 28, 28]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Chuyển dữ liệu NumPy thành tensor\n",
    "X_tensor = torch.tensor(X_train_imbalanced, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train_imbalanced, dtype=torch.long)\n",
    "\n",
    "# Tạo DataLoader với batch size mong muốn\n",
    "batch_size = 8\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = {}\n",
    "dataloader['train'] = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Duyệt qua từng batch\n",
    "for X_batch, y_batch in dataloader['train']:\n",
    "    print(X_batch.shape, y_batch.shape)\n",
    "X_tensor = torch.tensor(X_test_imbalanced, dtype= torch.float32)\n",
    "y_tensor = torch.tensor(y_test_imbalanced, dtype = torch.long)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader['test'] = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "for X_batch, y_batch in dataloader['test']:\n",
    "    print(X_batch.shape, y_batch.shape)\n",
    "#print(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "\n",
    "# Định nghĩa mô hình PreActResNet18 như đã thực hiện trước đó\n",
    "class PreActBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def PreActResNet18():\n",
    "    return PreActResNet(PreActBlock, [2,2,2,2])\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net_test = PreActResNet18().to(device)\n",
    "feature_extractor_name = 'preact_resnet18_test_mnist.pth'\n",
    "net_test.load_state_dict(torch.load('checkpoint/' + feature_extractor_name, map_location=torch.device('cpu')))\n",
    "net_test.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = net_test.to(device)\n",
    "embedder.fc = torch.nn.Identity()\n",
    "for p in embedder.parameters():\n",
    "    p.requires_grad = False\n",
    "embedder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.0\n",
      "2.3.0\n",
      "Cuda device:  0\n",
      "cude devices:  1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda_num = 0\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(cuda_num)\n",
    "#print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "#torch.cuda.set_device(cuda_num)\n",
    "print(\"Cuda device: \", torch.cuda.current_device())\n",
    "print(\"cude devices: \", torch.cuda.device_count())\n",
    "device = torch.device('cuda:' + str(cuda_num) if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "training_size = 1000\n",
    "valid_size = 200\n",
    "resize = 32\n",
    "portion = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transfer to batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from otdd.pytorch.distance_fast import DatasetDistance, FeatureCost, batch_augmented_cost\n",
    "from otdd.pytorch.wasserstein import pwdist_exact\n",
    "from functools import partial\n",
    "from lava import train_with_corrupt_flag, get_indices, values, sort_and_keep_indices\n",
    "resize = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cost = FeatureCost(src_embedding = embedder,\n",
    "                               src_dim = (1, resize,resize),\n",
    "                               tgt_embedding = embedder,\n",
    "                               tgt_dim = (1, resize,resize),\n",
    "                               p = 2,\n",
    "                               device='cuda')\n",
    "dist = DatasetDistance(dataloader['train'], dataloader['test'],\n",
    "                           inner_ot_method = 'exact',\n",
    "                           debiased_loss = True,\n",
    "                           feature_cost = feature_cost,\n",
    "                           λ_x=1.0, λ_y=1.0,\n",
    "                           sqrt_method = 'spectral',\n",
    "                           sqrt_niters=10,\n",
    "                           precision='single',\n",
    "                           p = 2, entreg = 1e-1,\n",
    "                           device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d4f1664dcd4a6b843dcf02815a9bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load full dataset: torch.Size([1000])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af5b2428fa744af8a04e6edbd43c01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load full dataset: torch.Size([4000])\n",
      "2 2\n",
      "[(0, 1)]\n",
      "cost function:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b4fc0af0b141cc9c11060284939b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "huhu: torch.Size([833, 784]) torch.Size([167, 784])\n",
      "1 833 784\n",
      "torch.Size([1, 833, 100])\n",
      "1 167 784\n",
      "torch.Size([1, 167, 100])\n",
      "torch.Size([1, 833, 167])\n",
      "1 167 784\n",
      "torch.Size([1, 167, 100])\n",
      "1 833 784\n",
      "torch.Size([1, 833, 100])\n",
      "torch.Size([1, 167, 833])\n",
      "was: tensor(7.9273e+09, device='cuda:0')\n",
      "2 2\n",
      "[(0, 1)]\n",
      "cost function:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7725035375d84915b1eb6c195ffdb8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "huhu: torch.Size([3334, 784]) torch.Size([666, 784])\n",
      "1 3334 784\n",
      "torch.Size([1, 3334, 100])\n",
      "1 666 784\n",
      "torch.Size([1, 666, 100])\n",
      "torch.Size([1, 3334, 666])\n",
      "1 666 784\n",
      "torch.Size([1, 666, 100])\n",
      "1 3334 784\n",
      "torch.Size([1, 3334, 100])\n",
      "torch.Size([1, 666, 3334])\n",
      "was: tensor(1.2678e+09, device='cuda:0')\n",
      "2 2\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "cost function:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebede6b8ce6247db856d32de32e417a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "huhu: torch.Size([833, 784]) torch.Size([3334, 784])\n",
      "1 833 784\n",
      "torch.Size([1, 833, 100])\n",
      "1 3334 784\n",
      "torch.Size([1, 3334, 100])\n",
      "torch.Size([1, 833, 3334])\n",
      "1 3334 784\n",
      "torch.Size([1, 3334, 100])\n",
      "1 833 784\n",
      "torch.Size([1, 833, 100])\n",
      "torch.Size([1, 3334, 833])\n",
      "was: tensor(2.5219e+09, device='cuda:0')\n",
      "0 1\n",
      "huhu: torch.Size([833, 784]) torch.Size([666, 784])\n",
      "1 833 784\n",
      "torch.Size([1, 833, 100])\n",
      "1 666 784\n",
      "torch.Size([1, 666, 100])\n",
      "torch.Size([1, 833, 666])\n",
      "1 666 784\n",
      "torch.Size([1, 666, 100])\n",
      "1 833 784\n",
      "torch.Size([1, 833, 100])\n",
      "torch.Size([1, 666, 833])\n",
      "was: tensor(4.2460e+09, device='cuda:0')\n",
      "1 0\n",
      "huhu: torch.Size([167, 784]) torch.Size([3334, 784])\n",
      "1 167 784\n",
      "torch.Size([1, 167, 100])\n",
      "1 3334 784\n",
      "torch.Size([1, 3334, 100])\n",
      "torch.Size([1, 167, 3334])\n",
      "1 3334 784\n",
      "torch.Size([1, 3334, 100])\n",
      "1 167 784\n",
      "torch.Size([1, 167, 100])\n",
      "torch.Size([1, 3334, 167])\n",
      "was: tensor(1.3821e+10, device='cuda:0')\n",
      "1 1\n",
      "huhu: torch.Size([167, 784]) torch.Size([666, 784])\n",
      "1 167 784\n",
      "torch.Size([1, 167, 100])\n",
      "1 666 784\n",
      "torch.Size([1, 666, 100])\n",
      "torch.Size([1, 167, 666])\n",
      "1 666 784\n",
      "torch.Size([1, 666, 100])\n",
      "1 167 784\n",
      "torch.Size([1, 167, 100])\n",
      "torch.Size([1, 666, 167])\n",
      "was: tensor(1.1660e+10, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "W = dist._get_label_distances().to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geomloss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_geomloss = partial(\n",
    "                batch_augmented_cost,\n",
    "                W=W,\n",
    "                λ_x=dist.λ_x,\n",
    "                λ_y=dist.λ_y,\n",
    "                feature_cost=dist.feature_cost\n",
    "            )\n",
    "\n",
    "loss = geomloss.SamplesLoss(\n",
    "                loss=dist.loss, p=dist.p,\n",
    "                cost=cost_geomloss,\n",
    "                debias=dist.debiased_loss,\n",
    "                blur=dist.entreg**(1 / dist.p),\n",
    "                backend='tensorized'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsamples = None\n",
    "if maxsamples and dist.X1.shape[0] > maxsamples:\n",
    "    idxs_1 = sorted(np.random.choice(\n",
    "    dist.X1.shape[0], maxsamples, replace=False))\n",
    "else:\n",
    "    idxs_1 = np.s_[:]  # hack to get a full slice\n",
    "\n",
    "if maxsamples and dist.X2.shape[0] > maxsamples:\n",
    "    idxs_2 = sorted(np.random.choice(\n",
    "    dist.X2.shape[0], maxsamples, replace=False))\n",
    "else:\n",
    "    idxs_2 = np.s_[:]  # hack to get a full slice\n",
    "Z1 = torch.cat((dist.X1[idxs_1],\n",
    "                dist.Y1[idxs_1].type(dist.X1.dtype).unsqueeze(1)), -1)\n",
    "Z2 = torch.cat((dist.X2[idxs_2],\n",
    "                dist.Y2[idxs_2].type(dist.X2.dtype).unsqueeze(1)), -1)\n",
    "Z1 = Z1.to(device)\n",
    "Z2 = Z2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 785]) torch.Size([4000, 785])\n"
     ]
    }
   ],
   "source": [
    "print(Z1.shape, Z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 785]) torch.Size([4000, 785])\n",
      "Z1 shape in batch:  torch.Size([1, 1000, 785])\n",
      "Z2 shape in batch:  torch.Size([1, 4000, 785])\n",
      "1 1000 784\n",
      "torch.Size([1, 1000, 100])\n",
      "1 4000 784\n",
      "torch.Size([1, 4000, 100])\n",
      "torch.Size([1, 1000, 4000])\n",
      "torch.Size([1, 1000, 4000])\n",
      "Gia tri M: tensor([[[3, 2, 2,  ..., 2, 2, 2],\n",
      "         [7, 6, 6,  ..., 6, 6, 6],\n",
      "         [7, 6, 6,  ..., 6, 6, 6],\n",
      "         ...,\n",
      "         [7, 6, 6,  ..., 6, 6, 6],\n",
      "         [3, 2, 2,  ..., 2, 2, 2],\n",
      "         [3, 2, 2,  ..., 2, 2, 2]]], device='cuda:0')\n",
      "torch.Size([1, 1000, 4000])\n",
      "torch.Size([1, 1000, 4000])\n",
      "gia tri D: tensor([[[6.1884e+11, 4.5241e+12, 8.3257e+10,  ..., 1.4598e+12,\n",
      "          2.0739e+12, 1.4346e+11],\n",
      "         [6.6336e+10, 1.2097e+12, 1.7407e+12,  ..., 3.9091e+10,\n",
      "          1.7530e+11, 4.3406e+11],\n",
      "         [1.2148e+12, 6.5539e+10, 4.7208e+12,  ..., 4.6557e+11,\n",
      "          2.0537e+11, 2.2803e+12],\n",
      "         ...,\n",
      "         [1.5724e+12, 1.5813e+10, 5.4037e+12,  ..., 6.9498e+11,\n",
      "          3.6443e+11, 2.7618e+12],\n",
      "         [4.8911e+11, 4.1545e+11, 3.1323e+12,  ..., 7.7165e+10,\n",
      "          3.2155e+09, 1.2247e+12],\n",
      "         [4.4865e+12, 6.0413e+11, 1.0172e+13,  ..., 2.8753e+12,\n",
      "          2.1430e+12, 6.3812e+12]]], device='cuda:0')\n",
      "torch.Size([1, 1000, 4000])\n",
      "Z1 shape in batch:  torch.Size([1, 4000, 785])\n",
      "Z2 shape in batch:  torch.Size([1, 1000, 785])\n",
      "1 4000 784\n",
      "torch.Size([1, 4000, 100])\n",
      "1 1000 784\n",
      "torch.Size([1, 1000, 100])\n",
      "torch.Size([1, 4000, 1000])\n",
      "torch.Size([1, 4000, 1000])\n",
      "Gia tri M: tensor([[[12, 13, 13,  ..., 13, 12, 12],\n",
      "         [ 8,  9,  9,  ...,  9,  8,  8],\n",
      "         [ 8,  9,  9,  ...,  9,  8,  8],\n",
      "         ...,\n",
      "         [ 8,  9,  9,  ...,  9,  8,  8],\n",
      "         [ 8,  9,  9,  ...,  9,  8,  8],\n",
      "         [ 8,  9,  9,  ...,  9,  8,  8]]], device='cuda:0')\n",
      "torch.Size([1, 4000, 1000])\n",
      "torch.Size([1, 4000, 1000])\n",
      "gia tri D: tensor([[[6.1884e+11, 6.6336e+10, 1.2148e+12,  ..., 1.5724e+12,\n",
      "          4.8911e+11, 4.4865e+12],\n",
      "         [4.5241e+12, 1.2097e+12, 6.5539e+10,  ..., 1.5813e+10,\n",
      "          4.1545e+11, 6.0413e+11],\n",
      "         [8.3258e+10, 1.7407e+12, 4.7208e+12,  ..., 5.4037e+12,\n",
      "          3.1323e+12, 1.0172e+13],\n",
      "         ...,\n",
      "         [1.4598e+12, 3.9091e+10, 4.6557e+11,  ..., 6.9498e+11,\n",
      "          7.7165e+10, 2.8753e+12],\n",
      "         [2.0739e+12, 1.7530e+11, 2.0537e+11,  ..., 3.6443e+11,\n",
      "          3.2155e+09, 2.1430e+12],\n",
      "         [1.4346e+11, 4.3406e+11, 2.2803e+12,  ..., 2.7618e+12,\n",
      "          1.2247e+12, 6.3812e+12]]], device='cuda:0')\n",
      "torch.Size([1, 4000, 1000])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss.debias = False\n",
    "    loss.potentials = True\n",
    "    print(Z1.shape, Z2.shape)\n",
    "    F_i, G_j = loss(Z1, Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "π = [F_i, G_j]\n",
    "dual_sol = π\n",
    "for i in range(len(dual_sol)):\n",
    "    dual_sol[i] = dual_sol[i].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_gradient = values(dual_sol, training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('calibrated_gradient.pkl', 'wb') as f:\n",
    "    pickle.dump(calibrated_gradient, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('calibrated_gradient.pkl', 'rb') as f:\n",
    "    loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-787359700.0, 3658300000.0, 3414521900.0, -761628860.0, 3264091600.0, -787799100.0, -799351500.0, -777636860.0, -670477000.0, -778479740.0, -766360770.0, -694445500.0, -773709600.0, -771062900.0, -777780000.0, -697117700.0, 2670000000.0, -768481000.0, 3159559700.0, -721409700.0, 3570951700.0, -774206140.0, -741824260.0, -734327100.0, -734817300.0, -789919040.0, -749694140.0, -690686140.0, -741808060.0, -777462140.0, -720458100.0, 3955789300.0, -720562940.0, -713150600.0, 3360567800.0, -726425800.0, -761662400.0, -774631940.0, -782463400.0, 3692124200.0, -790641300.0, -755597250.0, -775275900.0, -768129600.0, -748097660.0, 2961792000.0, -723380000.0, 4443694000.0, -792601600.0, -742540000.0, -724486100.0, -788722900.0, -476425100.0, -713548740.0, -740720300.0, -737659100.0, -724926200.0, 3719465000.0, -778126140.0, -762900900.0, -769574600.0, -802372500.0, -805926600.0, -816397100.0, -794836700.0, -738564900.0, -803942000.0, -769908900.0, -741420200.0, -737920960.0, -761706240.0, -759030850.0, -786805440.0, -756763840.0, -791396100.0, -773516350.0, -746544900.0, -759255700.0, -722296640.0, -768681300.0, -771222600.0, -814797800.0, 2917576200.0, -778363970.0, 3436430300.0, -787790200.0, -789222660.0, -804739260.0, -766082750.0, -811085440.0, -775014500.0, 3204250000.0, 3249477600.0, -603555840.0, -787188160.0, -751149000.0, -726633600.0, -774776770.0, -776355300.0, -668185400.0, -695152800.0, -796312260.0, -748100500.0, 3356590000.0, -695138600.0, 3398654000.0, 3394100200.0, -822337660.0, -762170100.0, -800497540.0, -780250900.0, 3409910800.0, 3125612000.0, 2102406800.0, -795490560.0, -760655000.0, -791026500.0, -775446340.0, 3358762000.0, -767640260.0, 2926049300.0, -769811900.0, -785371500.0, -757402300.0, -739285760.0, -771198700.0, -767744400.0, -779869440.0, 3019980300.0, -744033660.0, -762641340.0, -795135400.0, 3203787300.0, -762056450.0, -800607100.0, -749294700.0, -798862000.0, -741501250.0, -785978500.0, -777791000.0, -758481800.0, -785873900.0, -713235900.0, -741267650.0, -770043900.0, -733734700.0, 3845601300.0, -779973000.0, 3172292000.0, 3470401500.0, -746640600.0, 3167988700.0, -775923100.0, -711902460.0, -738577200.0, -800678340.0, -783505500.0, 469208960.0, 3388788700.0, -760174400.0, -737957950.0, -785780000.0, -703334140.0, 3533981700.0, -785152400.0, -781162940.0, -733231550.0, -757094660.0, -793743800.0, -792304500.0, -791885250.0, -778032600.0, -786010750.0, -827333600.0, -809626200.0, -763983740.0, -766007700.0, -809264960.0, 3372189200.0, -783462500.0, -816558000.0, -768052030.0, 3363842000.0, 583064600.0, -816097000.0, -812505200.0, 3281921500.0, -784388540.0, -772710500.0, 3207329800.0, -767506370.0, -610967100.0, 3207731700.0, -766162800.0, -804511800.0, -769145400.0, 3205061600.0, -797490800.0, -767388100.0, -781967940.0, -810055200.0, -748630400.0, -789771200.0, 3214306800.0, -740088770.0, -761012600.0, 2783895000.0, -774420700.0, -789345000.0, -809184400.0, -708009660.0, -772730050.0, -763615400.0, -790154430.0, -620266000.0, -714880830.0, -770231600.0, 3341227500.0, 3234430000.0, -714173100.0, -795511500.0, -733877600.0, -787657600.0, -752637100.0, -786157700.0, -707612500.0, -753242000.0, -784294700.0, -767343000.0, -776754200.0, -752152640.0, -747148500.0, -798551200.0, -796140540.0, -764117500.0, -806536500.0, -749119200.0, -677619900.0, -766573950.0, -709444100.0, -777187650.0, -804929100.0, -677838000.0, -779561660.0, -810288450.0, -773969540.0, -707748300.0, -601450430.0, -774158000.0, 3005683200.0, -726332800.0, 4033518000.0, -795642800.0, -741373900.0, -759996700.0, -794123300.0, -749822800.0, 3064079400.0, 3111690800.0, -675506200.0, -495457660.0, -777631940.0, -779382700.0, -690646700.0, -791014600.0, -761498300.0, -790799800.0, -789378240.0, -758607600.0, -734285060.0, -766725900.0, 3407102500.0, -819550340.0, -781618300.0, -751145540.0, -792030600.0, -764887800.0, -785344400.0, -770173900.0, -734691200.0, -755942400.0, -786970050.0, -783872960.0, -767758800.0, 2984214000.0, -766299100.0, -764262600.0, -772730700.0, -784814400.0, -804950700.0, 3374380000.0, -806371400.0, -773415550.0, 3466176000.0, 3481033700.0, -811053760.0, -781009660.0, 3316967400.0, -743001600.0, -747240200.0, -767646900.0, -784332400.0, -716145300.0, -742686800.0, -760593340.0, -767762900.0, -783814460.0, 3412202000.0, -783491140.0, -773577340.0, -785525800.0, -769810000.0, -751548540.0, -628690560.0, 2786166300.0, -739922700.0, -760519940.0, -782976700.0, -771426100.0, -723411700.0, -777519040.0, -643399500.0, -798318600.0, -799059800.0, -764537900.0, 3364233700.0, -776606140.0, -798844740.0, -767700300.0, -749618800.0, 3335825400.0, 3281943600.0, -754301400.0, 3460058600.0, -800644700.0, 3338201600.0, -777931140.0, -717837400.0, -748042300.0, -721159740.0, -763386940.0, 3351648300.0, -751022400.0, -729053060.0, -787836100.0, -233768960.0, -756961540.0, -771731000.0, -786014850.0, -738729860.0, -794808700.0, -773906370.0, -766673400.0, -556486340.0, -747905860.0, -790911800.0, -782524300.0, -745749950.0, -799583740.0, -793348030.0, -769783360.0, -809692700.0, 3190721500.0, -728146940.0, -737553340.0, -763954940.0, -630119500.0, -787927040.0, -734922500.0, -789348100.0, -757146300.0, -790161500.0, -793091260.0, -731241300.0, -766226100.0, 2750578700.0, -758229760.0, -761041900.0, -770021250.0, -782379970.0, -751012800.0, -780222340.0, -769589250.0, -772904770.0, -761910400.0, -744832200.0, -794257100.0, -728107300.0, -784072450.0, -808745540.0, -766172000.0, -722933200.0, -742125300.0, 3521223700.0, -794760900.0, -781894900.0, 3416678400.0, -776853950.0, -769997300.0, -748232700.0, -799779260.0, -735059300.0, -729633500.0, -627110100.0, -788612030.0, -764064200.0, -758603650.0, -769202400.0, -764085000.0, -747751800.0, -776163700.0, -653762700.0, -789831300.0, -790608640.0, -808080450.0, 2946709500.0, -732840500.0, -684897150.0, -762108860.0, -687687300.0, -760193700.0, -755622340.0, -753416900.0, -796462300.0, 3463348200.0, -713287550.0, -734310800.0, -779357950.0, 2634532900.0, -765715100.0, 122052480.0, 3412327400.0, 69672180000.0, -776252400.0, -766967230.0, -720294850.0, -628076160.0, -789848200.0, -762756900.0, -793098940.0, -775504000.0, -811814100.0, 3252923000.0, -753781950.0, 3296692700.0, -765369100.0, 3901150200.0, -780102660.0, -601797100.0, -691885250.0, 3209568300.0, -776703800.0, -793260860.0, -768136770.0, -746152770.0, -687124400.0, -794086300.0, -782395300.0, -748316740.0, -815068160.0, -794713860.0, 3511636000.0, 3369763300.0, -777628600.0, -738607800.0, -757673400.0, -754132600.0, 3338899500.0, -830580030.0, -761182800.0, -749472450.0, -693929150.0, -775314750.0, -750280800.0, -749749400.0, -740619840.0, -783499460.0, 3584875000.0, 3629163000.0, -794204900.0, -653158460.0, -752769100.0, -746015900.0, -745269600.0, -684810940.0, 3468031000.0, -729811400.0, -785331300.0, -721960400.0, -760213950.0, -790740100.0, -756772900.0, -805308900.0, -751961100.0, -799707500.0, -742697400.0, -785249300.0, -794573600.0, -784661200.0, -784054900.0, 3693731300.0, -800496500.0, -793856260.0, -630923200.0, -718876860.0, 3103576600.0, -649227900.0, -784249800.0, -794297700.0, -735385540.0, -780581570.0, 3355611100.0, -814375900.0, -762177800.0, -794555260.0, -797891840.0, -770163140.0, -763796350.0, -801660600.0, 3462867500.0, -767750340.0, 3236338200.0, -739378050.0, -662883140.0, -766892200.0, -730766800.0, -802445100.0, -784833800.0, -794095500.0, -723189600.0, -768504770.0, -772138050.0, -765241700.0, -787394050.0, -712206100.0, -750494900.0, -749122940.0, -713537150.0, -776630400.0, -682469300.0, -750219600.0, -695611900.0, -784993000.0, -763495300.0, -793074240.0, -703204600.0, -798876740.0, 3151817700.0, -773084160.0, 3176090600.0, -764917000.0, -779100600.0, -806232200.0, -737166500.0, 2606588400.0, -783087000.0, -779106940.0, -762313500.0, -803242800.0, -665074900.0, 3630165000.0, -728130300.0, -796392200.0, -601053950.0, -776850370.0, -798946560.0, -768052400.0, -730623300.0, -711024000.0, -765403260.0, -760491100.0, -782927900.0, -699635140.0, -798237250.0, 3133651000.0, -801919040.0, -701924500.0, 3141167000.0, -750650700.0, -646487700.0, -744309300.0, -766098900.0, -698737660.0, -762163700.0, -635191300.0, -808477500.0, -785499400.0, -765840700.0, -777770560.0, -779253060.0, -763160640.0, -781173300.0, 3197474300.0, -755671940.0, -713631040.0, 3218118100.0, 3362132000.0, 3480057300.0, -778464500.0, -762353400.0, -764225400.0, -782915460.0, -702299650.0, -771535550.0, -660409700.0, 3597165600.0, -799857700.0, -782984260.0, -795406400.0, -771817200.0, -795112960.0, -712181200.0, 3464757800.0, -742500740.0, -771986900.0, -811167400.0, -763349760.0, -771394700.0, -802726600.0, 3458549800.0, -658577100.0, -783299460.0, -759677630.0, -775203800.0, -726570700.0, 3182888000.0, -746408900.0, 3366912500.0, -670285600.0, -763449340.0, -801789400.0, -776754050.0, -804271800.0, -756715900.0, -758302300.0, 3576539600.0, -787813500.0, -786499460.0, -786069570.0, -735085200.0, -775917300.0, -719037600.0, -797958700.0, -761517100.0, 2930614300.0, -735057400.0, -811641800.0, -690137700.0, -715567940.0, 3012814300.0, -778959800.0, -779867400.0, -753965400.0, -727270900.0, -706960600.0, -715314750.0, -740813200.0, -770734340.0, -771386900.0, -734228030.0, 2997066800.0, -770649100.0, -781748350.0, -786283400.0, -715471500.0, -586971140.0, -730052300.0, -653110400.0, -751588350.0, -746242500.0, -818856640.0, -787678340.0, -697275900.0, -790514940.0, -764325300.0, -779700200.0, -713285250.0, -732799900.0, -752271940.0, -746040260.0, 3711263200.0, -773801300.0, -619881700.0, -791002400.0, -774266800.0, 3843897300.0, -781095230.0, -756357060.0, -745390900.0, -791032100.0, -734482000.0, -725273900.0, -780471700.0, -499381570.0, -799480000.0, -799408600.0, -729357000.0, -673492030.0, -767781400.0, -742977540.0, -785896960.0, -776892860.0, -673278500.0, -764835400.0, 3089685000.0, -783735000.0, 3603912200.0, -798654850.0, -807194430.0, -718154900.0, -791730940.0, 3409813500.0, -633524740.0, -720069000.0, -756572300.0, 2525629400.0, 3284020700.0, -753256200.0, -783176300.0, 3411553300.0, -769951400.0, -772119500.0, -803140350.0, -746059650.0, -745125760.0, -320520200.0, -736806300.0, 3967600000.0, -736976260.0, 3316149200.0, -774824300.0, -785079550.0, -728556900.0, -799627600.0, -769708740.0, -798591170.0, -780362750.0, 2910360600.0, -763062300.0, -717082430.0, -751904700.0, -716079700.0, -813714370.0, -797642900.0, -754210200.0, -711639040.0, -768609150.0, 3767735800.0, -773858200.0, -777911900.0, 3075124700.0, 3071200800.0, -795336260.0, -737164500.0, -731844000.0, -758744600.0, 3435028000.0, -714037600.0, 2387837400.0, 3193111600.0, -775648640.0, 3381067800.0, 4644369000.0, -794228800.0, -739821700.0, -775057340.0, -731925500.0, -808966100.0, -806118400.0, -766507200.0, -782444700.0, -750515840.0, -755034700.0, -743473600.0, -796890240.0, -639111100.0, 1542991500.0, -788517570.0, -781795600.0, -796887550.0, -790788000.0, 3806365200.0, -781641200.0, -740913800.0, 3250834400.0, -752935740.0, -504565570.0, -773233000.0, -788815100.0, -814723900.0, -637966300.0, -759826500.0, -725092500.0, -763498500.0, 3268506600.0, -768479940.0, -809098800.0, -765564200.0, -786814400.0, -810046100.0, -754673340.0, 3467080700.0, -750972100.0, 3108231700.0, -754895200.0, -791734800.0, -762025300.0, -735766340.0, 3236222000.0, -775347460.0, -742843600.0, -761550600.0, -772784060.0, -785089100.0, -686981440.0, -792085950.0, -807429440.0, -787335600.0, -700722800.0, -714598400.0, -761768800.0, 3128989700.0, -769798100.0, -802648060.0, -780654200.0, 1645795700.0, -789858240.0, -761389630.0, -759684540.0, -775882900.0, -778593340.0, 4280734700.0, -755847230.0, -748347800.0, -699001100.0, 3462027800.0, 3374563300.0, -789734660.0, -821355840.0, -585160300.0, 3391676000.0, -764010900.0, -791578050.0, -744262800.0, -773533100.0, -645852540.0, -758789200.0, -772553860.0, -792068300.0, -803699650.0, -741688450.0, 2910360600.0, -758267900.0, 3594157000.0, -744397700.0, -772379700.0, -781171400.0, -722459460.0, -759161150.0, -778931600.0, -736004800.0, -797484740.0, -209095680.0, -542500400.0, -775238400.0, -753518460.0, 3502064600.0, 3633917000.0, -765492860.0, -800881540.0, 3431393800.0, 3598826500.0, -786964030.0, -772524860.0, -786941760.0, 3391915500.0, -773058940.0, -793895230.0, 3485116000.0, -669029000.0, -719075200.0, 3340158000.0, 3397297200.0, -744358300.0, 2690558500.0, 3364120600.0, -785707840.0, -703828540.0, -687792960.0, -784414500.0, -791072450.0, 3434736000.0, -757821950.0, -664891800.0, 3349571600.0, -761962500.0, -634457340.0, 3591550500.0, -784689700.0, 3401236000.0, -720094100.0, -774062800.0, -763613300.0, -691229630.0, -768489100.0, 3139972000.0, -792325440.0, -776964540.0, -747465200.0, -771522560.0, -774430140.0, 3331874300.0, -773588600.0, -788004300.0, -657228700.0, 3297213400.0, -741595800.0, -733799600.0, -686103100.0, 3295691300.0, 3525328000.0, 3379195400.0, -739151600.0, -794746750.0, 2896210000.0, -773379000.0, 3119820800.0, 3369477600.0, -755491260.0, -762349300.0, -806311100.0, -791964740.0, 2412174300.0, -604918800.0, -585776060.0, 3217965600.0, -784737400.0, -752619800.0, -664709700.0, -730630100.0, -769878500.0, -703661100.0, -775096960.0, -758645900.0, -753918800.0, -743835100.0, -727433800.0, -788348860.0, -575917900.0, -786001540.0, -767915000.0, -789927230.0, -777224900.0, -793435650.0, 3388377000.0, 3458445800.0, 3142306800.0, 3488222200.0, -768346750.0, -738638700.0, -789971100.0, -761866940.0, -801682240.0, -558078000.0, -762106940.0, -779215360.0, -744430600.0, -759799600.0, -773319300.0, -808841150.0, -767605700.0, -769629950.0, -784393660.0, -763801660.0, -747908100.0, -797578050.0, -775320000.0, -795814850.0, -781433800.0, -743918700.0, 3306087000.0, -787538240.0, 3155844000.0, -801296830.0, -811297300.0, -763881700.0, -778656800.0, -790225660.0, -758682100.0, -752401200.0, -787870500.0, -703431550.0, -789570560.0, -765128300.0, -766432640.0, -769569600.0, -763499140.0, 3445255700.0, -808960450.0, -796493060.0]\n"
     ]
    }
   ],
   "source": [
    "print(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([432], dtype=int64), array([762], dtype=int64), array([47], dtype=int64), array([831], dtype=int64), array([251], dtype=int64), array([727], dtype=int64), array([31], dtype=int64), array([446], dtype=int64), array([146], dtype=int64), array([685], dtype=int64), array([781], dtype=int64), array([747], dtype=int64), array([57], dtype=int64), array([680], dtype=int64), array([500], dtype=int64), array([39], dtype=int64), array([1], dtype=int64), array([867], dtype=int64), array([560], dtype=int64), array([478], dtype=int64), array([706], dtype=int64), array([871], dtype=int64), array([605], dtype=int64), array([853], dtype=int64), array([897], dtype=int64), array([477], dtype=int64), array([635], dtype=int64), array([20], dtype=int64), array([163], dtype=int64), array([920], dtype=int64), array([393], dtype=int64), array([461], dtype=int64), array([866], dtype=int64), array([957], dtype=int64), array([878], dtype=int64), array([294], dtype=int64), array([597], dtype=int64), array([149], dtype=int64), array([485], dtype=int64), array([801], dtype=int64), array([293], dtype=int64), array([612], dtype=int64), array([424], dtype=int64), array([519], dtype=int64), array([835], dtype=int64), array([333], dtype=int64), array([619], dtype=int64), array([955], dtype=int64), array([997], dtype=int64), array([84], dtype=int64), array([756], dtype=int64), array([891], dtype=int64), array([870], dtype=int64), array([396], dtype=int64), array([2], dtype=int64), array([431], dtype=int64), array([307], dtype=int64), array([719], dtype=int64), array([111], dtype=int64), array([711], dtype=int64), array([271], dtype=int64), array([899], dtype=int64), array([105], dtype=int64), array([882], dtype=int64), array([106], dtype=int64), array([875], dtype=int64), array([840], dtype=int64), array([158], dtype=int64), array([954], dtype=int64), array([761], dtype=int64), array([921], dtype=int64), array([836], dtype=int64), array([290], dtype=int64), array([178], dtype=int64), array([462], dtype=int64), array([927], dtype=int64), array([627], dtype=int64), array([325], dtype=int64), array([885], dtype=int64), array([182], dtype=int64), array([596], dtype=int64), array([34], dtype=int64), array([118], dtype=int64), array([103], dtype=int64), array([511], dtype=int64), array([341], dtype=int64), array([894], dtype=int64), array([217], dtype=int64), array([881], dtype=int64), array([467], dtype=int64), array([335], dtype=int64), array([330], dtype=int64), array([911], dtype=int64), array([297], dtype=int64), array([729], dtype=int64), array([980], dtype=int64), array([915], dtype=int64), array([444], dtype=int64), array([919], dtype=int64), array([716], dtype=int64), array([331], dtype=int64), array([186], dtype=int64), array([794], dtype=int64), array([4], dtype=int64), array([442], dtype=int64), array([784], dtype=int64), array([92], dtype=int64), array([521], dtype=int64), array([808], dtype=int64), array([218], dtype=int64), array([595], dtype=int64), array([935], dtype=int64), array([203], dtype=int64), array([450], dtype=int64), array([192], dtype=int64), array([189], dtype=int64), array([196], dtype=int64), array([91], dtype=int64), array([132], dtype=int64), array([592], dtype=int64), array([759], dtype=int64), array([362], dtype=int64), array([625], dtype=int64), array([549], dtype=int64), array([148], dtype=int64), array([151], dtype=int64), array([18], dtype=int64), array([982], dtype=int64), array([547], dtype=int64), array([956], dtype=int64), array([577], dtype=int64), array([905], dtype=int64), array([574], dtype=int64), array([821], dtype=int64), array([112], dtype=int64), array([926], dtype=int64), array([258], dtype=int64), array([803], dtype=int64), array([505], dtype=int64), array([704], dtype=int64), array([750], dtype=int64), array([751], dtype=int64), array([257], dtype=int64), array([128], dtype=int64), array([649], dtype=int64), array([249], dtype=int64), array([660], dtype=int64), array([284], dtype=int64), array([45], dtype=int64), array([415], dtype=int64), array([644], dtype=int64), array([120], dtype=int64), array([82], dtype=int64), array([737, 851], dtype=int64), array([737, 851], dtype=int64), array([924], dtype=int64), array([314], dtype=int64), array([206], dtype=int64), array([375], dtype=int64), array([884], dtype=int64), array([16], dtype=int64), array([428], dtype=int64), array([554], dtype=int64), array([715], dtype=int64), array([932], dtype=int64), array([758], dtype=int64), array([113], dtype=int64), array([825], dtype=int64), array([776], dtype=int64), array([183], dtype=int64), array([157], dtype=int64), array([430], dtype=int64), array([862], dtype=int64), array([345], dtype=int64), array([725], dtype=int64), array([52], dtype=int64), array([260], dtype=int64), array([693], dtype=int64), array([786], dtype=int64), array([863], dtype=int64), array([353], dtype=int64), array([963], dtype=int64), array([948], dtype=int64), array([839], dtype=int64), array([934], dtype=int64), array([665], dtype=int64), array([563], dtype=int64), array([247], dtype=int64), array([448], dtype=int64), array([93], dtype=int64), array([933], dtype=int64), array([191], dtype=int64), array([682], dtype=int64), array([214], dtype=int64), array([403], dtype=int64), array([436], dtype=int64), array([313], dtype=int64), array([366], dtype=int64), array([503], dtype=int64), array([712], dtype=int64), array([896], dtype=int64), array([584], dtype=int64), array([790], dtype=int64), array([775], dtype=int64), array([321], dtype=int64), array([845], dtype=int64), array([579], dtype=int64), array([506], dtype=int64), array([667], dtype=int64), array([480], dtype=int64), array([411], dtype=int64), array([914], dtype=int64), array([620], dtype=int64), array([604], dtype=int64), array([523], dtype=int64), array([938], dtype=int64), array([893], dtype=int64), array([559], dtype=int64), array([99], dtype=int64), array([879], dtype=int64), array([628], dtype=int64), array([8], dtype=int64), array([702], dtype=int64), array([697], dtype=int64), array([259], dtype=int64), array([237], dtype=int64), array([242], dtype=int64), array([539], dtype=int64), array([484], dtype=int64), array([417], dtype=int64), array([918], dtype=int64), array([814], dtype=int64), array([455], dtype=int64), array([419], dtype=int64), array([888], dtype=int64), array([647], dtype=int64), array([263], dtype=int64), array([27], dtype=int64), array([903], dtype=int64), array([449], dtype=int64), array([471], dtype=int64), array([11], dtype=int64), array([104], dtype=int64), array([100], dtype=int64), array([541], dtype=int64), array([15], dtype=int64), array([672], dtype=int64), array([582], dtype=int64), array([834], dtype=int64), array([572], dtype=int64), array([818], dtype=int64), array([576], dtype=int64), array([602], dtype=int64), array([545], dtype=int64), array([162], dtype=int64), array([991], dtype=int64), array([941], dtype=int64), array([887], dtype=int64), array([654], dtype=int64), array([225], dtype=int64), array([246], dtype=int64), array([210], dtype=int64), array([239], dtype=int64), array([568], dtype=int64), array([745], dtype=int64), array([153], dtype=int64), array([611], dtype=int64), array([534], dtype=int64), array([33], dtype=int64), array([142], dtype=int64), array([676], dtype=int64), array([425], dtype=int64), array([537], dtype=int64), array([53], dtype=int64), array([594], dtype=int64), array([757], dtype=int64), array([219], dtype=int64), array([819], dtype=int64), array([215], dtype=int64), array([655], dtype=int64), array([664], dtype=int64), array([648], dtype=int64), array([741], dtype=int64), array([302], dtype=int64), array([739], dtype=int64), array([337], dtype=int64), array([709], dtype=int64), array([504], dtype=int64), array([641], dtype=int64), array([880], dtype=int64), array([713], dtype=int64), array([900], dtype=int64), array([435], dtype=int64), array([30], dtype=int64), array([32], dtype=int64), array([339], dtype=int64), array([19], dtype=int64), array([488], dtype=int64), array([78], dtype=int64), array([857], dtype=int64), array([391], dtype=int64), array([529], dtype=int64), array([46], dtype=int64), array([319], dtype=int64), array([50], dtype=int64), array([56], dtype=int64), array([792], dtype=int64), array([691], dtype=int64), array([250], dtype=int64), array([35], dtype=int64), array([624], dtype=int64), array([96], dtype=int64), array([653], dtype=int64), array([946], dtype=int64), array([387], dtype=int64), array([561], dtype=int64), array([363], dtype=int64), array([732], dtype=int64), array([343], dtype=int64), array([696], dtype=int64), array([402], dtype=int64), array([486], dtype=int64), array([666], dtype=int64), array([567], dtype=int64), array([939], dtype=int64), array([525], dtype=int64), array([373], dtype=int64), array([754], dtype=int64), array([766], dtype=int64), array([677], dtype=int64), array([416], dtype=int64), array([166], dtype=int64), array([145], dtype=int64), array([917], dtype=int64), array([221], dtype=int64), array([659], dtype=int64), array([269], dtype=int64), array([426], dtype=int64), array([23], dtype=int64), array([690], dtype=int64), array([279], dtype=int64), array([24], dtype=int64), array([368], dtype=int64), array([645], dtype=int64), array([401], dtype=int64), array([639], dtype=int64), array([509], dtype=int64), array([807], dtype=int64), array([860], dtype=int64), array([726], dtype=int64), array([728], dtype=int64), array([753], dtype=int64), array([553], dtype=int64), array([364], dtype=int64), array([55], dtype=int64), array([69], dtype=int64), array([160], dtype=int64), array([65], dtype=int64), array([154], dtype=int64), array([464], dtype=int64), array([959], dtype=int64), array([349], dtype=int64), array([922], dtype=int64), array([124], dtype=int64), array([522], dtype=int64), array([764], dtype=int64), array([315], dtype=int64), array([204], dtype=int64), array([475], dtype=int64), array([54], dtype=int64), array([656], dtype=int64), array([783], dtype=int64), array([143], dtype=int64), array([253], dtype=int64), array([68], dtype=int64), array([137], dtype=int64), array([916], dtype=int64), array([850], dtype=int64), array([28], dtype=int64), array([22], dtype=int64), array([392], dtype=int64), array([613], dtype=int64), array([49], dtype=int64), array([303], dtype=int64), array([495], dtype=int64), array([810], dtype=int64), array([699], dtype=int64), array([298], dtype=int64), array([773], dtype=int64), array([945], dtype=int64), array([979], dtype=int64), array([129], dtype=int64), array([843], dtype=int64), array([580], dtype=int64), array([883], dtype=int64), array([854], dtype=int64), array([966], dtype=int64), array([385], dtype=int64), array([724], dtype=int64), array([483], dtype=int64), array([688], dtype=int64), array([357], dtype=int64), array([482], dtype=int64), array([679], dtype=int64), array([723], dtype=int64), array([454], dtype=int64), array([669], dtype=int64), array([626], dtype=int64), array([76], dtype=int64), array([150], dtype=int64), array([231], dtype=int64), array([299], dtype=int64), array([908], dtype=int64), array([409], dtype=int64), array([354], dtype=int64), array([974], dtype=int64), array([338], dtype=int64), array([44], dtype=int64), array([102], dtype=int64), array([399], dtype=int64), array([458], dtype=int64), array([833], dtype=int64), array([201], dtype=int64), array([236], dtype=int64), array([536], dtype=int64), array([135], dtype=int64), array([470], dtype=int64), array([329], dtype=int64), array([26], dtype=int64), array([474], dtype=int64), array([256], dtype=int64), array([540], dtype=int64), array([473], dtype=int64), array([535], dtype=int64), array([771], dtype=int64), array([578], dtype=int64), array([802], dtype=int64), array([380], dtype=int64), array([342], dtype=int64), array([274], dtype=int64), array([95], dtype=int64), array([312], dtype=int64), array([668], dtype=int64), array([740], dtype=int64), array([493], dtype=int64), array([230], dtype=int64), array([678], dtype=int64), array([989], dtype=int64), array([937], dtype=int64), array([223], dtype=int64), array([481], dtype=int64), array([785], dtype=int64), array([226], dtype=int64), array([717], dtype=int64), array([422], dtype=int64), array([865], dtype=int64), array([443], dtype=int64), array([944], dtype=int64), array([652], dtype=int64), array([466], dtype=int64), array([744], dtype=int64), array([332], dtype=int64), array([800], dtype=int64), array([804], dtype=int64), array([772], dtype=int64), array([928], dtype=int64), array([41], dtype=int64), array([421], dtype=int64), array([593], dtype=int64), array([832], dtype=int64), array([280], dtype=int64), array([687], dtype=int64), array([714], dtype=int64), array([633], dtype=int64), array([73], dtype=int64), array([491], dtype=int64), array([346], dtype=int64), array([167], dtype=int64), array([370], dtype=int64), array([123], dtype=int64), array([465], dtype=int64), array([892], dtype=int64), array([376], dtype=int64), array([852], dtype=int64), array([634], dtype=int64), array([140], dtype=int64), array([406], dtype=int64), array([268], dtype=int64), array([943], dtype=int64), array([988], dtype=int64), array([755], dtype=int64), array([846], dtype=int64), array([71], dtype=int64), array([858], dtype=int64), array([77], dtype=int64), array([622], dtype=int64), array([828], dtype=int64), array([967], dtype=int64), array([791], dtype=int64), array([254], dtype=int64), array([159], dtype=int64), array([420], dtype=int64), array([489], dtype=int64), array([570], dtype=int64), array([316], dtype=int64), array([304], dtype=int64), array([115], dtype=int64), array([205], dtype=int64), array([377], dtype=int64), array([469], dtype=int64), array([827], dtype=int64), array([265], dtype=int64), array([643], dtype=int64), array([811], dtype=int64), array([3], dtype=int64), array([36], dtype=int64), array([70], dtype=int64), array([820], dtype=int64), array([961], dtype=int64), array([384], dtype=int64), array([895], dtype=int64), array([806], dtype=int64), array([133], dtype=int64), array([964], dtype=int64), array([418], dtype=int64), array([583], dtype=int64), array([108], dtype=int64), array([513], dtype=int64), array([557], dtype=int64), array([929], dtype=int64), array([599], dtype=int64), array([130], dtype=int64), array([438], dtype=int64), array([59], dtype=int64), array([738], dtype=int64), array([590], dtype=int64), array([616], dtype=int64), array([340], dtype=int64), array([629], dtype=int64), array([543], dtype=int64), array([793], dtype=int64), array([996], dtype=int64), array([902], dtype=int64), array([212], dtype=int64), array([517], dtype=int64), array([973], dtype=int64), array([985], dtype=int64), array([365], dtype=int64), array([175], dtype=int64), array([841], dtype=int64), array([405], dtype=int64), array([408], dtype=int64), array([234], dtype=int64), array([600], dtype=int64), array([286], dtype=int64), array([674], dtype=int64), array([324], dtype=int64), array([703], dtype=int64), array([276], dtype=int64), array([550], dtype=int64), array([993], dtype=int64), array([532], dtype=int64), array([445], dtype=int64), array([569], dtype=int64), array([868], dtype=int64), array([797], dtype=int64), array([429], dtype=int64), array([587], dtype=int64), array([176], dtype=int64), array([88], dtype=int64), array([581], dtype=int64), array([193], dtype=int64), array([390], dtype=int64), array([374], dtype=int64), array([285], dtype=int64), array([10], dtype=int64), array([994], dtype=int64), array([769], dtype=int64), array([238], dtype=int64), array([352], dtype=int64), array([270], dtype=int64), array([524], dtype=int64), array([434], dtype=int64), array([228], dtype=int64), array([198], dtype=int64), array([190], dtype=int64), array([970], dtype=int64), array([119], dtype=int64), array([300], dtype=int64), array([328], dtype=int64), array([126], dtype=int64), array([520], dtype=int64), array([283], dtype=int64), array([305], dtype=int64), array([698], dtype=int64), array([950], dtype=int64), array([181], dtype=int64), array([566], dtype=int64), array([43], dtype=int64), array([453], dtype=int64), array([958], dtype=int64), array([795], dtype=int64), array([17], dtype=int64), array([904], dtype=int64), array([530], dtype=int64), array([746], dtype=int64), array([79], dtype=int64), array([195], dtype=int64), array([407], dtype=int64), array([995], dtype=int64), array([60], dtype=int64), array([382], dtype=int64), array([971], dtype=int64), array([734], dtype=int64), array([360], dtype=int64), array([822], dtype=int64), array([311], dtype=int64), array([121], dtype=int64), array([940], dtype=int64), array([67], dtype=int64), array([720], dtype=int64), array([398], dtype=int64), array([378], dtype=int64), array([144], dtype=int64), array([516], dtype=int64), array([278], dtype=int64), array([216], dtype=int64), array([661], dtype=int64), array([657], dtype=int64), array([13], dtype=int64), array([125], dtype=int64), array([80], dtype=int64), array([658], dtype=int64), array([617], dtype=int64), array([318], dtype=int64), array([909], dtype=int64), array([603], dtype=int64), array([347], dtype=int64), array([609], dtype=int64), array([614], dtype=int64), array([721], dtype=int64), array([531], dtype=int64), array([855], dtype=int64), array([873], dtype=int64), array([847], dtype=int64), array([188], dtype=int64), array([211], dtype=int64), array([287], dtype=int64), array([812], dtype=int64), array([383], dtype=int64), array([876], dtype=int64), array([548], dtype=int64), array([787], dtype=int64), array([968], dtype=int64), array([925], dtype=int64), array([292], dtype=int64), array([75], dtype=int64), array([844], dtype=int64), array([309], dtype=int64), array([912], dtype=int64), array([12], dtype=int64), array([681], dtype=int64), array([748], dtype=int64), array([351], dtype=int64), array([245], dtype=int64), array([901], dtype=int64), array([248], dtype=int64), array([21], dtype=int64), array([684], dtype=int64), array([207], dtype=int64), array([910], dtype=int64), array([37], dtype=int64), array([97], dtype=int64), array([730], dtype=int64), array([90], dtype=int64), array([765], dtype=int64), array([942], dtype=int64), array([623], dtype=int64), array([864], dtype=int64), array([42], dtype=int64), array([472], dtype=int64), array([976], dtype=int64), array([809], dtype=int64), array([117], dtype=int64), array([440], dtype=int64), array([760], dtype=int64), array([829], dtype=int64), array([640], dtype=int64), array([152], dtype=int64), array([410], dtype=int64), array([433], dtype=int64), array([98], dtype=int64), array([326], dtype=int64), array([538], dtype=int64), array([451], dtype=int64), array([631], dtype=int64), array([229], dtype=int64), array([564], dtype=int64), array([397], dtype=int64), array([701], dtype=int64), array([907], dtype=int64), array([240], dtype=int64), array([952], dtype=int64), array([29], dtype=int64), array([320], dtype=int64), array([463], dtype=int64), array([261], dtype=int64), array([7], dtype=int64), array([588], dtype=int64), array([14], dtype=int64), array([139], dtype=int64), array([749], dtype=int64), array([336], dtype=int64), array([171], dtype=int64), array([58], dtype=int64), array([83], dtype=int64), array([598], dtype=int64), array([9], dtype=int64), array([830], dtype=int64), array([986], dtype=int64), array([859], dtype=int64), array([650], dtype=int64), array([551], dtype=int64), array([556], dtype=int64), array([965], dtype=int64), array([589], dtype=int64), array([427], dtype=int64), array([262], dtype=int64), array([243], dtype=int64), array([675], dtype=int64), array([651], dtype=int64), array([127], dtype=int64), array([147], dtype=int64), array([447], dtype=int64), array([381], dtype=int64), array([110], dtype=int64), array([736], dtype=int64), array([692], dtype=int64), array([510], dtype=int64), array([824], dtype=int64), array([296], dtype=int64), array([686], dtype=int64), array([165], dtype=int64), array([856], dtype=int64), array([591], dtype=int64), array([978], dtype=int64), array([273], dtype=int64), array([782], dtype=int64), array([662], dtype=int64), array([778], dtype=int64), array([395], dtype=int64), array([199], dtype=int64), array([379], dtype=int64), array([457], dtype=int64), array([770], dtype=int64), array([38], dtype=int64), array([356], dtype=int64), array([601], dtype=int64), array([571], dtype=int64), array([317], dtype=int64), array([607], dtype=int64), array([555], dtype=int64), array([718], dtype=int64), array([621], dtype=int64), array([179], dtype=int64), array([308], dtype=int64), array([476], dtype=int64), array([156], dtype=int64), array([705], dtype=int64), array([306], dtype=int64), array([282], dtype=int64), array([499], dtype=int64), array([388], dtype=int64), array([507], dtype=int64), array([227], dtype=int64), array([301], dtype=int64), array([187], dtype=int64), array([972], dtype=int64), array([889], dtype=int64), array([498], dtype=int64), array([898], dtype=int64), array([936], dtype=int64), array([288], dtype=int64), array([527], dtype=int64), array([542], dtype=int64), array([731], dtype=int64), array([813], dtype=int64), array([164], dtype=int64), array([496], dtype=int64), array([487], dtype=int64), array([277], dtype=int64), array([122], dtype=int64), array([586], dtype=int64), array([310], dtype=int64), array([886], dtype=int64), array([161], dtype=int64), array([141], dtype=int64), array([700], dtype=int64), array([138], dtype=int64), array([949], dtype=int64), array([172], dtype=int64), array([348], dtype=int64), array([638], dtype=int64), array([224], dtype=int64), array([663], dtype=int64), array([637], dtype=int64), array([72], dtype=int64), array([798], dtype=int64), array([874], dtype=int64), array([872], dtype=int64), array([281], dtype=int64), array([94], dtype=int64), array([817], dtype=int64), array([0], dtype=int64), array([533], dtype=int64), array([981], dtype=int64), array([222], dtype=int64), array([671], dtype=int64), array([85], dtype=int64), array([5], dtype=int64), array([636], dtype=int64), array([344], dtype=int64), array([990], dtype=int64), array([367], dtype=int64), array([913], dtype=int64), array([947], dtype=int64), array([777], dtype=int64), array([404], dtype=int64), array([51], dtype=int64), array([788], dtype=int64), array([86], dtype=int64), array([208], dtype=int64), array([369], dtype=int64), array([267], dtype=int64), array([992], dtype=int64), array([837], dtype=int64), array([202], dtype=int64), array([412], dtype=int64), array([437], dtype=int64), array([826], dtype=int64), array([25], dtype=int64), array([951], dtype=int64), array([960], dtype=int64), array([213], dtype=int64), array([371], dtype=int64), array([987], dtype=int64), array([673], dtype=int64), array([413], dtype=int64), array([40], dtype=int64), array([490], dtype=int64), array([780], dtype=int64), array([266], dtype=int64), array([355], dtype=int64), array([683], dtype=int64), array([264], dtype=int64), array([116], dtype=int64), array([689], dtype=int64), array([890], dtype=int64), array([74], dtype=int64), array([842], dtype=int64), array([710], dtype=int64), array([805], dtype=int64), array([170], dtype=int64), array([931], dtype=int64), array([275], dtype=int64), array([848], dtype=int64), array([815], dtype=int64), array([169], dtype=int64), array([906], dtype=int64), array([48], dtype=int64), array([544], dtype=int64), array([372], dtype=int64), array([439], dtype=int64), array([452], dtype=int64), array([359], dtype=int64), array([953], dtype=int64), array([168], dtype=int64), array([502], dtype=int64), array([877], dtype=int64), array([456], dtype=int64), array([528], dtype=int64), array([255], dtype=int64), array([479], dtype=int64), array([763], dtype=int64), array([386], dtype=int64), array([508], dtype=int64), array([514], dtype=int64), array([497], dtype=int64), array([460], dtype=int64), array([923], dtype=int64), array([394], dtype=int64), array([350], dtype=int64), array([64], dtype=int64), array([610], dtype=int64), array([131], dtype=int64), array([752], dtype=int64), array([608], dtype=int64), array([114], dtype=int64), array([220], dtype=int64), array([252], dtype=int64), array([977], dtype=int64), array([233], dtype=int64), array([101], dtype=int64), array([562], dtype=int64), array([423], dtype=int64), array([999], dtype=int64), array([779], dtype=int64), array([774], dtype=int64), array([861], dtype=int64), array([197], dtype=int64), array([975], dtype=int64), array([743], dtype=int64), array([515], dtype=int64), array([642], dtype=int64), array([573], dtype=int64), array([322], dtype=int64), array([232], dtype=int64), array([735], dtype=int64), array([707], dtype=int64), array([327], dtype=int64), array([136], dtype=int64), array([546], dtype=int64), array([565], dtype=int64), array([323], dtype=int64), array([6], dtype=int64), array([695], dtype=int64), array([694], dtype=int64), array([358], dtype=int64), array([733], dtype=int64), array([494], dtype=int64), array([400], dtype=int64), array([606], dtype=int64), array([501], dtype=int64), array([109], dtype=int64), array([134], dtype=int64), array([334], dtype=int64), array([155], dtype=int64), array([869], dtype=int64), array([983], dtype=int64), array([518], dtype=int64), array([962], dtype=int64), array([630], dtype=int64), array([575], dtype=int64), array([61], dtype=int64), array([526], dtype=int64), array([823], dtype=int64), array([618], dtype=int64), array([722], dtype=int64), array([558], dtype=int64), array([849], dtype=int64), array([66], dtype=int64), array([632], dtype=int64), array([194], dtype=int64), array([87], dtype=int64), array([241], dtype=int64), array([289], dtype=int64), array([492], dtype=int64), array([62], dtype=int64), array([768], dtype=int64), array([552], dtype=int64), array([930], dtype=int64), array([291], dtype=int64), array([235], dtype=int64), array([708], dtype=int64), array([816], dtype=int64), array([414], dtype=int64), array([585], dtype=int64), array([389], dtype=int64), array([969], dtype=int64), array([998], dtype=int64), array([767], dtype=int64), array([796], dtype=int64), array([209], dtype=int64), array([177], dtype=int64), array([174], dtype=int64), array([361], dtype=int64), array([799], dtype=int64), array([200], dtype=int64), array([244], dtype=int64), array([295], dtype=int64), array([89], dtype=int64), array([615], dtype=int64), array([984], dtype=int64), array([646], dtype=int64), array([441], dtype=int64), array([185], dtype=int64), array([742], dtype=int64), array([512], dtype=int64), array([789], dtype=int64), array([81], dtype=int64), array([459], dtype=int64), array([184], dtype=int64), array([63], dtype=int64), array([180], dtype=int64), array([670], dtype=int64), array([272], dtype=int64), array([838], dtype=int64), array([107], dtype=int64), array([173], dtype=int64), array([468], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "sorted_gradient_ind = sort_and_keep_indices(loaded, training_size)\n",
    "print(sorted_gradient_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trích xuất đặc trưng từ các tập dữ liệu\n",
    "# Hàm để trích xuất đặc trưng từ X\n",
    "def extract_features(net, X):\n",
    "    X = torch.tensor(X).float().unsqueeze(1)  # Thêm chiều để có kích thước [N, 1, 28, 28]\n",
    "    X = X.repeat(1, 3, 1, 1).to(device)  # Chuyển thành [N, 3, 28, 28]\n",
    "    with torch.no_grad():\n",
    "        features = net(X)\n",
    "    return features.cpu().numpy()\n",
    "X_train_features = extract_features(net_test, X_train_imbalanced)\n",
    "X_test_features = extract_features(net_test, X_test_imbalanced)\n",
    "\n",
    "# Lưu các đặc trưng đã trích xuất\n",
    "with open('balanced_train_features.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train_features, y_train_imbalanced), f)\n",
    "with open('balanced_test_features.pkl', 'wb') as f:\n",
    "    pickle.dump((X_test_features, y_test_imbalanced), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_features_loaded shape: (1000, 100)\n",
      "y_train_loaded shape: (1000,)\n",
      "X_test_features_loaded shape: (4000, 100)\n",
      "y_test_loaded shape: (4000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load lại các đặc trưng đã lưu cho tập huấn luyện\n",
    "with open('balanced_train_features.pkl', 'rb') as f:\n",
    "    X_train_features_loaded, y_train_loaded = pickle.load(f)\n",
    "\n",
    "\n",
    "# Load lại các đặc trưng đã lưu cho tập kiểm tra (test)\n",
    "with open('balanced_test_features.pkl', 'rb') as f:\n",
    "    X_test_features_loaded, y_test_loaded = pickle.load(f)\n",
    "\n",
    "# Kiểm tra kích thước để đảm bảo rằng dữ liệu đã được tải đúng\n",
    "print(f\"X_train_features_loaded shape: {X_train_features_loaded.shape}\")\n",
    "print(f\"y_train_loaded shape: {y_train_loaded.shape}\")\n",
    "print(f\"X_test_features_loaded shape: {X_test_features_loaded.shape}\")\n",
    "print(f\"y_test_loaded shape: {y_test_loaded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features_loaded)\n",
    "X_test_scaled = scaler.transform(X_test_features_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 69672180000.0, 1: 4644369000.0, 2: 4443694000.0, 3: 4280734700.0, 4: 4033518000.0, 5: 3967600000.0, 6: 3955789300.0, 7: 3901150200.0, 8: 3845601300.0, 9: 3843897300.0, 10: 3806365200.0, 11: 3767735800.0, 12: 3719465000.0, 13: 3711263200.0, 14: 3693731300.0, 15: 3692124200.0, 16: 3658300000.0, 17: 3633917000.0, 18: 3630165000.0, 19: 3629163000.0, 20: 3603912200.0, 21: 3598826500.0, 22: 3597165600.0, 23: 3594157000.0, 24: 3591550500.0, 25: 3584875000.0, 26: 3576539600.0, 27: 3570951700.0, 28: 3533981700.0, 29: 3525328000.0, 30: 3521223700.0, 31: 3511636000.0, 32: 3502064600.0, 33: 3488222200.0, 34: 3485116000.0, 35: 3481033700.0, 36: 3480057300.0, 37: 3470401500.0, 38: 3468031000.0, 39: 3467080700.0, 40: 3466176000.0, 41: 3464757800.0, 42: 3463348200.0, 43: 3462867500.0, 44: 3462027800.0, 45: 3460058600.0, 46: 3458549800.0, 47: 3458445800.0, 48: 3445255700.0, 49: 3436430300.0, 50: 3435028000.0, 51: 3434736000.0, 52: 3431393800.0, 53: 3416678400.0, 54: 3414521900.0, 55: 3412327400.0, 56: 3412202000.0, 57: 3411553300.0, 58: 3409910800.0, 59: 3409813500.0, 60: 3407102500.0, 61: 3401236000.0, 62: 3398654000.0, 63: 3397297200.0, 64: 3394100200.0, 65: 3391915500.0, 66: 3391676000.0, 67: 3388788700.0, 68: 3388377000.0, 69: 3381067800.0, 70: 3379195400.0, 71: 3374563300.0, 72: 3374380000.0, 73: 3372189200.0, 74: 3369763300.0, 75: 3369477600.0, 76: 3366912500.0, 77: 3364233700.0, 78: 3364120600.0, 79: 3363842000.0, 80: 3362132000.0, 81: 3360567800.0, 82: 3358762000.0, 83: 3356590000.0, 84: 3355611100.0, 85: 3351648300.0, 86: 3349571600.0, 87: 3341227500.0, 88: 3340158000.0, 89: 3338899500.0, 90: 3338201600.0, 91: 3335825400.0, 92: 3331874300.0, 93: 3316967400.0, 94: 3316149200.0, 95: 3306087000.0, 96: 3297213400.0, 97: 3296692700.0, 98: 3295691300.0, 99: 3284020700.0, 100: 3281943600.0, 101: 3281921500.0, 102: 3268506600.0, 103: 3264091600.0, 104: 3252923000.0, 105: 3250834400.0, 106: 3249477600.0, 107: 3236338200.0, 108: 3236222000.0, 109: 3234430000.0, 110: 3218118100.0, 111: 3217965600.0, 112: 3214306800.0, 113: 3209568300.0, 114: 3207731700.0, 115: 3207329800.0, 116: 3205061600.0, 117: 3204250000.0, 118: 3203787300.0, 119: 3197474300.0, 120: 3193111600.0, 121: 3190721500.0, 122: 3182888000.0, 123: 3176090600.0, 124: 3172292000.0, 125: 3167988700.0, 126: 3159559700.0, 127: 3155844000.0, 128: 3151817700.0, 129: 3142306800.0, 130: 3141167000.0, 131: 3139972000.0, 132: 3133651000.0, 133: 3128989700.0, 134: 3125612000.0, 135: 3119820800.0, 136: 3111690800.0, 137: 3108231700.0, 138: 3103576600.0, 139: 3089685000.0, 140: 3075124700.0, 141: 3071200800.0, 142: 3064079400.0, 143: 3019980300.0, 144: 3012814300.0, 145: 3005683200.0, 146: 2997066800.0, 147: 2984214000.0, 148: 2961792000.0, 149: 2946709500.0, 150: 2930614300.0, 151: 2926049300.0, 152: 2917576200.0, 153: 2910360600.0, 154: 2910360600.0, 155: 2896210000.0, 156: 2786166300.0, 157: 2783895000.0, 158: 2750578700.0, 159: 2690558500.0, 160: 2670000000.0, 161: 2634532900.0, 162: 2606588400.0, 163: 2525629400.0, 164: 2412174300.0, 165: 2387837400.0, 166: 2102406800.0, 167: 1645795700.0, 168: 1542991500.0, 169: 583064600.0, 170: 469208960.0, 171: 122052480.0, 172: -209095680.0, 173: -233768960.0, 174: -320520200.0, 175: -476425100.0, 176: -495457660.0, 177: -499381570.0, 178: -504565570.0, 179: -542500400.0, 180: -556486340.0, 181: -558078000.0, 182: -575917900.0, 183: -585160300.0, 184: -585776060.0, 185: -586971140.0, 186: -601053950.0, 187: -601450430.0, 188: -601797100.0, 189: -603555840.0, 190: -604918800.0, 191: -610967100.0, 192: -619881700.0, 193: -620266000.0, 194: -627110100.0, 195: -628076160.0, 196: -628690560.0, 197: -630119500.0, 198: -630923200.0, 199: -633524740.0, 200: -634457340.0, 201: -635191300.0, 202: -637966300.0, 203: -639111100.0, 204: -643399500.0, 205: -645852540.0, 206: -646487700.0, 207: -649227900.0, 208: -653110400.0, 209: -653158460.0, 210: -653762700.0, 211: -657228700.0, 212: -658577100.0, 213: -660409700.0, 214: -662883140.0, 215: -664709700.0, 216: -664891800.0, 217: -665074900.0, 218: -668185400.0, 219: -669029000.0, 220: -670285600.0, 221: -670477000.0, 222: -673278500.0, 223: -673492030.0, 224: -675506200.0, 225: -677619900.0, 226: -677838000.0, 227: -682469300.0, 228: -684810940.0, 229: -684897150.0, 230: -686103100.0, 231: -686981440.0, 232: -687124400.0, 233: -687687300.0, 234: -687792960.0, 235: -690137700.0, 236: -690646700.0, 237: -690686140.0, 238: -691229630.0, 239: -691885250.0, 240: -693929150.0, 241: -694445500.0, 242: -695138600.0, 243: -695152800.0, 244: -695611900.0, 245: -697117700.0, 246: -697275900.0, 247: -698737660.0, 248: -699001100.0, 249: -699635140.0, 250: -700722800.0, 251: -701924500.0, 252: -702299650.0, 253: -703204600.0, 254: -703334140.0, 255: -703431550.0, 256: -703661100.0, 257: -703828540.0, 258: -706960600.0, 259: -707612500.0, 260: -707748300.0, 261: -708009660.0, 262: -709444100.0, 263: -711024000.0, 264: -711639040.0, 265: -711902460.0, 266: -712181200.0, 267: -712206100.0, 268: -713150600.0, 269: -713235900.0, 270: -713285250.0, 271: -713287550.0, 272: -713537150.0, 273: -713548740.0, 274: -713631040.0, 275: -714037600.0, 276: -714173100.0, 277: -714598400.0, 278: -714880830.0, 279: -715314750.0, 280: -715471500.0, 281: -715567940.0, 282: -716079700.0, 283: -716145300.0, 284: -717082430.0, 285: -717837400.0, 286: -718154900.0, 287: -718876860.0, 288: -719037600.0, 289: -719075200.0, 290: -720069000.0, 291: -720094100.0, 292: -720294850.0, 293: -720458100.0, 294: -720562940.0, 295: -721159740.0, 296: -721409700.0, 297: -721960400.0, 298: -722296640.0, 299: -722459460.0, 300: -722933200.0, 301: -723189600.0, 302: -723380000.0, 303: -723411700.0, 304: -724486100.0, 305: -724926200.0, 306: -725092500.0, 307: -725273900.0, 308: -726332800.0, 309: -726425800.0, 310: -726570700.0, 311: -726633600.0, 312: -727270900.0, 313: -727433800.0, 314: -728107300.0, 315: -728130300.0, 316: -728146940.0, 317: -728556900.0, 318: -729053060.0, 319: -729357000.0, 320: -729633500.0, 321: -729811400.0, 322: -730052300.0, 323: -730623300.0, 324: -730630100.0, 325: -730766800.0, 326: -731241300.0, 327: -731844000.0, 328: -731925500.0, 329: -732799900.0, 330: -732840500.0, 331: -733231550.0, 332: -733734700.0, 333: -733799600.0, 334: -733877600.0, 335: -734228030.0, 336: -734285060.0, 337: -734310800.0, 338: -734327100.0, 339: -734482000.0, 340: -734691200.0, 341: -734817300.0, 342: -734922500.0, 343: -735057400.0, 344: -735059300.0, 345: -735085200.0, 346: -735385540.0, 347: -735766340.0, 348: -736004800.0, 349: -736806300.0, 350: -736976260.0, 351: -737164500.0, 352: -737166500.0, 353: -737553340.0, 354: -737659100.0, 355: -737920960.0, 356: -737957950.0, 357: -738564900.0, 358: -738577200.0, 359: -738607800.0, 360: -738638700.0, 361: -738729860.0, 362: -739151600.0, 363: -739285760.0, 364: -739378050.0, 365: -739821700.0, 366: -739922700.0, 367: -740088770.0, 368: -740619840.0, 369: -740720300.0, 370: -740813200.0, 371: -740913800.0, 372: -741267650.0, 373: -741373900.0, 374: -741420200.0, 375: -741501250.0, 376: -741595800.0, 377: -741688450.0, 378: -741808060.0, 379: -741824260.0, 380: -742125300.0, 381: -742500740.0, 382: -742540000.0, 383: -742686800.0, 384: -742697400.0, 385: -742843600.0, 386: -742977540.0, 387: -743001600.0, 388: -743473600.0, 389: -743835100.0, 390: -743918700.0, 391: -744033660.0, 392: -744262800.0, 393: -744309300.0, 394: -744358300.0, 395: -744397700.0, 396: -744430600.0, 397: -744832200.0, 398: -745125760.0, 399: -745269600.0, 400: -745390900.0, 401: -745749950.0, 402: -746015900.0, 403: -746040260.0, 404: -746059650.0, 405: -746152770.0, 406: -746242500.0, 407: -746408900.0, 408: -746544900.0, 409: -746640600.0, 410: -747148500.0, 411: -747240200.0, 412: -747465200.0, 413: -747751800.0, 414: -747905860.0, 415: -747908100.0, 416: -748042300.0, 417: -748097660.0, 418: -748100500.0, 419: -748232700.0, 420: -748316740.0, 421: -748347800.0, 422: -748630400.0, 423: -749119200.0, 424: -749122940.0, 425: -749294700.0, 426: -749472450.0, 427: -749618800.0, 428: -749694140.0, 429: -749749400.0, 430: -749822800.0, 431: -750219600.0, 432: -750280800.0, 433: -750494900.0, 434: -750515840.0, 435: -750650700.0, 436: -750972100.0, 437: -751012800.0, 438: -751022400.0, 439: -751145540.0, 440: -751149000.0, 441: -751548540.0, 442: -751588350.0, 443: -751904700.0, 444: -751961100.0, 445: -752152640.0, 446: -752271940.0, 447: -752401200.0, 448: -752619800.0, 449: -752637100.0, 450: -752769100.0, 451: -752935740.0, 452: -753242000.0, 453: -753256200.0, 454: -753416900.0, 455: -753518460.0, 456: -753781950.0, 457: -753918800.0, 458: -753965400.0, 459: -754132600.0, 460: -754210200.0, 461: -754301400.0, 462: -754673340.0, 463: -754895200.0, 464: -755034700.0, 465: -755491260.0, 466: -755597250.0, 467: -755622340.0, 468: -755671940.0, 469: -755847230.0, 470: -755942400.0, 471: -756357060.0, 472: -756572300.0, 473: -756715900.0, 474: -756763840.0, 475: -756772900.0, 476: -756961540.0, 477: -757094660.0, 478: -757146300.0, 479: -757402300.0, 480: -757673400.0, 481: -757821950.0, 482: -758229760.0, 483: -758267900.0, 484: -758302300.0, 485: -758481800.0, 486: -758603650.0, 487: -758607600.0, 488: -758645900.0, 489: -758682100.0, 490: -758744600.0, 491: -758789200.0, 492: -759030850.0, 493: -759161150.0, 494: -759255700.0, 495: -759677630.0, 496: -759684540.0, 497: -759799600.0, 498: -759826500.0, 499: -759996700.0, 500: -760174400.0, 501: -760193700.0, 502: -760213950.0, 503: -760491100.0, 504: -760519940.0, 505: -760593340.0, 506: -760655000.0, 507: -761012600.0, 508: -761041900.0, 509: -761182800.0, 510: -761389630.0, 511: -761498300.0, 512: -761517100.0, 513: -761550600.0, 514: -761628860.0, 515: -761662400.0, 516: -761706240.0, 517: -761768800.0, 518: -761866940.0, 519: -761910400.0, 520: -761962500.0, 521: -762025300.0, 522: -762056450.0, 523: -762106940.0, 524: -762108860.0, 525: -762163700.0, 526: -762170100.0, 527: -762177800.0, 528: -762313500.0, 529: -762349300.0, 530: -762353400.0, 531: -762641340.0, 532: -762756900.0, 533: -762900900.0, 534: -763062300.0, 535: -763160640.0, 536: -763349760.0, 537: -763386940.0, 538: -763449340.0, 539: -763495300.0, 540: -763498500.0, 541: -763499140.0, 542: -763613300.0, 543: -763615400.0, 544: -763796350.0, 545: -763801660.0, 546: -763881700.0, 547: -763954940.0, 548: -763983740.0, 549: -764010900.0, 550: -764064200.0, 551: -764085000.0, 552: -764117500.0, 553: -764225400.0, 554: -764262600.0, 555: -764325300.0, 556: -764537900.0, 557: -764835400.0, 558: -764887800.0, 559: -764917000.0, 560: -765128300.0, 561: -765241700.0, 562: -765369100.0, 563: -765403260.0, 564: -765492860.0, 565: -765564200.0, 566: -765715100.0, 567: -765840700.0, 568: -766007700.0, 569: -766082750.0, 570: -766098900.0, 571: -766162800.0, 572: -766172000.0, 573: -766226100.0, 574: -766299100.0, 575: -766360770.0, 576: -766432640.0, 577: -766507200.0, 578: -766573950.0, 579: -766673400.0, 580: -766725900.0, 581: -766892200.0, 582: -766967230.0, 583: -767343000.0, 584: -767388100.0, 585: -767506370.0, 586: -767605700.0, 587: -767640260.0, 588: -767646900.0, 589: -767700300.0, 590: -767744400.0, 591: -767750340.0, 592: -767758800.0, 593: -767762900.0, 594: -767781400.0, 595: -767915000.0, 596: -768052030.0, 597: -768052400.0, 598: -768129600.0, 599: -768136770.0, 600: -768346750.0, 601: -768479940.0, 602: -768481000.0, 603: -768489100.0, 604: -768504770.0, 605: -768609150.0, 606: -768681300.0, 607: -769145400.0, 608: -769202400.0, 609: -769569600.0, 610: -769574600.0, 611: -769589250.0, 612: -769629950.0, 613: -769708740.0, 614: -769783360.0, 615: -769798100.0, 616: -769810000.0, 617: -769811900.0, 618: -769878500.0, 619: -769908900.0, 620: -769951400.0, 621: -769997300.0, 622: -770021250.0, 623: -770043900.0, 624: -770163140.0, 625: -770173900.0, 626: -770231600.0, 627: -770649100.0, 628: -770734340.0, 629: -771062900.0, 630: -771198700.0, 631: -771222600.0, 632: -771386900.0, 633: -771394700.0, 634: -771426100.0, 635: -771522560.0, 636: -771535550.0, 637: -771731000.0, 638: -771817200.0, 639: -771986900.0, 640: -772119500.0, 641: -772138050.0, 642: -772379700.0, 643: -772524860.0, 644: -772553860.0, 645: -772710500.0, 646: -772730050.0, 647: -772730700.0, 648: -772784060.0, 649: -772904770.0, 650: -773058940.0, 651: -773084160.0, 652: -773233000.0, 653: -773319300.0, 654: -773379000.0, 655: -773415550.0, 656: -773516350.0, 657: -773533100.0, 658: -773577340.0, 659: -773588600.0, 660: -773709600.0, 661: -773801300.0, 662: -773858200.0, 663: -773906370.0, 664: -773969540.0, 665: -774062800.0, 666: -774158000.0, 667: -774206140.0, 668: -774266800.0, 669: -774420700.0, 670: -774430140.0, 671: -774631940.0, 672: -774776770.0, 673: -774824300.0, 674: -775014500.0, 675: -775057340.0, 676: -775096960.0, 677: -775203800.0, 678: -775238400.0, 679: -775275900.0, 680: -775314750.0, 681: -775320000.0, 682: -775347460.0, 683: -775446340.0, 684: -775504000.0, 685: -775648640.0, 686: -775882900.0, 687: -775917300.0, 688: -775923100.0, 689: -776163700.0, 690: -776252400.0, 691: -776355300.0, 692: -776606140.0, 693: -776630400.0, 694: -776703800.0, 695: -776754050.0, 696: -776754200.0, 697: -776850370.0, 698: -776853950.0, 699: -776892860.0, 700: -776964540.0, 701: -777187650.0, 702: -777224900.0, 703: -777462140.0, 704: -777519040.0, 705: -777628600.0, 706: -777631940.0, 707: -777636860.0, 708: -777770560.0, 709: -777780000.0, 710: -777791000.0, 711: -777911900.0, 712: -777931140.0, 713: -778032600.0, 714: -778126140.0, 715: -778363970.0, 716: -778464500.0, 717: -778479740.0, 718: -778593340.0, 719: -778656800.0, 720: -778931600.0, 721: -778959800.0, 722: -779100600.0, 723: -779106940.0, 724: -779215360.0, 725: -779253060.0, 726: -779357950.0, 727: -779382700.0, 728: -779561660.0, 729: -779700200.0, 730: -779867400.0, 731: -779869440.0, 732: -779973000.0, 733: -780102660.0, 734: -780222340.0, 735: -780250900.0, 736: -780362750.0, 737: -780471700.0, 738: -780581570.0, 739: -780654200.0, 740: -781009660.0, 741: -781095230.0, 742: -781162940.0, 743: -781171400.0, 744: -781173300.0, 745: -781433800.0, 746: -781618300.0, 747: -781641200.0, 748: -781748350.0, 749: -781795600.0, 750: -781894900.0, 751: -781967940.0, 752: -782379970.0, 753: -782395300.0, 754: -782444700.0, 755: -782463400.0, 756: -782524300.0, 757: -782915460.0, 758: -782927900.0, 759: -782976700.0, 760: -782984260.0, 761: -783087000.0, 762: -783176300.0, 763: -783299460.0, 764: -783462500.0, 765: -783491140.0, 766: -783499460.0, 767: -783505500.0, 768: -783735000.0, 769: -783814460.0, 770: -783872960.0, 771: -784054900.0, 772: -784072450.0, 773: -784249800.0, 774: -784294700.0, 775: -784332400.0, 776: -784388540.0, 777: -784393660.0, 778: -784414500.0, 779: -784661200.0, 780: -784689700.0, 781: -784737400.0, 782: -784814400.0, 783: -784833800.0, 784: -784993000.0, 785: -785079550.0, 786: -785089100.0, 787: -785152400.0, 788: -785249300.0, 789: -785331300.0, 790: -785344400.0, 791: -785371500.0, 792: -785499400.0, 793: -785525800.0, 794: -785707840.0, 795: -785780000.0, 796: -785873900.0, 797: -785896960.0, 798: -785978500.0, 799: -786001540.0, 800: -786010750.0, 801: -786014850.0, 802: -786069570.0, 803: -786157700.0, 804: -786283400.0, 805: -786499460.0, 806: -786805440.0, 807: -786814400.0, 808: -786941760.0, 809: -786964030.0, 810: -786970050.0, 811: -787188160.0, 812: -787335600.0, 813: -787359700.0, 814: -787394050.0, 815: -787538240.0, 816: -787657600.0, 817: -787678340.0, 818: -787790200.0, 819: -787799100.0, 820: -787813500.0, 821: -787836100.0, 822: -787870500.0, 823: -787927040.0, 824: -788004300.0, 825: -788348860.0, 826: -788517570.0, 827: -788612030.0, 828: -788722900.0, 829: -788815100.0, 830: -789222660.0, 831: -789345000.0, 832: -789348100.0, 833: -789378240.0, 834: -789570560.0, 835: -789734660.0, 836: -789771200.0, 837: -789831300.0, 838: -789848200.0, 839: -789858240.0, 840: -789919040.0, 841: -789927230.0, 842: -789971100.0, 843: -790154430.0, 844: -790161500.0, 845: -790225660.0, 846: -790514940.0, 847: -790608640.0, 848: -790641300.0, 849: -790740100.0, 850: -790788000.0, 851: -790799800.0, 852: -790911800.0, 853: -791002400.0, 854: -791014600.0, 855: -791026500.0, 856: -791032100.0, 857: -791072450.0, 858: -791396100.0, 859: -791578050.0, 860: -791730940.0, 861: -791734800.0, 862: -791885250.0, 863: -791964740.0, 864: -792030600.0, 865: -792068300.0, 866: -792085950.0, 867: -792304500.0, 868: -792325440.0, 869: -792601600.0, 870: -793074240.0, 871: -793091260.0, 872: -793098940.0, 873: -793260860.0, 874: -793348030.0, 875: -793435650.0, 876: -793743800.0, 877: -793856260.0, 878: -793895230.0, 879: -794086300.0, 880: -794095500.0, 881: -794123300.0, 882: -794204900.0, 883: -794228800.0, 884: -794257100.0, 885: -794297700.0, 886: -794555260.0, 887: -794573600.0, 888: -794713860.0, 889: -794746750.0, 890: -794760900.0, 891: -794808700.0, 892: -794836700.0, 893: -795112960.0, 894: -795135400.0, 895: -795336260.0, 896: -795406400.0, 897: -795490560.0, 898: -795511500.0, 899: -795642800.0, 900: -795814850.0, 901: -796140540.0, 902: -796312260.0, 903: -796392200.0, 904: -796462300.0, 905: -796493060.0, 906: -796887550.0, 907: -796890240.0, 908: -797484740.0, 909: -797490800.0, 910: -797578050.0, 911: -797642900.0, 912: -797891840.0, 913: -797958700.0, 914: -798237250.0, 915: -798318600.0, 916: -798551200.0, 917: -798591170.0, 918: -798654850.0, 919: -798844740.0, 920: -798862000.0, 921: -798876740.0, 922: -798946560.0, 923: -799059800.0, 924: -799351500.0, 925: -799408600.0, 926: -799480000.0, 927: -799583740.0, 928: -799627600.0, 929: -799707500.0, 930: -799779260.0, 931: -799857700.0, 932: -800496500.0, 933: -800497540.0, 934: -800607100.0, 935: -800644700.0, 936: -800678340.0, 937: -800881540.0, 938: -801296830.0, 939: -801660600.0, 940: -801682240.0, 941: -801789400.0, 942: -801919040.0, 943: -802372500.0, 944: -802445100.0, 945: -802648060.0, 946: -802726600.0, 947: -803140350.0, 948: -803242800.0, 949: -803699650.0, 950: -803942000.0, 951: -804271800.0, 952: -804511800.0, 953: -804739260.0, 954: -804929100.0, 955: -804950700.0, 956: -805308900.0, 957: -805926600.0, 958: -806118400.0, 959: -806232200.0, 960: -806311100.0, 961: -806371400.0, 962: -806536500.0, 963: -807194430.0, 964: -807429440.0, 965: -808080450.0, 966: -808477500.0, 967: -808745540.0, 968: -808841150.0, 969: -808960450.0, 970: -808966100.0, 971: -809098800.0, 972: -809184400.0, 973: -809264960.0, 974: -809626200.0, 975: -809692700.0, 976: -810046100.0, 977: -810055200.0, 978: -810288450.0, 979: -811053760.0, 980: -811085440.0, 981: -811167400.0, 982: -811297300.0, 983: -811641800.0, 984: -811814100.0, 985: -812505200.0, 986: -813714370.0, 987: -814375900.0, 988: -814723900.0, 989: -814797800.0, 990: -815068160.0, 991: -816097000.0, 992: -816397100.0, 993: -816558000.0, 994: -818856640.0, 995: -819550340.0, 996: -821355840.0, 997: -822337660.0, 998: -827333600.0, 999: -830580030.0}\n"
     ]
    }
   ],
   "source": [
    "dic_loaded = {}\n",
    "for i, l in enumerate(loaded):\n",
    "    dic_loaded[i] = l\n",
    "print(dic_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted accuracy drop is 0.024\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from eval import data_removal_f1\n",
    "from metrics import weighted_acc_drop\n",
    "acc = data_removal_f1(dic_loaded, X_train_scaled, y_train_imbalanced, X_test_scaled, y_test_imbalanced)\n",
    "plt.plot(range(len(acc)), acc)\n",
    "res = weighted_acc_drop(acc)\n",
    "print(\"The weighted accuracy drop is {:.3f}\".format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m get_indices(loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m trained_with_flag \u001b[38;5;241m=\u001b[39m train_with_corrupt_flag(loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle_ind, train_indices)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaders' is not defined"
     ]
    }
   ],
   "source": [
    "train_indices = get_indices(dataloader['train'])\n",
    "trained_with_flag = train_with_corrupt_flag(dataloader['train'], train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "calibrated_gradient = lava.compute_values_and_visualize(dual_sol, trained_with_flag, training_size, portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28) (1000,)\n",
      "(4000, 28, 28) (4000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('balanced_train.pkl', 'rb') as f:\n",
    "    X_train_balanced, y_train_balanced = pickle.load(f)\n",
    "with open('balanced_test.pkl', 'rb') as f:\n",
    "    X_test_balanced, y_test_balanced = pickle.load(f)\n",
    "print(X_train_balanced.shape, y_train_balanced.shape)\n",
    "print(X_test_balanced.shape, y_test_balanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\utils.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import lava\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as n\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X_train_balanced, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train_balanced, dtype=torch.long)\n",
    "batch_size = 8\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = {}\n",
    "dataloader['train'] = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "X_tensor = torch.tensor(X_test_balanced, dtype= torch.float32)\n",
    "y_tensor = torch.tensor(y_test_balanced, dtype = torch.long)\n",
    "dataset = TensorDataset(X_tensor, y_tensor) \n",
    "dataloader['test'] = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "\n",
    "# Định nghĩa mô hình PreActResNet18 như đã thực hiện trước đó\n",
    "class PreActBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def PreActResNet18():\n",
    "    return PreActResNet(PreActBlock, [2,2,2,2])\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net_test = PreActResNet18().to(device)\n",
    "feature_extractor_name = 'preact_resnet18_test_mnist.pth'\n",
    "net_test.load_state_dict(torch.load('checkpoint/' + feature_extractor_name, map_location=torch.device('cpu')))\n",
    "net_test.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.0\n",
      "2.3.0\n",
      "Cuda device:  0\n",
      "cude devices:  1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "embedder = net_test.to(device)\n",
    "embedder.fc = torch.nn.Identity()\n",
    "for p in embedder.parameters():\n",
    "    p.requires_grad = False\n",
    "embedder.to(device)\n",
    "cuda_num = 0\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(cuda_num)\n",
    "#print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "#torch.cuda.set_device(cuda_num)\n",
    "print(\"Cuda device: \", torch.cuda.current_device())\n",
    "print(\"cude devices: \", torch.cuda.device_count())\n",
    "device = torch.device('cuda:' + str(cuda_num) if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "training_size = 1000\n",
    "valid_size = 200\n",
    "resize = 32\n",
    "portion = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from otdd.pytorch.distance_fast import DatasetDistance, FeatureCost, batch_augmented_cost\n",
    "from otdd.pytorch.wasserstein import pwdist_exact\n",
    "from functools import partial\n",
    "from lava import train_with_corrupt_flag, get_indices, values, sort_and_keep_indices\n",
    "resize = 28\n",
    "feature_cost = FeatureCost(src_embedding = embedder,\n",
    "                               src_dim = (1, resize,resize),\n",
    "                               tgt_embedding = embedder,\n",
    "                               tgt_dim = (1, resize,resize),\n",
    "                               p = 2,\n",
    "                               device='cuda')\n",
    "dist = DatasetDistance(dataloader['train'], dataloader['test'],\n",
    "                           inner_ot_method = 'exact',\n",
    "                           debiased_loss = True,\n",
    "                           feature_cost = feature_cost,\n",
    "                           λ_x=1.0, λ_y=1.0,\n",
    "                           sqrt_method = 'spectral',\n",
    "                           sqrt_niters=10,\n",
    "                           precision='single',\n",
    "                           p = 2, entreg = 1e-1,\n",
    "                           device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8fbe7df2984b1ca9e7290b2920d1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load full dataset: torch.Size([1000])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be51b068623843ab9317bfb6c99f80ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load full dataset: torch.Size([4000])\n",
      "2 2\n",
      "[(0, 1)]\n",
      "cost function:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48dd9673db149bc875ba3a84067f832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "huhu: torch.Size([500, 784]) torch.Size([500, 784])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "torch.Size([1, 500, 500])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "torch.Size([1, 500, 500])\n",
      "was: tensor(2.6559e+09, device='cuda:0')\n",
      "2 2\n",
      "[(0, 1)]\n",
      "cost function:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66604c651d874d2cb34ec9d54ce70c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "huhu: torch.Size([2000, 784]) torch.Size([2000, 784])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "torch.Size([1, 2000, 2000])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "torch.Size([1, 2000, 2000])\n",
      "was: tensor(9.9594e+08, device='cuda:0')\n",
      "2 2\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "cost function:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffa158cdc6e4a9ca151d83e6683c405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "huhu: torch.Size([500, 784]) torch.Size([2000, 784])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "torch.Size([1, 500, 2000])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "torch.Size([1, 2000, 500])\n",
      "was: tensor(4.7015e+09, device='cuda:0')\n",
      "0 1\n",
      "huhu: torch.Size([500, 784]) torch.Size([2000, 784])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "torch.Size([1, 500, 2000])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "torch.Size([1, 2000, 500])\n",
      "was: tensor(2.5343e+09, device='cuda:0')\n",
      "1 0\n",
      "huhu: torch.Size([500, 784]) torch.Size([2000, 784])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "torch.Size([1, 500, 2000])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "torch.Size([1, 2000, 500])\n",
      "was: tensor(6.5910e+09, device='cuda:0')\n",
      "1 1\n",
      "huhu: torch.Size([500, 784]) torch.Size([2000, 784])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "torch.Size([1, 500, 2000])\n",
      "1 2000 784\n",
      "torch.Size([1, 2000, 100])\n",
      "1 500 784\n",
      "torch.Size([1, 500, 100])\n",
      "torch.Size([1, 2000, 500])\n",
      "was: tensor(1.8565e+09, device='cuda:0')\n",
      "torch.Size([1000, 785]) torch.Size([4000, 785])\n"
     ]
    }
   ],
   "source": [
    "W = dist._get_label_distances().to(torch.device(device))\n",
    "import geomloss\n",
    "import numpy as np\n",
    "cost_geomloss = partial(\n",
    "                batch_augmented_cost,\n",
    "                W=W,\n",
    "                λ_x=dist.λ_x,\n",
    "                λ_y=dist.λ_y,\n",
    "                feature_cost=dist.feature_cost\n",
    "            )\n",
    "\n",
    "loss = geomloss.SamplesLoss(\n",
    "                loss=dist.loss, p=dist.p,\n",
    "                cost=cost_geomloss,\n",
    "                debias=dist.debiased_loss,\n",
    "                blur=dist.entreg**(1 / dist.p),\n",
    "                backend='tensorized'\n",
    "            )\n",
    "maxsamples = None\n",
    "if maxsamples and dist.X1.shape[0] > maxsamples:\n",
    "    idxs_1 = sorted(np.random.choice(\n",
    "    dist.X1.shape[0], maxsamples, replace=False))\n",
    "else:\n",
    "    idxs_1 = np.s_[:]  # hack to get a full slice\n",
    "\n",
    "if maxsamples and dist.X2.shape[0] > maxsamples:\n",
    "    idxs_2 = sorted(np.random.choice(\n",
    "    dist.X2.shape[0], maxsamples, replace=False))\n",
    "else:\n",
    "    idxs_2 = np.s_[:]  # hack to get a full slice\n",
    "Z1 = torch.cat((dist.X1[idxs_1],\n",
    "                dist.Y1[idxs_1].type(dist.X1.dtype).unsqueeze(1)), -1)\n",
    "Z2 = torch.cat((dist.X2[idxs_2],\n",
    "                dist.Y2[idxs_2].type(dist.X2.dtype).unsqueeze(1)), -1)\n",
    "Z1 = Z1.to(device)\n",
    "Z2 = Z2.to(device)\n",
    "print(Z1.shape, Z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 785]) torch.Size([4000, 785])\n",
      "Z1 shape in batch:  torch.Size([1, 1000, 785])\n",
      "Z2 shape in batch:  torch.Size([1, 4000, 785])\n",
      "1 1000 784\n",
      "torch.Size([1, 1000, 100])\n",
      "1 4000 784\n",
      "torch.Size([1, 4000, 100])\n",
      "torch.Size([1, 1000, 4000])\n",
      "torch.Size([1, 1000, 4000])\n",
      "Gia tri M: tensor([[[6, 6, 7,  ..., 7, 7, 7],\n",
      "         [2, 2, 3,  ..., 3, 3, 3],\n",
      "         [6, 6, 7,  ..., 7, 7, 7],\n",
      "         ...,\n",
      "         [6, 6, 7,  ..., 7, 7, 7],\n",
      "         [2, 2, 3,  ..., 3, 3, 3],\n",
      "         [6, 6, 7,  ..., 7, 7, 7]]], device='cuda:0')\n",
      "torch.Size([1, 1000, 4000])\n",
      "torch.Size([1, 1000, 4000])\n",
      "gia tri D: tensor([[[8.1817e+10, 2.5932e+12, 2.9280e+13,  ..., 1.6112e+13,\n",
      "          1.1389e+13, 1.6589e+13],\n",
      "         [9.0699e+12, 1.2609e+12, 7.1840e+12,  ..., 1.6479e+12,\n",
      "          4.1574e+11, 1.8018e+12],\n",
      "         [2.9469e+12, 3.3708e+10, 1.5806e+13,  ..., 6.6494e+12,\n",
      "          3.7614e+12, 6.9566e+12],\n",
      "         ...,\n",
      "         [3.2776e+13, 1.4712e+13, 2.0984e+09,  ..., 2.0478e+12,\n",
      "          4.2854e+12, 1.8837e+12],\n",
      "         [5.3718e+12, 1.8541e+11, 1.1385e+13,  ..., 3.9096e+12,\n",
      "          1.7906e+12, 4.1460e+12],\n",
      "         [3.3070e+12, 8.7188e+09, 1.5006e+13,  ..., 6.1343e+12,\n",
      "          3.3765e+12, 6.4294e+12]]], device='cuda:0')\n",
      "torch.Size([1, 1000, 4000])\n",
      "Z1 shape in batch:  torch.Size([1, 4000, 785])\n",
      "Z2 shape in batch:  torch.Size([1, 1000, 785])\n",
      "1 4000 784\n",
      "torch.Size([1, 4000, 100])\n",
      "1 1000 784\n",
      "torch.Size([1, 1000, 100])\n",
      "torch.Size([1, 4000, 1000])\n",
      "torch.Size([1, 4000, 1000])\n",
      "Gia tri M: tensor([[[ 9,  8,  9,  ...,  9,  8,  9],\n",
      "         [ 9,  8,  9,  ...,  9,  8,  9],\n",
      "         [13, 12, 13,  ..., 13, 12, 13],\n",
      "         ...,\n",
      "         [13, 12, 13,  ..., 13, 12, 13],\n",
      "         [13, 12, 13,  ..., 13, 12, 13],\n",
      "         [13, 12, 13,  ..., 13, 12, 13]]], device='cuda:0')\n",
      "torch.Size([1, 4000, 1000])\n",
      "torch.Size([1, 4000, 1000])\n",
      "gia tri D: tensor([[[8.1817e+10, 9.0699e+12, 2.9469e+12,  ..., 3.2776e+13,\n",
      "          5.3718e+12, 3.3070e+12],\n",
      "         [2.5932e+12, 1.2609e+12, 3.3708e+10,  ..., 1.4712e+13,\n",
      "          1.8541e+11, 8.7188e+09],\n",
      "         [2.9280e+13, 7.1840e+12, 1.5806e+13,  ..., 2.0984e+09,\n",
      "          1.1385e+13, 1.5006e+13],\n",
      "         ...,\n",
      "         [1.6112e+13, 1.6479e+12, 6.6494e+12,  ..., 2.0478e+12,\n",
      "          3.9096e+12, 6.1343e+12],\n",
      "         [1.1389e+13, 4.1574e+11, 3.7614e+12,  ..., 4.2854e+12,\n",
      "          1.7906e+12, 3.3765e+12],\n",
      "         [1.6589e+13, 1.8018e+12, 6.9566e+12,  ..., 1.8837e+12,\n",
      "          4.1460e+12, 6.4294e+12]]], device='cuda:0')\n",
      "torch.Size([1, 4000, 1000])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss.debias = False\n",
    "    loss.potentials = True\n",
    "    print(Z1.shape, Z2.shape)\n",
    "    F_i, G_j = loss(Z1, Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "π = [F_i, G_j]\n",
    "dual_sol = π\n",
    "for i in range(len(dual_sol)):\n",
    "    dual_sol[i] = dual_sol[i].to('cpu')\n",
    "calibrated_gradient = values(dual_sol, training_size)\n",
    "with open('calibrated_gradient.pkl', 'wb') as f:\n",
    "    pickle.dump(calibrated_gradient, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-105436540.0, 25310208.0, -153372000.0, 11471168.0, 206820480.0, 55940290.0, -161260160.0, 34693250.0, 10488832.0, 1445568.0, -142162110.0, -104090620.0, -9019072.0, 35934464.0, 27531392.0, -186393250.0, -124985380.0, 158693310.0, 77251580.0, 87436930.0, -128984510.0, -10882752.0, 83981440.0, -61718910.0, 70794430.0, 139254720.0, -127395710.0, 123487420.0, 28170496.0, -196940260.0, -79057860.0, 47008000.0, 13428416.0, 104150460.0, 51729470.0, 43572736.0, 40529790.0, -110372480.0, -177694200.0, -188921500.0, 25147648.0, 389410370.0, -187263740.0, -157860900.0, 33210368.0, 12548672.0, -179210720.0, 20706112.0, 52150144.0, -170180510.0, 33033600.0, -113388670.0, -62483200.0, 222385150.0, -78486590.0, -145819100.0, -181676740.0, -152524640.0, -150749380.0, 39723390.0, 377143870.0, -126252990.0, 919648400.0, -126964420.0, 48309056.0, 143271800.0, 130538560.0, 20231488.0, 27271552.0, -131344800.0, -125991940.0, 21103808.0, -160133380.0, 69509440.0, -150001700.0, -162275170.0, 471608830.0, -141778240.0, 77563070.0, -153614050.0, 42698690.0, -169892800.0, 42761280.0, -128875740.0, -188234620.0, 359141700.0, 105145980.0, 110170880.0, -181409920.0, -146193980.0, -142682530.0, 183327550.0, -123235230.0, -123979390.0, -128961280.0, -129695260.0, -159095650.0, -92901250.0, 92939710.0, 33198848.0, -122026430.0, 23838976.0, 58595520.0, 10195456.0, 15513664.0, -31581888.0, 31589504.0, 229996930.0, -155986560.0, 47885824.0, 12333440.0, -200388450.0, 152447490.0, 298914560.0, 259317310.0, -113662210.0, 85856700.0, -173729760.0, 19407360.0, -149080900.0, -180593800.0, -112647230.0, -123800260.0, -139598620.0, 1429824.0, 111904380.0, -138749310.0, 135477120.0, 40303550.0, -175006500.0, 17752320.0, 34700736.0, -176519620.0, 41657280.0, 128411650.0, 104448260.0, 57668670.0, 199019140.0, 28866304.0, 42668990.0, 34007360.0, -187329950.0, 62934016.0, -164499460.0, 77617540.0, 187762370.0, 35830336.0, 10875776.0, 27483328.0, -120361570.0, 71918980.0, -179685400.0, 18214464.0, 13789248.0, 24159424.0, -183254880.0, -36544000.0, -123284670.0, 95640700.0, -145846530.0, 29756416.0, 1502976.0, -179623400.0, -18647488.0, -151561540.0, -122196670.0, -163186500.0, -146231650.0, 26124032.0, 158798660.0, 381835070.0, -185279800.0, -164436320.0, 13841664.0, -108295620.0, 73824320.0, -101751170.0, 82816190.0, -97322750.0, -167615230.0, 90319620.0, 88899330.0, 7820032.0, 64293630.0, 102976320.0, -177478430.0, -142107490.0, -165140600.0, -164748130.0, 98879810.0, -164922820.0, 57625920.0, -141808900.0, 246292220.0, -151043970.0, -102149630.0, -179980930.0, -147475940.0, 65973824.0, -135993950.0, 291801280.0, 79646460.0, -138815840.0, 152325180.0, -167697950.0, 54243264.0, 81447620.0, 8842176.0, -167793120.0, 22382016.0, -116742460.0, 85556030.0, -150783100.0, -166977250.0, 34643710.0, -166648900.0, 2957312.0, 120129090.0, 3603904.0, 29692480.0, -85495940.0, 113421700.0, 20701696.0, -161681060.0, -131893150.0, 393472200.0, -179139680.0, 16169920.0, 65806144.0, -134623000.0, 8837696.0, -81313470.0, -113467650.0, -166130530.0, 29148608.0, -93080320.0, -146137090.0, -148509090.0, 18745600.0, 255080130.0, 13902336.0, -163407900.0, 47625856.0, 19824576.0, -146740380.0, -154847000.0, 583571840.0, 71235710.0, 42275970.0, 48863550.0, -131470780.0, 80499070.0, -145859070.0, 16077440.0, 75109700.0, -162238460.0, -1083328.0, 25645056.0, -129573120.0, -15313280.0, -179404600.0, -159118780.0, 82954180.0, 119396160.0, -151362850.0, 402850000.0, 109045380.0, 27849664.0, 15672192.0, -110697410.0, 16786944.0, 40853310.0, 76939970.0, -167076800.0, -169572160.0, -122637890.0, -157292960.0, -152183300.0, 190012930.0, 173469890.0, 4731456.0, 12204736.0, 75019840.0, 9315712.0, -117480740.0, -81100160.0, 237856640.0, 77414590.0, -159898430.0, -127137760.0, 29112960.0, -181006400.0, -104499970.0, -157638820.0, -114703420.0, -88998530.0, 139807230.0, 254174780.0, 206580220.0, 81337540.0, 29211968.0, 31721024.0, -136136450.0, -127150300.0, -158066500.0, 40304960.0, 22457216.0, -174945440.0, -157490690.0, 274029950.0, -142837280.0, -3583744.0, -166956960.0, -157959580.0, 181936380.0, -167267840.0, 82924990.0, 7892736.0, 71931070.0, -163583040.0, -149557150.0, 14579072.0, -123265790.0, -141898880.0, -139194460.0, 1361434100.0, -142858200.0, 27949312.0, -163410880.0, 573303200.0, -159615940.0, -64343104.0, -196114240.0, 254202370.0, 1192154400.0, -138015330.0, -151761660.0, 271410800.0, -171391420.0, -139394050.0, -146748960.0, -181564860.0, 29978496.0, 535015040.0, -194384580.0, 51789950.0, -148554690.0, 26894144.0, -135309470.0, -164677250.0, -173124130.0, -132806210.0, 19766144.0, 36366400.0, 26847808.0, -104739900.0, 52110464.0, 48994750.0, -138414880.0, -157275040.0, -167283170.0, -3507328.0, 63614336.0, -42603970.0, -157432960.0, 190520130.0, 111832190.0, -148381470.0, -102007360.0, 41840256.0, 114253950.0, -196465920.0, 56752510.0, 104133060.0, -148942660.0, 36387136.0, -142187230.0, 25324224.0, 5471552.0, -135803400.0, -82326210.0, 587190400.0, -120827780.0, 79996990.0, -61014656.0, -115503550.0, -148580320.0, 76146180.0, -160926530.0, 32427264.0, -8885568.0, 114829700.0, 37811390.0, 89331710.0, 30541568.0, 13457984.0, 129196540.0, 96979390.0, -139689250.0, -167032770.0, -170588830.0, -160275780.0, -157447900.0, -120307740.0, 146358780.0, -142175040.0, 28813696.0, -175810050.0, 73236930.0, -166610080.0, -134066910.0, 94720450.0, 24654464.0, -149907360.0, -133048320.0, -135240060.0, -13811648.0, 41401344.0, 61702720.0, -136084130.0, 56505470.0, -156726140.0, -155232640.0, -157322600.0, -153192100.0, 160014140.0, 12652480.0, -161461120.0, 435132160.0, 134709630.0, -181603900.0, 54689150.0, 29144256.0, 182421180.0, -142488030.0, 75102140.0, 36126400.0, -140461800.0, 48793790.0, -104576900.0, -127452450.0, 68271740.0, 131004930.0, 9395840.0, 284226370.0, 12666176.0, -123474370.0, 40691776.0, 181253380.0, -150378560.0, 89782910.0, 47234750.0, 55221310.0, -182427580.0, 50538176.0, 44105024.0, 208187460.0, -113251780.0, -136159360.0, 47463744.0, -106456700.0, 206900860.0, -151275800.0, -125987140.0, 35482176.0, -151661540.0, -166703200.0, -82217220.0, -163146530.0, 216782270.0, 260055550.0, 6918848.0, -126823040.0, -5037056.0, 13935040.0, 47277890.0, 41433664.0, 24461120.0, 181642180.0, 112168320.0, -114246530.0, 26177856.0, 94848960.0, 10621632.0, 36205504.0, -139866600.0, -102865660.0, 54464.0, -150853820.0, -157298400.0, 14741824.0, 106525890.0, 30983552.0, -167313150.0, -127361630.0, 119404290.0, 26044992.0, 32830080.0, -27947520.0, 154886530.0, 59532544.0, -187872030.0, -70905020.0, -162493500.0, -9198400.0, -129511970.0, 441704580.0, -140206880.0, 45134976.0, -158956290.0, -148620580.0, -166675680.0, 309764860.0, 23177408.0, 39023040.0, -158148350.0, -123036700.0, 28129536.0, -102584960.0, -150355070.0, -101154370.0, -184246020.0, 37671490.0, -151297000.0, 125284290.0, 81020220.0, -171555170.0, 302172540.0, 13414464.0, -72741250.0, 194992830.0, 39678336.0, 228550780.0, 269212540.0, 53658240.0, -129973440.0, 20614848.0, 87077500.0, -164944000.0, 19043520.0, -131614110.0, 42186880.0, -164155360.0, -165734050.0, 176057860.0, 170905280.0, -135682940.0, 413354000.0, 138389380.0, 42277310.0, -146732100.0, -129083740.0, -184217820.0, 300880830.0, -136632060.0, -10462272.0, -114176130.0, 332948160.0, 45641410.0, 118537920.0, 648020860.0, -131701950.0, 282598980.0, -145057600.0, -1680256.0, -161299680.0, -166706780.0, 92227580.0, 32790272.0, -276160.0, 469521920.0, -172256600.0, -7244928.0, -175078020.0, -70436930.0, 67448960.0, 222217980.0, -9103936.0, -181015400.0, -177534300.0, 7202432.0, -97817790.0, -160501250.0, -70135940.0, -86245440.0, 21449856.0, 163574270.0, -154112380.0, -67352260.0, 79834180.0, -149403520.0, -160743200.0, -62941376.0, -61049536.0, -174346370.0, -171447100.0, -173961150.0, 11864576.0, 16953472.0, 125604800.0, 220013700.0, 194900860.0, 95823490.0, 93310020.0, -99526910.0, 94077060.0, 15495872.0, -115680770.0, 72143870.0, 118061310.0, -149453820.0, -80953280.0, 56078400.0, 85120060.0, -187627710.0, -122700900.0, 19746048.0, 120101760.0, -129156700.0, -169380380.0, -98068160.0, -178026560.0, -94785090.0, -7319936.0, 42707584.0, 6079616.0, -141165980.0, -89817600.0, -108735170.0, -175204510.0, 95262720.0, 25536384.0, 62789504.0, 612968800.0, 312339400.0, -160542460.0, 33838210.0, -163195800.0, 302970300.0, 176153730.0, 110369730.0, 27444608.0, 552971000.0, -110344450.0, 418872130.0, -120589440.0, -105468480.0, -159461310.0, 61962496.0, 4322112.0, -95354560.0, -174898020.0, -172785150.0, -94755260.0, -147118110.0, -138030850.0, 236040260.0, 20823936.0, -183558050.0, 12200640.0, -136523550.0, -168611550.0, -134605540.0, -176429400.0, -171105180.0, 26802112.0, 75059780.0, -169097730.0, 29900224.0, 15380416.0, -163183260.0, -3563072.0, 593849200.0, -12309184.0, -172586850.0, -156572960.0, 87021700.0, 85761340.0, -153379680.0, -121192670.0, -157142270.0, -164564130.0, 88192260.0, -56039616.0, -157069000.0, -137423100.0, 24342848.0, 151329220.0, 7416444000.0, 180343680.0, 65007870.0, 5647680.0, 16360192.0, -184791680.0, 57953470.0, -18851456.0, 15447872.0, -113035580.0, 583976700.0, -113598210.0, -166657000.0, 48474430.0, 46182080.0, -131012930.0, 33858176.0, -186642720.0, 73466750.0, 65240704.0, 186084860.0, 54263360.0, -132427740.0, -144956640.0, -139105280.0, -147831710.0, 35351616.0, 217233090.0, 162277440.0, -93778750.0, -8457536.0, -140756380.0, 155512380.0, -106070140.0, -153653760.0, -172019230.0, 111210690.0, 64943870.0, -117642660.0, 264403400.0, 60374910.0, -171123620.0, -42832960.0, 65339520.0, 45359170.0, 105630140.0, 35522370.0, 82380350.0, -145192600.0, 119052100.0, 31904704.0, 22238528.0, 82767620.0, -157371870.0, -118062750.0, 229934780.0, -62605376.0, -5312768.0, -155275260.0, -144662180.0, 100494400.0, 397199170.0, 27036864.0, 384530240.0, 72132160.0, -111069820.0, -172639100.0, 33914110.0, 24125568.0, -183403580.0, -127236420.0, 22430784.0, -84256260.0, -109615100.0, -147052960.0, 13408320.0, -94718340.0, -149915940.0, 20614848.0, -75934720.0, -138632800.0, 4382592.0, 57995650.0, -142490750.0, -161317730.0, -10952576.0, 57242240.0, 194401730.0, 212608.0, 36948990.0, 37330944.0, 87272260.0, 35533760.0, 37083330.0, 383166200.0, 159677120.0, -145186910.0, 1154944.0, -182838500.0, 12810176.0, 407853630.0, -106964290.0, -7035456.0, -150844200.0, -94222530.0, 693147800.0, -162462850.0, 157739520.0, 31074240.0, -141286690.0, 100305790.0, -126503230.0, 47217856.0, 207988600.0, -150474050.0, 334077000.0, 80324610.0, 417549630.0, -146822180.0, -102370690.0, 435987700.0, -180058430.0, -187483400.0, -165042050.0, -168173660.0, 74717950.0, -146260100.0, 12693632.0, -158667360.0, -70242110.0, 95687870.0, -182065310.0, 6802624.0, 150674240.0, -156284510.0, -195655360.0, 117355780.0, 92711040.0, 57247936.0, -184487460.0, 28071104.0, 58995070.0, 50180800.0, -86725250.0, -171461730.0, 508673540.0, 5368000.0, 49009470.0, -136430270.0, -161994460.0, 18253568.0, 16272384.0, 9867968.0, 33275328.0, 15230592.0, 72636290.0, -176323740.0, 30820416.0, -147241120.0, 47960830.0, -154375140.0, -162193440.0, -161411680.0, -156969980.0, -64491456.0, 976954600.0, 240256580.0, -84653570.0, 17145472.0, -148150340.0, -129646240.0, -156502940.0, 75274880.0, 40597056.0, 415857540.0, -158474980.0, -104335620.0, -68464380.0, 168742460.0, -171810560.0, -184379620.0, -175411550.0, 174730430.0, -146330850.0, -172538240.0, -186134270.0, 152915840.0, -128964160.0, -158427680.0, -155430240.0, -154407260.0, 18076096.0, 27633472.0, 990569600.0, -138224380.0, -99988930.0, -168989020.0, 20126528.0, -134318460.0, -53531650.0, -169030530.0, -137066110.0, 13056320.0, -116234820.0, 213904830.0, -148829700.0, 78830270.0, 83324860.0, -122018240.0, -158642400.0, 88980420.0, -118283550.0, 381857470.0, 181611650.0, -155633700.0, 350674300.0, -165853700.0, -169369660.0, 41267070.0, -161504860.0, -125857470.0, 45834560.0, -159892420.0, 38794880.0, 153371400.0, 24458880.0, 163215680.0, -2237504.0, 72383420.0, 41773440.0, -88613060.0, 1998528.0, -21368576.0, -184759000.0, 79550720.0, -186660510.0, -160116960.0, -125802300.0, 38955010.0, 219538050.0, 6836480.0, 39428160.0, 37924670.0, -134457380.0, -94544130.0, 184303550.0, 427278340.0, -131396190.0, -168339800.0, -72398590.0, 36213250.0, -174477920.0, 6703232.0, -118244190.0, -147436320.0, -175172030.0, 90930240.0, -87699900.0, -137057860.0, 81356990.0, 26318464.0, 334902080.0, -145697800.0, 53075710.0, -144975040.0, -128048960.0, -127546080.0, 322528500.0, -180030240.0, 6855936.0, 204020100.0, 27333568.0, -168003000.0, 22575104.0, 68607300.0, -166790240.0, -13429952.0, 34976256.0, 178943300.0, -151017730.0, -152262200.0, -157851360.0, 41008770.0, -124287840.0, 31738816.0, 77604480.0, 100084420.0, -170965090.0, -128589860.0, 35584704.0, 42443776.0, -80297600.0, -101072320.0, -25139008.0, 643953000.0, 404180600.0, 191040.0, -114964290.0, 24350464.0, 7891584.0, 111036610.0, 70855100.0, -145057000.0, -168113180.0, 30126272.0, -155948640.0, -128049920.0, -150456220.0, -157291420.0, -131550980.0, 55829250.0, -114528160.0]\n"
     ]
    }
   ],
   "source": [
    "with open('calibrated_gradient.pkl', 'rb') as f:\n",
    "    loaded = pickle.load(f)\n",
    "print(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trích xuất đặc trưng từ các tập dữ liệu\n",
    "# Hàm để trích xuất đặc trưng từ X\n",
    "def extract_features(net, X):\n",
    "    X = torch.tensor(X).float().unsqueeze(1)  # Thêm chiều để có kích thước [N, 1, 28, 28]\n",
    "    X = X.repeat(1, 3, 1, 1).to(device)  # Chuyển thành [N, 3, 28, 28]\n",
    "    with torch.no_grad():\n",
    "        features = net(X)\n",
    "    return features.cpu().numpy()\n",
    "X_train_features = extract_features(net_test, X_train_balanced)\n",
    "X_test_features = extract_features(net_test, X_test_balanced)\n",
    "\n",
    "# Lưu các đặc trưng đã trích xuất\n",
    "with open('balanced_train_features.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train_features, y_train_balanced), f)\n",
    "with open('balanced_test_features.pkl', 'wb') as f:\n",
    "    pickle.dump((X_test_features, y_test_balanced), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_features_loaded shape: (1000, 100)\n",
      "y_train_loaded shape: (1000,)\n",
      "X_test_features_loaded shape: (4000, 100)\n",
      "y_test_loaded shape: (4000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load lại các đặc trưng đã lưu cho tập huấn luyện\n",
    "with open('balanced_train_features.pkl', 'rb') as f:\n",
    "    X_train_features_loaded, y_train_loaded = pickle.load(f)\n",
    "\n",
    "\n",
    "# Load lại các đặc trưng đã lưu cho tập kiểm tra (test)\n",
    "with open('balanced_test_features.pkl', 'rb') as f:\n",
    "    X_test_features_loaded, y_test_loaded = pickle.load(f)\n",
    "\n",
    "# Kiểm tra kích thước để đảm bảo rằng dữ liệu đã được tải đúng\n",
    "print(f\"X_train_features_loaded shape: {X_train_features_loaded.shape}\")\n",
    "print(f\"y_train_loaded shape: {y_train_loaded.shape}\")\n",
    "print(f\"X_test_features_loaded shape: {X_test_features_loaded.shape}\")\n",
    "print(f\"y_test_loaded shape: {y_test_loaded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_features_loaded)\n",
    "X_test_scaled = scaler.transform(X_test_features_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -105436540.0, 1: 25310208.0, 2: -153372000.0, 3: 11471168.0, 4: 206820480.0, 5: 55940290.0, 6: -161260160.0, 7: 34693250.0, 8: 10488832.0, 9: 1445568.0, 10: -142162110.0, 11: -104090620.0, 12: -9019072.0, 13: 35934464.0, 14: 27531392.0, 15: -186393250.0, 16: -124985380.0, 17: 158693310.0, 18: 77251580.0, 19: 87436930.0, 20: -128984510.0, 21: -10882752.0, 22: 83981440.0, 23: -61718910.0, 24: 70794430.0, 25: 139254720.0, 26: -127395710.0, 27: 123487420.0, 28: 28170496.0, 29: -196940260.0, 30: -79057860.0, 31: 47008000.0, 32: 13428416.0, 33: 104150460.0, 34: 51729470.0, 35: 43572736.0, 36: 40529790.0, 37: -110372480.0, 38: -177694200.0, 39: -188921500.0, 40: 25147648.0, 41: 389410370.0, 42: -187263740.0, 43: -157860900.0, 44: 33210368.0, 45: 12548672.0, 46: -179210720.0, 47: 20706112.0, 48: 52150144.0, 49: -170180510.0, 50: 33033600.0, 51: -113388670.0, 52: -62483200.0, 53: 222385150.0, 54: -78486590.0, 55: -145819100.0, 56: -181676740.0, 57: -152524640.0, 58: -150749380.0, 59: 39723390.0, 60: 377143870.0, 61: -126252990.0, 62: 919648400.0, 63: -126964420.0, 64: 48309056.0, 65: 143271800.0, 66: 130538560.0, 67: 20231488.0, 68: 27271552.0, 69: -131344800.0, 70: -125991940.0, 71: 21103808.0, 72: -160133380.0, 73: 69509440.0, 74: -150001700.0, 75: -162275170.0, 76: 471608830.0, 77: -141778240.0, 78: 77563070.0, 79: -153614050.0, 80: 42698690.0, 81: -169892800.0, 82: 42761280.0, 83: -128875740.0, 84: -188234620.0, 85: 359141700.0, 86: 105145980.0, 87: 110170880.0, 88: -181409920.0, 89: -146193980.0, 90: -142682530.0, 91: 183327550.0, 92: -123235230.0, 93: -123979390.0, 94: -128961280.0, 95: -129695260.0, 96: -159095650.0, 97: -92901250.0, 98: 92939710.0, 99: 33198848.0, 100: -122026430.0, 101: 23838976.0, 102: 58595520.0, 103: 10195456.0, 104: 15513664.0, 105: -31581888.0, 106: 31589504.0, 107: 229996930.0, 108: -155986560.0, 109: 47885824.0, 110: 12333440.0, 111: -200388450.0, 112: 152447490.0, 113: 298914560.0, 114: 259317310.0, 115: -113662210.0, 116: 85856700.0, 117: -173729760.0, 118: 19407360.0, 119: -149080900.0, 120: -180593800.0, 121: -112647230.0, 122: -123800260.0, 123: -139598620.0, 124: 1429824.0, 125: 111904380.0, 126: -138749310.0, 127: 135477120.0, 128: 40303550.0, 129: -175006500.0, 130: 17752320.0, 131: 34700736.0, 132: -176519620.0, 133: 41657280.0, 134: 128411650.0, 135: 104448260.0, 136: 57668670.0, 137: 199019140.0, 138: 28866304.0, 139: 42668990.0, 140: 34007360.0, 141: -187329950.0, 142: 62934016.0, 143: -164499460.0, 144: 77617540.0, 145: 187762370.0, 146: 35830336.0, 147: 10875776.0, 148: 27483328.0, 149: -120361570.0, 150: 71918980.0, 151: -179685400.0, 152: 18214464.0, 153: 13789248.0, 154: 24159424.0, 155: -183254880.0, 156: -36544000.0, 157: -123284670.0, 158: 95640700.0, 159: -145846530.0, 160: 29756416.0, 161: 1502976.0, 162: -179623400.0, 163: -18647488.0, 164: -151561540.0, 165: -122196670.0, 166: -163186500.0, 167: -146231650.0, 168: 26124032.0, 169: 158798660.0, 170: 381835070.0, 171: -185279800.0, 172: -164436320.0, 173: 13841664.0, 174: -108295620.0, 175: 73824320.0, 176: -101751170.0, 177: 82816190.0, 178: -97322750.0, 179: -167615230.0, 180: 90319620.0, 181: 88899330.0, 182: 7820032.0, 183: 64293630.0, 184: 102976320.0, 185: -177478430.0, 186: -142107490.0, 187: -165140600.0, 188: -164748130.0, 189: 98879810.0, 190: -164922820.0, 191: 57625920.0, 192: -141808900.0, 193: 246292220.0, 194: -151043970.0, 195: -102149630.0, 196: -179980930.0, 197: -147475940.0, 198: 65973824.0, 199: -135993950.0, 200: 291801280.0, 201: 79646460.0, 202: -138815840.0, 203: 152325180.0, 204: -167697950.0, 205: 54243264.0, 206: 81447620.0, 207: 8842176.0, 208: -167793120.0, 209: 22382016.0, 210: -116742460.0, 211: 85556030.0, 212: -150783100.0, 213: -166977250.0, 214: 34643710.0, 215: -166648900.0, 216: 2957312.0, 217: 120129090.0, 218: 3603904.0, 219: 29692480.0, 220: -85495940.0, 221: 113421700.0, 222: 20701696.0, 223: -161681060.0, 224: -131893150.0, 225: 393472200.0, 226: -179139680.0, 227: 16169920.0, 228: 65806144.0, 229: -134623000.0, 230: 8837696.0, 231: -81313470.0, 232: -113467650.0, 233: -166130530.0, 234: 29148608.0, 235: -93080320.0, 236: -146137090.0, 237: -148509090.0, 238: 18745600.0, 239: 255080130.0, 240: 13902336.0, 241: -163407900.0, 242: 47625856.0, 243: 19824576.0, 244: -146740380.0, 245: -154847000.0, 246: 583571840.0, 247: 71235710.0, 248: 42275970.0, 249: 48863550.0, 250: -131470780.0, 251: 80499070.0, 252: -145859070.0, 253: 16077440.0, 254: 75109700.0, 255: -162238460.0, 256: -1083328.0, 257: 25645056.0, 258: -129573120.0, 259: -15313280.0, 260: -179404600.0, 261: -159118780.0, 262: 82954180.0, 263: 119396160.0, 264: -151362850.0, 265: 402850000.0, 266: 109045380.0, 267: 27849664.0, 268: 15672192.0, 269: -110697410.0, 270: 16786944.0, 271: 40853310.0, 272: 76939970.0, 273: -167076800.0, 274: -169572160.0, 275: -122637890.0, 276: -157292960.0, 277: -152183300.0, 278: 190012930.0, 279: 173469890.0, 280: 4731456.0, 281: 12204736.0, 282: 75019840.0, 283: 9315712.0, 284: -117480740.0, 285: -81100160.0, 286: 237856640.0, 287: 77414590.0, 288: -159898430.0, 289: -127137760.0, 290: 29112960.0, 291: -181006400.0, 292: -104499970.0, 293: -157638820.0, 294: -114703420.0, 295: -88998530.0, 296: 139807230.0, 297: 254174780.0, 298: 206580220.0, 299: 81337540.0, 300: 29211968.0, 301: 31721024.0, 302: -136136450.0, 303: -127150300.0, 304: -158066500.0, 305: 40304960.0, 306: 22457216.0, 307: -174945440.0, 308: -157490690.0, 309: 274029950.0, 310: -142837280.0, 311: -3583744.0, 312: -166956960.0, 313: -157959580.0, 314: 181936380.0, 315: -167267840.0, 316: 82924990.0, 317: 7892736.0, 318: 71931070.0, 319: -163583040.0, 320: -149557150.0, 321: 14579072.0, 322: -123265790.0, 323: -141898880.0, 324: -139194460.0, 325: 1361434100.0, 326: -142858200.0, 327: 27949312.0, 328: -163410880.0, 329: 573303200.0, 330: -159615940.0, 331: -64343104.0, 332: -196114240.0, 333: 254202370.0, 334: 1192154400.0, 335: -138015330.0, 336: -151761660.0, 337: 271410800.0, 338: -171391420.0, 339: -139394050.0, 340: -146748960.0, 341: -181564860.0, 342: 29978496.0, 343: 535015040.0, 344: -194384580.0, 345: 51789950.0, 346: -148554690.0, 347: 26894144.0, 348: -135309470.0, 349: -164677250.0, 350: -173124130.0, 351: -132806210.0, 352: 19766144.0, 353: 36366400.0, 354: 26847808.0, 355: -104739900.0, 356: 52110464.0, 357: 48994750.0, 358: -138414880.0, 359: -157275040.0, 360: -167283170.0, 361: -3507328.0, 362: 63614336.0, 363: -42603970.0, 364: -157432960.0, 365: 190520130.0, 366: 111832190.0, 367: -148381470.0, 368: -102007360.0, 369: 41840256.0, 370: 114253950.0, 371: -196465920.0, 372: 56752510.0, 373: 104133060.0, 374: -148942660.0, 375: 36387136.0, 376: -142187230.0, 377: 25324224.0, 378: 5471552.0, 379: -135803400.0, 380: -82326210.0, 381: 587190400.0, 382: -120827780.0, 383: 79996990.0, 384: -61014656.0, 385: -115503550.0, 386: -148580320.0, 387: 76146180.0, 388: -160926530.0, 389: 32427264.0, 390: -8885568.0, 391: 114829700.0, 392: 37811390.0, 393: 89331710.0, 394: 30541568.0, 395: 13457984.0, 396: 129196540.0, 397: 96979390.0, 398: -139689250.0, 399: -167032770.0, 400: -170588830.0, 401: -160275780.0, 402: -157447900.0, 403: -120307740.0, 404: 146358780.0, 405: -142175040.0, 406: 28813696.0, 407: -175810050.0, 408: 73236930.0, 409: -166610080.0, 410: -134066910.0, 411: 94720450.0, 412: 24654464.0, 413: -149907360.0, 414: -133048320.0, 415: -135240060.0, 416: -13811648.0, 417: 41401344.0, 418: 61702720.0, 419: -136084130.0, 420: 56505470.0, 421: -156726140.0, 422: -155232640.0, 423: -157322600.0, 424: -153192100.0, 425: 160014140.0, 426: 12652480.0, 427: -161461120.0, 428: 435132160.0, 429: 134709630.0, 430: -181603900.0, 431: 54689150.0, 432: 29144256.0, 433: 182421180.0, 434: -142488030.0, 435: 75102140.0, 436: 36126400.0, 437: -140461800.0, 438: 48793790.0, 439: -104576900.0, 440: -127452450.0, 441: 68271740.0, 442: 131004930.0, 443: 9395840.0, 444: 284226370.0, 445: 12666176.0, 446: -123474370.0, 447: 40691776.0, 448: 181253380.0, 449: -150378560.0, 450: 89782910.0, 451: 47234750.0, 452: 55221310.0, 453: -182427580.0, 454: 50538176.0, 455: 44105024.0, 456: 208187460.0, 457: -113251780.0, 458: -136159360.0, 459: 47463744.0, 460: -106456700.0, 461: 206900860.0, 462: -151275800.0, 463: -125987140.0, 464: 35482176.0, 465: -151661540.0, 466: -166703200.0, 467: -82217220.0, 468: -163146530.0, 469: 216782270.0, 470: 260055550.0, 471: 6918848.0, 472: -126823040.0, 473: -5037056.0, 474: 13935040.0, 475: 47277890.0, 476: 41433664.0, 477: 24461120.0, 478: 181642180.0, 479: 112168320.0, 480: -114246530.0, 481: 26177856.0, 482: 94848960.0, 483: 10621632.0, 484: 36205504.0, 485: -139866600.0, 486: -102865660.0, 487: 54464.0, 488: -150853820.0, 489: -157298400.0, 490: 14741824.0, 491: 106525890.0, 492: 30983552.0, 493: -167313150.0, 494: -127361630.0, 495: 119404290.0, 496: 26044992.0, 497: 32830080.0, 498: -27947520.0, 499: 154886530.0, 500: 59532544.0, 501: -187872030.0, 502: -70905020.0, 503: -162493500.0, 504: -9198400.0, 505: -129511970.0, 506: 441704580.0, 507: -140206880.0, 508: 45134976.0, 509: -158956290.0, 510: -148620580.0, 511: -166675680.0, 512: 309764860.0, 513: 23177408.0, 514: 39023040.0, 515: -158148350.0, 516: -123036700.0, 517: 28129536.0, 518: -102584960.0, 519: -150355070.0, 520: -101154370.0, 521: -184246020.0, 522: 37671490.0, 523: -151297000.0, 524: 125284290.0, 525: 81020220.0, 526: -171555170.0, 527: 302172540.0, 528: 13414464.0, 529: -72741250.0, 530: 194992830.0, 531: 39678336.0, 532: 228550780.0, 533: 269212540.0, 534: 53658240.0, 535: -129973440.0, 536: 20614848.0, 537: 87077500.0, 538: -164944000.0, 539: 19043520.0, 540: -131614110.0, 541: 42186880.0, 542: -164155360.0, 543: -165734050.0, 544: 176057860.0, 545: 170905280.0, 546: -135682940.0, 547: 413354000.0, 548: 138389380.0, 549: 42277310.0, 550: -146732100.0, 551: -129083740.0, 552: -184217820.0, 553: 300880830.0, 554: -136632060.0, 555: -10462272.0, 556: -114176130.0, 557: 332948160.0, 558: 45641410.0, 559: 118537920.0, 560: 648020860.0, 561: -131701950.0, 562: 282598980.0, 563: -145057600.0, 564: -1680256.0, 565: -161299680.0, 566: -166706780.0, 567: 92227580.0, 568: 32790272.0, 569: -276160.0, 570: 469521920.0, 571: -172256600.0, 572: -7244928.0, 573: -175078020.0, 574: -70436930.0, 575: 67448960.0, 576: 222217980.0, 577: -9103936.0, 578: -181015400.0, 579: -177534300.0, 580: 7202432.0, 581: -97817790.0, 582: -160501250.0, 583: -70135940.0, 584: -86245440.0, 585: 21449856.0, 586: 163574270.0, 587: -154112380.0, 588: -67352260.0, 589: 79834180.0, 590: -149403520.0, 591: -160743200.0, 592: -62941376.0, 593: -61049536.0, 594: -174346370.0, 595: -171447100.0, 596: -173961150.0, 597: 11864576.0, 598: 16953472.0, 599: 125604800.0, 600: 220013700.0, 601: 194900860.0, 602: 95823490.0, 603: 93310020.0, 604: -99526910.0, 605: 94077060.0, 606: 15495872.0, 607: -115680770.0, 608: 72143870.0, 609: 118061310.0, 610: -149453820.0, 611: -80953280.0, 612: 56078400.0, 613: 85120060.0, 614: -187627710.0, 615: -122700900.0, 616: 19746048.0, 617: 120101760.0, 618: -129156700.0, 619: -169380380.0, 620: -98068160.0, 621: -178026560.0, 622: -94785090.0, 623: -7319936.0, 624: 42707584.0, 625: 6079616.0, 626: -141165980.0, 627: -89817600.0, 628: -108735170.0, 629: -175204510.0, 630: 95262720.0, 631: 25536384.0, 632: 62789504.0, 633: 612968800.0, 634: 312339400.0, 635: -160542460.0, 636: 33838210.0, 637: -163195800.0, 638: 302970300.0, 639: 176153730.0, 640: 110369730.0, 641: 27444608.0, 642: 552971000.0, 643: -110344450.0, 644: 418872130.0, 645: -120589440.0, 646: -105468480.0, 647: -159461310.0, 648: 61962496.0, 649: 4322112.0, 650: -95354560.0, 651: -174898020.0, 652: -172785150.0, 653: -94755260.0, 654: -147118110.0, 655: -138030850.0, 656: 236040260.0, 657: 20823936.0, 658: -183558050.0, 659: 12200640.0, 660: -136523550.0, 661: -168611550.0, 662: -134605540.0, 663: -176429400.0, 664: -171105180.0, 665: 26802112.0, 666: 75059780.0, 667: -169097730.0, 668: 29900224.0, 669: 15380416.0, 670: -163183260.0, 671: -3563072.0, 672: 593849200.0, 673: -12309184.0, 674: -172586850.0, 675: -156572960.0, 676: 87021700.0, 677: 85761340.0, 678: -153379680.0, 679: -121192670.0, 680: -157142270.0, 681: -164564130.0, 682: 88192260.0, 683: -56039616.0, 684: -157069000.0, 685: -137423100.0, 686: 24342848.0, 687: 151329220.0, 688: 7416444000.0, 689: 180343680.0, 690: 65007870.0, 691: 5647680.0, 692: 16360192.0, 693: -184791680.0, 694: 57953470.0, 695: -18851456.0, 696: 15447872.0, 697: -113035580.0, 698: 583976700.0, 699: -113598210.0, 700: -166657000.0, 701: 48474430.0, 702: 46182080.0, 703: -131012930.0, 704: 33858176.0, 705: -186642720.0, 706: 73466750.0, 707: 65240704.0, 708: 186084860.0, 709: 54263360.0, 710: -132427740.0, 711: -144956640.0, 712: -139105280.0, 713: -147831710.0, 714: 35351616.0, 715: 217233090.0, 716: 162277440.0, 717: -93778750.0, 718: -8457536.0, 719: -140756380.0, 720: 155512380.0, 721: -106070140.0, 722: -153653760.0, 723: -172019230.0, 724: 111210690.0, 725: 64943870.0, 726: -117642660.0, 727: 264403400.0, 728: 60374910.0, 729: -171123620.0, 730: -42832960.0, 731: 65339520.0, 732: 45359170.0, 733: 105630140.0, 734: 35522370.0, 735: 82380350.0, 736: -145192600.0, 737: 119052100.0, 738: 31904704.0, 739: 22238528.0, 740: 82767620.0, 741: -157371870.0, 742: -118062750.0, 743: 229934780.0, 744: -62605376.0, 745: -5312768.0, 746: -155275260.0, 747: -144662180.0, 748: 100494400.0, 749: 397199170.0, 750: 27036864.0, 751: 384530240.0, 752: 72132160.0, 753: -111069820.0, 754: -172639100.0, 755: 33914110.0, 756: 24125568.0, 757: -183403580.0, 758: -127236420.0, 759: 22430784.0, 760: -84256260.0, 761: -109615100.0, 762: -147052960.0, 763: 13408320.0, 764: -94718340.0, 765: -149915940.0, 766: 20614848.0, 767: -75934720.0, 768: -138632800.0, 769: 4382592.0, 770: 57995650.0, 771: -142490750.0, 772: -161317730.0, 773: -10952576.0, 774: 57242240.0, 775: 194401730.0, 776: 212608.0, 777: 36948990.0, 778: 37330944.0, 779: 87272260.0, 780: 35533760.0, 781: 37083330.0, 782: 383166200.0, 783: 159677120.0, 784: -145186910.0, 785: 1154944.0, 786: -182838500.0, 787: 12810176.0, 788: 407853630.0, 789: -106964290.0, 790: -7035456.0, 791: -150844200.0, 792: -94222530.0, 793: 693147800.0, 794: -162462850.0, 795: 157739520.0, 796: 31074240.0, 797: -141286690.0, 798: 100305790.0, 799: -126503230.0, 800: 47217856.0, 801: 207988600.0, 802: -150474050.0, 803: 334077000.0, 804: 80324610.0, 805: 417549630.0, 806: -146822180.0, 807: -102370690.0, 808: 435987700.0, 809: -180058430.0, 810: -187483400.0, 811: -165042050.0, 812: -168173660.0, 813: 74717950.0, 814: -146260100.0, 815: 12693632.0, 816: -158667360.0, 817: -70242110.0, 818: 95687870.0, 819: -182065310.0, 820: 6802624.0, 821: 150674240.0, 822: -156284510.0, 823: -195655360.0, 824: 117355780.0, 825: 92711040.0, 826: 57247936.0, 827: -184487460.0, 828: 28071104.0, 829: 58995070.0, 830: 50180800.0, 831: -86725250.0, 832: -171461730.0, 833: 508673540.0, 834: 5368000.0, 835: 49009470.0, 836: -136430270.0, 837: -161994460.0, 838: 18253568.0, 839: 16272384.0, 840: 9867968.0, 841: 33275328.0, 842: 15230592.0, 843: 72636290.0, 844: -176323740.0, 845: 30820416.0, 846: -147241120.0, 847: 47960830.0, 848: -154375140.0, 849: -162193440.0, 850: -161411680.0, 851: -156969980.0, 852: -64491456.0, 853: 976954600.0, 854: 240256580.0, 855: -84653570.0, 856: 17145472.0, 857: -148150340.0, 858: -129646240.0, 859: -156502940.0, 860: 75274880.0, 861: 40597056.0, 862: 415857540.0, 863: -158474980.0, 864: -104335620.0, 865: -68464380.0, 866: 168742460.0, 867: -171810560.0, 868: -184379620.0, 869: -175411550.0, 870: 174730430.0, 871: -146330850.0, 872: -172538240.0, 873: -186134270.0, 874: 152915840.0, 875: -128964160.0, 876: -158427680.0, 877: -155430240.0, 878: -154407260.0, 879: 18076096.0, 880: 27633472.0, 881: 990569600.0, 882: -138224380.0, 883: -99988930.0, 884: -168989020.0, 885: 20126528.0, 886: -134318460.0, 887: -53531650.0, 888: -169030530.0, 889: -137066110.0, 890: 13056320.0, 891: -116234820.0, 892: 213904830.0, 893: -148829700.0, 894: 78830270.0, 895: 83324860.0, 896: -122018240.0, 897: -158642400.0, 898: 88980420.0, 899: -118283550.0, 900: 381857470.0, 901: 181611650.0, 902: -155633700.0, 903: 350674300.0, 904: -165853700.0, 905: -169369660.0, 906: 41267070.0, 907: -161504860.0, 908: -125857470.0, 909: 45834560.0, 910: -159892420.0, 911: 38794880.0, 912: 153371400.0, 913: 24458880.0, 914: 163215680.0, 915: -2237504.0, 916: 72383420.0, 917: 41773440.0, 918: -88613060.0, 919: 1998528.0, 920: -21368576.0, 921: -184759000.0, 922: 79550720.0, 923: -186660510.0, 924: -160116960.0, 925: -125802300.0, 926: 38955010.0, 927: 219538050.0, 928: 6836480.0, 929: 39428160.0, 930: 37924670.0, 931: -134457380.0, 932: -94544130.0, 933: 184303550.0, 934: 427278340.0, 935: -131396190.0, 936: -168339800.0, 937: -72398590.0, 938: 36213250.0, 939: -174477920.0, 940: 6703232.0, 941: -118244190.0, 942: -147436320.0, 943: -175172030.0, 944: 90930240.0, 945: -87699900.0, 946: -137057860.0, 947: 81356990.0, 948: 26318464.0, 949: 334902080.0, 950: -145697800.0, 951: 53075710.0, 952: -144975040.0, 953: -128048960.0, 954: -127546080.0, 955: 322528500.0, 956: -180030240.0, 957: 6855936.0, 958: 204020100.0, 959: 27333568.0, 960: -168003000.0, 961: 22575104.0, 962: 68607300.0, 963: -166790240.0, 964: -13429952.0, 965: 34976256.0, 966: 178943300.0, 967: -151017730.0, 968: -152262200.0, 969: -157851360.0, 970: 41008770.0, 971: -124287840.0, 972: 31738816.0, 973: 77604480.0, 974: 100084420.0, 975: -170965090.0, 976: -128589860.0, 977: 35584704.0, 978: 42443776.0, 979: -80297600.0, 980: -101072320.0, 981: -25139008.0, 982: 643953000.0, 983: 404180600.0, 984: 191040.0, 985: -114964290.0, 986: 24350464.0, 987: 7891584.0, 988: 111036610.0, 989: 70855100.0, 990: -145057000.0, 991: -168113180.0, 992: 30126272.0, 993: -155948640.0, 994: -128049920.0, 995: -150456220.0, 996: -157291420.0, 997: -131550980.0, 998: 55829250.0, 999: -114528160.0}\n"
     ]
    }
   ],
   "source": [
    "dic_loaded = {}\n",
    "for i, l in enumerate(loaded):\n",
    "    dic_loaded[i] = l\n",
    "print(dic_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(688, 7416444000.0), (325, 1361434100.0), (334, 1192154400.0), (881, 990569600.0), (853, 976954600.0), (62, 919648400.0), (793, 693147800.0), (560, 648020860.0), (982, 643953000.0), (633, 612968800.0), (672, 593849200.0), (381, 587190400.0), (698, 583976700.0), (246, 583571840.0), (329, 573303200.0), (642, 552971000.0), (343, 535015040.0), (833, 508673540.0), (76, 471608830.0), (570, 469521920.0), (506, 441704580.0), (808, 435987700.0), (428, 435132160.0), (934, 427278340.0), (644, 418872130.0), (805, 417549630.0), (862, 415857540.0), (547, 413354000.0), (788, 407853630.0), (983, 404180600.0), (265, 402850000.0), (749, 397199170.0), (225, 393472200.0), (41, 389410370.0), (751, 384530240.0), (782, 383166200.0), (900, 381857470.0), (170, 381835070.0), (60, 377143870.0), (85, 359141700.0), (903, 350674300.0), (949, 334902080.0), (803, 334077000.0), (557, 332948160.0), (955, 322528500.0), (634, 312339400.0), (512, 309764860.0), (638, 302970300.0), (527, 302172540.0), (553, 300880830.0), (113, 298914560.0), (200, 291801280.0), (444, 284226370.0), (562, 282598980.0), (309, 274029950.0), (337, 271410800.0), (533, 269212540.0), (727, 264403400.0), (470, 260055550.0), (114, 259317310.0), (239, 255080130.0), (333, 254202370.0), (297, 254174780.0), (193, 246292220.0), (854, 240256580.0), (286, 237856640.0), (656, 236040260.0), (107, 229996930.0), (743, 229934780.0), (532, 228550780.0), (53, 222385150.0), (576, 222217980.0), (600, 220013700.0), (927, 219538050.0), (715, 217233090.0), (469, 216782270.0), (892, 213904830.0), (456, 208187460.0), (801, 207988600.0), (461, 206900860.0), (4, 206820480.0), (298, 206580220.0), (958, 204020100.0), (137, 199019140.0), (530, 194992830.0), (601, 194900860.0), (775, 194401730.0), (365, 190520130.0), (278, 190012930.0), (145, 187762370.0), (708, 186084860.0), (933, 184303550.0), (91, 183327550.0), (433, 182421180.0), (314, 181936380.0), (478, 181642180.0), (901, 181611650.0), (448, 181253380.0), (689, 180343680.0), (966, 178943300.0), (639, 176153730.0), (544, 176057860.0), (870, 174730430.0), (279, 173469890.0), (545, 170905280.0), (866, 168742460.0), (586, 163574270.0), (914, 163215680.0), (716, 162277440.0), (425, 160014140.0), (783, 159677120.0), (169, 158798660.0), (17, 158693310.0), (795, 157739520.0), (720, 155512380.0), (499, 154886530.0), (912, 153371400.0), (874, 152915840.0), (112, 152447490.0), (203, 152325180.0), (687, 151329220.0), (821, 150674240.0), (404, 146358780.0), (65, 143271800.0), (296, 139807230.0), (25, 139254720.0), (548, 138389380.0), (127, 135477120.0), (429, 134709630.0), (442, 131004930.0), (66, 130538560.0), (396, 129196540.0), (134, 128411650.0), (599, 125604800.0), (524, 125284290.0), (27, 123487420.0), (217, 120129090.0), (617, 120101760.0), (495, 119404290.0), (263, 119396160.0), (737, 119052100.0), (559, 118537920.0), (609, 118061310.0), (824, 117355780.0), (391, 114829700.0), (370, 114253950.0), (221, 113421700.0), (479, 112168320.0), (125, 111904380.0), (366, 111832190.0), (724, 111210690.0), (988, 111036610.0), (640, 110369730.0), (87, 110170880.0), (266, 109045380.0), (491, 106525890.0), (733, 105630140.0), (86, 105145980.0), (135, 104448260.0), (33, 104150460.0), (373, 104133060.0), (184, 102976320.0), (748, 100494400.0), (798, 100305790.0), (974, 100084420.0), (189, 98879810.0), (397, 96979390.0), (602, 95823490.0), (818, 95687870.0), (158, 95640700.0), (630, 95262720.0), (482, 94848960.0), (411, 94720450.0), (605, 94077060.0), (603, 93310020.0), (98, 92939710.0), (825, 92711040.0), (567, 92227580.0), (944, 90930240.0), (180, 90319620.0), (450, 89782910.0), (393, 89331710.0), (898, 88980420.0), (181, 88899330.0), (682, 88192260.0), (19, 87436930.0), (779, 87272260.0), (537, 87077500.0), (676, 87021700.0), (116, 85856700.0), (677, 85761340.0), (211, 85556030.0), (613, 85120060.0), (22, 83981440.0), (895, 83324860.0), (262, 82954180.0), (316, 82924990.0), (177, 82816190.0), (740, 82767620.0), (735, 82380350.0), (206, 81447620.0), (947, 81356990.0), (299, 81337540.0), (525, 81020220.0), (251, 80499070.0), (804, 80324610.0), (383, 79996990.0), (589, 79834180.0), (201, 79646460.0), (922, 79550720.0), (894, 78830270.0), (144, 77617540.0), (973, 77604480.0), (78, 77563070.0), (287, 77414590.0), (18, 77251580.0), (272, 76939970.0), (387, 76146180.0), (860, 75274880.0), (254, 75109700.0), (435, 75102140.0), (666, 75059780.0), (282, 75019840.0), (813, 74717950.0), (175, 73824320.0), (706, 73466750.0), (408, 73236930.0), (843, 72636290.0), (916, 72383420.0), (608, 72143870.0), (752, 72132160.0), (318, 71931070.0), (150, 71918980.0), (247, 71235710.0), (989, 70855100.0), (24, 70794430.0), (73, 69509440.0), (962, 68607300.0), (441, 68271740.0), (575, 67448960.0), (198, 65973824.0), (228, 65806144.0), (731, 65339520.0), (707, 65240704.0), (690, 65007870.0), (725, 64943870.0), (183, 64293630.0), (362, 63614336.0), (142, 62934016.0), (632, 62789504.0), (648, 61962496.0), (418, 61702720.0), (728, 60374910.0), (500, 59532544.0), (829, 58995070.0), (102, 58595520.0), (770, 57995650.0), (694, 57953470.0), (136, 57668670.0), (191, 57625920.0), (826, 57247936.0), (774, 57242240.0), (372, 56752510.0), (420, 56505470.0), (612, 56078400.0), (5, 55940290.0), (998, 55829250.0), (452, 55221310.0), (431, 54689150.0), (709, 54263360.0), (205, 54243264.0), (534, 53658240.0), (951, 53075710.0), (48, 52150144.0), (356, 52110464.0), (345, 51789950.0), (34, 51729470.0), (454, 50538176.0), (830, 50180800.0), (835, 49009470.0), (357, 48994750.0), (249, 48863550.0), (438, 48793790.0), (701, 48474430.0), (64, 48309056.0), (847, 47960830.0), (109, 47885824.0), (242, 47625856.0), (459, 47463744.0), (475, 47277890.0), (451, 47234750.0), (800, 47217856.0), (31, 47008000.0), (702, 46182080.0), (909, 45834560.0), (558, 45641410.0), (732, 45359170.0), (508, 45134976.0), (455, 44105024.0), (35, 43572736.0), (82, 42761280.0), (624, 42707584.0), (80, 42698690.0), (139, 42668990.0), (978, 42443776.0), (549, 42277310.0), (248, 42275970.0), (541, 42186880.0), (369, 41840256.0), (917, 41773440.0), (133, 41657280.0), (476, 41433664.0), (417, 41401344.0), (906, 41267070.0), (970, 41008770.0), (271, 40853310.0), (447, 40691776.0), (861, 40597056.0), (36, 40529790.0), (305, 40304960.0), (128, 40303550.0), (59, 39723390.0), (531, 39678336.0), (929, 39428160.0), (514, 39023040.0), (926, 38955010.0), (911, 38794880.0), (930, 37924670.0), (392, 37811390.0), (522, 37671490.0), (778, 37330944.0), (781, 37083330.0), (777, 36948990.0), (375, 36387136.0), (353, 36366400.0), (938, 36213250.0), (484, 36205504.0), (436, 36126400.0), (13, 35934464.0), (146, 35830336.0), (977, 35584704.0), (780, 35533760.0), (734, 35522370.0), (464, 35482176.0), (714, 35351616.0), (965, 34976256.0), (131, 34700736.0), (7, 34693250.0), (214, 34643710.0), (140, 34007360.0), (755, 33914110.0), (704, 33858176.0), (636, 33838210.0), (841, 33275328.0), (44, 33210368.0), (99, 33198848.0), (50, 33033600.0), (497, 32830080.0), (568, 32790272.0), (389, 32427264.0), (738, 31904704.0), (972, 31738816.0), (301, 31721024.0), (106, 31589504.0), (796, 31074240.0), (492, 30983552.0), (845, 30820416.0), (394, 30541568.0), (992, 30126272.0), (342, 29978496.0), (668, 29900224.0), (160, 29756416.0), (219, 29692480.0), (300, 29211968.0), (234, 29148608.0), (432, 29144256.0), (290, 29112960.0), (138, 28866304.0), (406, 28813696.0), (28, 28170496.0), (517, 28129536.0), (828, 28071104.0), (327, 27949312.0), (267, 27849664.0), (880, 27633472.0), (14, 27531392.0), (148, 27483328.0), (641, 27444608.0), (959, 27333568.0), (68, 27271552.0), (750, 27036864.0), (347, 26894144.0), (354, 26847808.0), (665, 26802112.0), (948, 26318464.0), (481, 26177856.0), (168, 26124032.0), (496, 26044992.0), (257, 25645056.0), (631, 25536384.0), (377, 25324224.0), (1, 25310208.0), (40, 25147648.0), (412, 24654464.0), (477, 24461120.0), (913, 24458880.0), (986, 24350464.0), (686, 24342848.0), (154, 24159424.0), (756, 24125568.0), (101, 23838976.0), (513, 23177408.0), (961, 22575104.0), (306, 22457216.0), (759, 22430784.0), (209, 22382016.0), (739, 22238528.0), (585, 21449856.0), (71, 21103808.0), (657, 20823936.0), (47, 20706112.0), (222, 20701696.0), (536, 20614848.0), (766, 20614848.0), (67, 20231488.0), (885, 20126528.0), (243, 19824576.0), (352, 19766144.0), (616, 19746048.0), (118, 19407360.0), (539, 19043520.0), (238, 18745600.0), (838, 18253568.0), (152, 18214464.0), (879, 18076096.0), (130, 17752320.0), (856, 17145472.0), (598, 16953472.0), (270, 16786944.0), (692, 16360192.0), (839, 16272384.0), (227, 16169920.0), (253, 16077440.0), (268, 15672192.0), (104, 15513664.0), (606, 15495872.0), (696, 15447872.0), (669, 15380416.0), (842, 15230592.0), (490, 14741824.0), (321, 14579072.0), (474, 13935040.0), (240, 13902336.0), (173, 13841664.0), (153, 13789248.0), (395, 13457984.0), (32, 13428416.0), (528, 13414464.0), (763, 13408320.0), (890, 13056320.0), (787, 12810176.0), (815, 12693632.0), (445, 12666176.0), (426, 12652480.0), (45, 12548672.0), (110, 12333440.0), (281, 12204736.0), (659, 12200640.0), (597, 11864576.0), (3, 11471168.0), (147, 10875776.0), (483, 10621632.0), (8, 10488832.0), (103, 10195456.0), (840, 9867968.0), (443, 9395840.0), (283, 9315712.0), (207, 8842176.0), (230, 8837696.0), (317, 7892736.0), (987, 7891584.0), (182, 7820032.0), (580, 7202432.0), (471, 6918848.0), (957, 6855936.0), (928, 6836480.0), (820, 6802624.0), (940, 6703232.0), (625, 6079616.0), (691, 5647680.0), (378, 5471552.0), (834, 5368000.0), (280, 4731456.0), (769, 4382592.0), (649, 4322112.0), (218, 3603904.0), (216, 2957312.0), (919, 1998528.0), (161, 1502976.0), (9, 1445568.0), (124, 1429824.0), (785, 1154944.0), (776, 212608.0), (984, 191040.0), (487, 54464.0), (569, -276160.0), (256, -1083328.0), (564, -1680256.0), (915, -2237504.0), (361, -3507328.0), (671, -3563072.0), (311, -3583744.0), (473, -5037056.0), (745, -5312768.0), (790, -7035456.0), (572, -7244928.0), (623, -7319936.0), (718, -8457536.0), (390, -8885568.0), (12, -9019072.0), (577, -9103936.0), (504, -9198400.0), (555, -10462272.0), (21, -10882752.0), (773, -10952576.0), (673, -12309184.0), (964, -13429952.0), (416, -13811648.0), (259, -15313280.0), (163, -18647488.0), (695, -18851456.0), (920, -21368576.0), (981, -25139008.0), (498, -27947520.0), (105, -31581888.0), (156, -36544000.0), (363, -42603970.0), (730, -42832960.0), (887, -53531650.0), (683, -56039616.0), (384, -61014656.0), (593, -61049536.0), (23, -61718910.0), (52, -62483200.0), (744, -62605376.0), (592, -62941376.0), (331, -64343104.0), (852, -64491456.0), (588, -67352260.0), (865, -68464380.0), (583, -70135940.0), (817, -70242110.0), (574, -70436930.0), (502, -70905020.0), (937, -72398590.0), (529, -72741250.0), (767, -75934720.0), (54, -78486590.0), (30, -79057860.0), (979, -80297600.0), (611, -80953280.0), (285, -81100160.0), (231, -81313470.0), (467, -82217220.0), (380, -82326210.0), (760, -84256260.0), (855, -84653570.0), (220, -85495940.0), (584, -86245440.0), (831, -86725250.0), (945, -87699900.0), (918, -88613060.0), (295, -88998530.0), (627, -89817600.0), (97, -92901250.0), (235, -93080320.0), (717, -93778750.0), (792, -94222530.0), (932, -94544130.0), (764, -94718340.0), (653, -94755260.0), (622, -94785090.0), (650, -95354560.0), (178, -97322750.0), (581, -97817790.0), (620, -98068160.0), (604, -99526910.0), (883, -99988930.0), (980, -101072320.0), (520, -101154370.0), (176, -101751170.0), (368, -102007360.0), (195, -102149630.0), (807, -102370690.0), (518, -102584960.0), (486, -102865660.0), (11, -104090620.0), (864, -104335620.0), (292, -104499970.0), (439, -104576900.0), (355, -104739900.0), (0, -105436540.0), (646, -105468480.0), (721, -106070140.0), (460, -106456700.0), (789, -106964290.0), (174, -108295620.0), (628, -108735170.0), (761, -109615100.0), (643, -110344450.0), (37, -110372480.0), (269, -110697410.0), (753, -111069820.0), (121, -112647230.0), (697, -113035580.0), (457, -113251780.0), (51, -113388670.0), (232, -113467650.0), (699, -113598210.0), (115, -113662210.0), (556, -114176130.0), (480, -114246530.0), (999, -114528160.0), (294, -114703420.0), (985, -114964290.0), (385, -115503550.0), (607, -115680770.0), (891, -116234820.0), (210, -116742460.0), (284, -117480740.0), (726, -117642660.0), (742, -118062750.0), (941, -118244190.0), (899, -118283550.0), (403, -120307740.0), (149, -120361570.0), (645, -120589440.0), (382, -120827780.0), (679, -121192670.0), (896, -122018240.0), (100, -122026430.0), (165, -122196670.0), (275, -122637890.0), (615, -122700900.0), (516, -123036700.0), (92, -123235230.0), (322, -123265790.0), (157, -123284670.0), (446, -123474370.0), (122, -123800260.0), (93, -123979390.0), (971, -124287840.0), (16, -124985380.0), (925, -125802300.0), (908, -125857470.0), (463, -125987140.0), (70, -125991940.0), (61, -126252990.0), (799, -126503230.0), (472, -126823040.0), (63, -126964420.0), (289, -127137760.0), (303, -127150300.0), (758, -127236420.0), (494, -127361630.0), (26, -127395710.0), (440, -127452450.0), (954, -127546080.0), (953, -128048960.0), (994, -128049920.0), (976, -128589860.0), (83, -128875740.0), (94, -128961280.0), (875, -128964160.0), (20, -128984510.0), (551, -129083740.0), (618, -129156700.0), (505, -129511970.0), (258, -129573120.0), (858, -129646240.0), (95, -129695260.0), (535, -129973440.0), (703, -131012930.0), (69, -131344800.0), (935, -131396190.0), (250, -131470780.0), (997, -131550980.0), (540, -131614110.0), (561, -131701950.0), (224, -131893150.0), (710, -132427740.0), (351, -132806210.0), (414, -133048320.0), (410, -134066910.0), (886, -134318460.0), (931, -134457380.0), (662, -134605540.0), (229, -134623000.0), (415, -135240060.0), (348, -135309470.0), (546, -135682940.0), (379, -135803400.0), (199, -135993950.0), (419, -136084130.0), (302, -136136450.0), (458, -136159360.0), (836, -136430270.0), (660, -136523550.0), (554, -136632060.0), (946, -137057860.0), (889, -137066110.0), (685, -137423100.0), (335, -138015330.0), (655, -138030850.0), (882, -138224380.0), (358, -138414880.0), (768, -138632800.0), (126, -138749310.0), (202, -138815840.0), (712, -139105280.0), (324, -139194460.0), (339, -139394050.0), (123, -139598620.0), (398, -139689250.0), (485, -139866600.0), (507, -140206880.0), (437, -140461800.0), (719, -140756380.0), (626, -141165980.0), (797, -141286690.0), (77, -141778240.0), (192, -141808900.0), (323, -141898880.0), (186, -142107490.0), (10, -142162110.0), (405, -142175040.0), (376, -142187230.0), (434, -142488030.0), (771, -142490750.0), (90, -142682530.0), (310, -142837280.0), (326, -142858200.0), (747, -144662180.0), (711, -144956640.0), (952, -144975040.0), (990, -145057000.0), (563, -145057600.0), (784, -145186910.0), (736, -145192600.0), (950, -145697800.0), (55, -145819100.0), (159, -145846530.0), (252, -145859070.0), (236, -146137090.0), (89, -146193980.0), (167, -146231650.0), (814, -146260100.0), (871, -146330850.0), (550, -146732100.0), (244, -146740380.0), (340, -146748960.0), (806, -146822180.0), (762, -147052960.0), (654, -147118110.0), (846, -147241120.0), (942, -147436320.0), (197, -147475940.0), (713, -147831710.0), (857, -148150340.0), (367, -148381470.0), (237, -148509090.0), (346, -148554690.0), (386, -148580320.0), (510, -148620580.0), (893, -148829700.0), (374, -148942660.0), (119, -149080900.0), (590, -149403520.0), (610, -149453820.0), (320, -149557150.0), (413, -149907360.0), (765, -149915940.0), (74, -150001700.0), (519, -150355070.0), (449, -150378560.0), (995, -150456220.0), (802, -150474050.0), (58, -150749380.0), (212, -150783100.0), (791, -150844200.0), (488, -150853820.0), (967, -151017730.0), (194, -151043970.0), (462, -151275800.0), (523, -151297000.0), (264, -151362850.0), (164, -151561540.0), (465, -151661540.0), (336, -151761660.0), (277, -152183300.0), (968, -152262200.0), (57, -152524640.0), (424, -153192100.0), (2, -153372000.0), (678, -153379680.0), (79, -153614050.0), (722, -153653760.0), (587, -154112380.0), (848, -154375140.0), (878, -154407260.0), (245, -154847000.0), (422, -155232640.0), (746, -155275260.0), (877, -155430240.0), (902, -155633700.0), (993, -155948640.0), (108, -155986560.0), (822, -156284510.0), (859, -156502940.0), (675, -156572960.0), (421, -156726140.0), (851, -156969980.0), (684, -157069000.0), (680, -157142270.0), (359, -157275040.0), (996, -157291420.0), (276, -157292960.0), (489, -157298400.0), (423, -157322600.0), (741, -157371870.0), (364, -157432960.0), (402, -157447900.0), (308, -157490690.0), (293, -157638820.0), (969, -157851360.0), (43, -157860900.0), (313, -157959580.0), (304, -158066500.0), (515, -158148350.0), (876, -158427680.0), (863, -158474980.0), (897, -158642400.0), (816, -158667360.0), (509, -158956290.0), (96, -159095650.0), (261, -159118780.0), (647, -159461310.0), (330, -159615940.0), (910, -159892420.0), (288, -159898430.0), (924, -160116960.0), (72, -160133380.0), (401, -160275780.0), (582, -160501250.0), (635, -160542460.0), (591, -160743200.0), (388, -160926530.0), (6, -161260160.0), (565, -161299680.0), (772, -161317730.0), (850, -161411680.0), (427, -161461120.0), (907, -161504860.0), (223, -161681060.0), (837, -161994460.0), (849, -162193440.0), (255, -162238460.0), (75, -162275170.0), (794, -162462850.0), (503, -162493500.0), (468, -163146530.0), (670, -163183260.0), (166, -163186500.0), (637, -163195800.0), (241, -163407900.0), (328, -163410880.0), (319, -163583040.0), (542, -164155360.0), (172, -164436320.0), (143, -164499460.0), (681, -164564130.0), (349, -164677250.0), (188, -164748130.0), (190, -164922820.0), (538, -164944000.0), (811, -165042050.0), (187, -165140600.0), (543, -165734050.0), (904, -165853700.0), (233, -166130530.0), (409, -166610080.0), (215, -166648900.0), (700, -166657000.0), (511, -166675680.0), (466, -166703200.0), (566, -166706780.0), (963, -166790240.0), (312, -166956960.0), (213, -166977250.0), (399, -167032770.0), (273, -167076800.0), (315, -167267840.0), (360, -167283170.0), (493, -167313150.0), (179, -167615230.0), (204, -167697950.0), (208, -167793120.0), (960, -168003000.0), (991, -168113180.0), (812, -168173660.0), (936, -168339800.0), (661, -168611550.0), (884, -168989020.0), (888, -169030530.0), (667, -169097730.0), (905, -169369660.0), (619, -169380380.0), (274, -169572160.0), (81, -169892800.0), (49, -170180510.0), (400, -170588830.0), (975, -170965090.0), (664, -171105180.0), (729, -171123620.0), (338, -171391420.0), (595, -171447100.0), (832, -171461730.0), (526, -171555170.0), (867, -171810560.0), (723, -172019230.0), (571, -172256600.0), (872, -172538240.0), (674, -172586850.0), (754, -172639100.0), (652, -172785150.0), (350, -173124130.0), (117, -173729760.0), (596, -173961150.0), (594, -174346370.0), (939, -174477920.0), (651, -174898020.0), (307, -174945440.0), (129, -175006500.0), (573, -175078020.0), (943, -175172030.0), (629, -175204510.0), (869, -175411550.0), (407, -175810050.0), (844, -176323740.0), (663, -176429400.0), (132, -176519620.0), (185, -177478430.0), (579, -177534300.0), (38, -177694200.0), (621, -178026560.0), (226, -179139680.0), (46, -179210720.0), (260, -179404600.0), (162, -179623400.0), (151, -179685400.0), (196, -179980930.0), (956, -180030240.0), (809, -180058430.0), (120, -180593800.0), (291, -181006400.0), (578, -181015400.0), (88, -181409920.0), (341, -181564860.0), (430, -181603900.0), (56, -181676740.0), (819, -182065310.0), (453, -182427580.0), (786, -182838500.0), (155, -183254880.0), (757, -183403580.0), (658, -183558050.0), (552, -184217820.0), (521, -184246020.0), (868, -184379620.0), (827, -184487460.0), (921, -184759000.0), (693, -184791680.0), (171, -185279800.0), (873, -186134270.0), (15, -186393250.0), (705, -186642720.0), (923, -186660510.0), (42, -187263740.0), (141, -187329950.0), (810, -187483400.0), (614, -187627710.0), (501, -187872030.0), (84, -188234620.0), (39, -188921500.0), (344, -194384580.0), (823, -195655360.0), (332, -196114240.0), (371, -196465920.0), (29, -196940260.0), (111, -200388450.0)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "sorted_dct = sorted(dic_loaded.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(sorted_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted accuracy drop is 0.067\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from eval import data_removal_f1\n",
    "from metrics import weighted_acc_drop\n",
    "acc = data_removal_f1(dic_loaded, X_train_scaled, y_train_balanced, X_test_scaled, y_test_balanced)\n",
    "plt.plot(range(len(acc)), acc)\n",
    "res = weighted_acc_drop(acc)\n",
    "print(\"The weighted accuracy drop is {:.3f}\".format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -0.0004520000000000002, 1: -0.002193999999999999, 2: 0.0009899999999999993, 3: 0.000688, 4: 0.001975999999999998, 5: 0.001773999999999998, 6: -0.0007680000000000006, 7: 0.0007699999999999997, 8: 0.0004660000000000004, 9: 0.0019260000000000006, 10: 0.002362000000000002, 11: 0.0007120000000000026, 12: 0.0012820000000000002, 13: -0.0003839999999999993, 14: 0.0005640000000000003, 15: 0.0017879999999999975, 16: 0.0010200000000000007, 17: 0.0007939999999999988, 18: 0.0009019999999999995, 19: 0.0015500000000000008, 20: 0.0001859999999999987, 21: 0.0009160000000000006, 22: 0.00048200000000000033, 23: -0.0008460000000000009, 24: 0.0017059999999999996, 25: 0.0016219999999999993, 26: 0.0006419999999999998, 27: 0.0011360000000000025, 28: 0.00029400000000000064, 29: 4.199999999999945e-05, 30: 0.00041400000000000106, 31: -0.001978000000000001, 32: -0.0005200000000000003, 33: 0.0016099999999999999, 34: 0.0008940000000000005, 35: 0.0015059999999999993, 36: 0.0008240000000000006, 37: 0.0016019999999999988, 38: -0.0001720000000000005, 39: 0.00020799999999999963, 40: -0.0003959999999999986, 41: 0.0016740000000000004, 42: 0.0011020000000000005, 43: 0.0005680000000000001, 44: 0.0013999999999999993, 45: 0.0017439999999999986, 46: -0.0005060000000000002, 47: 0.0016620000000000016, 48: 0.0010239999999999984, 49: 0.0015139999999999989, 50: 0.0010160000000000002, 51: 0.0018699999999999993, 52: 0.0015800000000000022, 53: -0.000551999999999999, 54: 3.1999999999998637e-05, 55: 0.0007820000000000007, 56: 0.0009839999999999985, 57: 0.0007499999999999998, 58: 0.0018160000000000006, 59: 0.0018379999999999978, 60: 0.0007080000000000011, 61: 0.0017319999999999994, 62: 0.0004099999999999996, 63: 0.0007499999999999996, 64: -0.0004900000000000003, 65: -0.00023600000000000018, 66: 0.0008959999999999991, 67: 0.0007580000000000009, 68: 0.0013739999999999987, 69: -0.0003979999999999992, 70: 0.0026360000000000008, 71: 0.001387999999999999, 72: 0.0019279999999999976, 73: -9.599999999999892e-05, 74: 0.001933999999999998, 75: -0.0003660000000000005, 76: 0.0018020000000000009, 77: 0.0017920000000000028, 78: -0.0011700000000000016, 79: 0.0014599999999999993, 80: 0.0016560000000000008, 81: 0.0020899999999999994, 82: 0.00014200000000000112, 83: 0.000800000000000001, 84: 0.0006459999999999992, 85: 0.002243999999999999, 86: 0.0007259999999999994, 87: 0.0014899999999999991, 88: -0.0015699999999999987, 89: 0.002871999999999996, 90: -6.999999999999944e-05, 91: 0.0021039999999999987, 92: 0.0020579999999999956, 93: 0.0008380000000000001, 94: -0.00014399999999999778, 95: 0.002217999999999997, 96: -0.00024999999999999984, 97: -5.3999999999998914e-05, 98: -0.0008380000000000002, 99: 0.0012400000000000015, 100: 0.00031799999999999927, 101: 0.0016879999999999996, 102: -0.0009040000000000006, 103: 0.001620000000000001, 104: 0.00011799999999999964, 105: -2.1999999999999593e-05, 106: 0.002144000000000001, 107: 0.0008580000000000024, 108: 0.0003100000000000004, 109: 0.000915999999999997, 110: 0.0005619999999999991, 111: 0.001746000000000004, 112: 0.000458, 113: 0.0010119999999999992, 114: 0.0007259999999999992, 115: 0.0006980000000000005, 116: 0.0006639999999999983, 117: 0.0007639999999999995, 118: 0.0008380000000000001, 119: -0.00024800000000000034, 120: 0.00028000000000000095, 121: 0.0004539999999999992, 122: 0.0006439999999999997, 123: -7.1999999999999e-05, 124: 0.00039399999999999955, 125: -0.0005720000000000002, 126: 0.0014639999999999994, 127: 0.0005459999999999998, 128: -0.001205999999999997, 129: -3.800000000000069e-05, 130: 0.0007260000000000004, 131: 0.0006279999999999999, 132: 0.0006040000000000007, 133: 0.0008240000000000014, 134: 0.0010339999999999995, 135: 0.0006560000000000001, 136: 0.0018840000000000005, 137: -0.0009139999999999979, 138: -4.19999999999991e-05, 139: 0.00044599999999999994, 140: 0.0010280000000000003, 141: 0.0009440000000000017, 142: -0.0005759999999999999, 143: -0.00041000000000000026, 144: 0.0016019999999999993, 145: 0.0013920000000000007, 146: 0.0011920000000000001, 147: 0.0005140000000000021, 148: 0.0006419999999999995, 149: 0.003, 150: 0.002352, 151: 0.0016539999999999988, 152: 0.0015339999999999993, 153: 0.0010060000000000021, 154: 0.0002959999999999995, 155: 0.00026600000000000034, 156: 0.0004279999999999999, 157: 0.000416, 158: 0.0016360000000000003, 159: 0.0007859999999999996, 160: 0.0020519999999999974, 161: -0.0004600000000000003, 162: 0.0019899999999999983, 163: 0.001548, 164: -0.0006880000000000007, 165: 0.0016499999999999998, 166: 0.002816, 167: 0.0009599999999999985, 168: -0.0005279999999999987, 169: 0.0014980000000000002, 170: -0.0005739999999999971, 171: 0.0010399999999999997, 172: 0.001078000000000001, 173: 0.0007379999999999997, 174: -0.00045999999999999985, 175: 0.002314000000000002, 176: 0.0009720000000000007, 177: 0.0009039999999999994, 178: 0.0030559999999999993, 179: 0.00046599999999999946, 180: 0.0006119999999999999, 181: 0.0003060000000000011, 182: 0.0003040000000000001, 183: 0.001686000000000001, 184: -0.0004020000000000002, 185: 0.0001759999999999991, 186: -0.00034999999999999935, 187: 0.001382, 188: -0.0008119999999999998, 189: 0.00039799999999999965, 190: 0.0003540000000000002, 191: 0.00018999999999999969, 192: 0.0006200000000000007, 193: 0.001957999999999999, 194: 0.0006819999999999993, 195: 0.001074000000000001, 196: 0.001276, 197: -0.000344, 198: -0.0011279999999999994, 199: 0.0019679999999999997, 200: 0.0015880000000000022, 201: 0.0020460000000000035, 202: 0.00031999999999999976, 203: -0.00022599999999999918, 204: 0.0014199999999999992, 205: 0.0005820000000000011, 206: 0.002261999999999998, 207: -0.0001160000000000008, 208: 0.0015220000000000025, 209: 0.0007359999999999991, 210: 0.0003099999999999997, 211: 0.0002980000000000013, 212: 0.0005759999999999989, 213: 0.002654, 214: 0.0014780000000000004, 215: 0.0010879999999999998, 216: 0.0025120000000000025, 217: 0.0006759999999999994, 218: -0.0006599999999999984, 219: 0.000858000000000001, 220: 0.0008439999999999989, 221: 0.00030199999999999953, 222: -0.00037600000000000025, 223: 0.0006119999999999999, 224: 0.0021639999999999962, 225: 0.0014300000000000011, 226: 0.002569999999999996, 227: 0.0016859999999999978, 228: 0.0005640000000000007, 229: 0.0007539999999999998, 230: 0.0030720000000000022, 231: -0.0009379999999999992, 232: -0.0012600000000000016, 233: 0.0007300000000000002, 234: 0.0005259999999999999, 235: 0.002046000000000003, 236: 0.0004700000000000005, 237: -0.00010199999999999939, 238: 0.0010560000000000003, 239: 0.00036400000000000007, 240: 0.0011159999999999985, 241: 0.0001740000000000001, 242: -0.00021599999999999885, 243: 0.0002859999999999991, 244: 0.0008499999999999993, 245: -0.0001240000000000004, 246: 0.0001640000000000006, 247: -0.0008579999999999994, 248: 0.00041200000000000167, 249: 0.0029219999999999966, 250: 0.0008799999999999998, 251: -0.00011600000000000022, 252: -0.0009479999999999995, 253: 0.001976000000000002, 254: 0.0017259999999999986, 255: -5.9999999999999365e-05, 256: 0.0017580000000000017, 257: 0.0020499999999999993, 258: -0.00024399999999999926, 259: -3.199999999999978e-05, 260: 0.0009340000000000007, 261: 0.00032000000000000084, 262: 0.0016500000000000006, 263: 0.0018340000000000008, 264: 0.0011340000000000011, 265: 0.0017260000000000005, 266: 0.002173999999999999, 267: -0.0002860000000000005, 268: 0.0011800000000000005, 269: 0.0007300000000000007, 270: 0.0032039999999999985, 271: 0.000534, 272: -0.00013799999999999937, 273: 0.0006559999999999999, 274: 0.0006220000000000008, 275: 0.0006640000000000011, 276: -0.000326, 277: 0.0018779999999999984, 278: -0.0005420000000000005, 279: 0.0016859999999999985, 280: 0.0005799999999999989, 281: -0.0011019999999999992, 282: 0.00042999999999999934, 283: -0.0008919999999999984, 284: 0.0004539999999999998, 285: -0.0007379999999999997, 286: -0.00016600000000000005, 287: 0.0044579999999999976, 288: -0.0006020000000000001, 289: 0.001784, 290: 0.0010920000000000018, 291: 0.00031400000000000004, 292: 0.0004579999999999986, 293: 0.0006679999999999988, 294: 0.0003499999999999996, 295: -0.0013839999999999996, 296: -5.400000000000008e-05, 297: 0.002516, 298: 0.0011560000000000008, 299: -0.0009840000000000005, 300: 0.002221999999999999, 301: 0.0008440000000000011, 302: 0.00034600000000000066, 303: 0.0020240000000000032, 304: 0.0002880000000000005, 305: 0.0014000000000000004, 306: 0.001524000000000001, 307: 0.003934000000000007, 308: 0.0010220000000000003, 309: 0.001174000000000001, 310: 0.0013779999999999995, 311: 0.002035999999999998, 312: 0.0005519999999999991, 313: 0.0007859999999999998, 314: 0.0003720000000000017, 315: 0.0009839999999999988, 316: 0.0001519999999999996, 317: -5.999999999998748e-06, 318: 0.002006, 319: -6.0000000000001645e-06, 320: -0.00019999999999999968, 321: 0.0006539999999999999, 322: 0.00017800000000000067, 323: 0.00048800000000000075, 324: 0.00020200000000000009, 325: 0.0013659999999999998, 326: 0.0004499999999999992, 327: 0.0005599999999999986, 328: -0.0004959999999999997, 329: -0.00015199999999999966, 330: -0.00011400000000000045, 331: 0.002618, 332: 0.0021700000000000014, 333: 0.0006179999999999994, 334: 0.0016659999999999995, 335: -0.00022599999999999986, 336: -0.0005719999999999988, 337: 0.00036200000000000007, 338: -0.00018999999999999993, 339: -0.0007419999999999997, 340: 0.0016760000000000006, 341: 0.0021960000000000013, 342: -0.000479999999999999, 343: 0.002572, 344: -1.999999999999815e-06, 345: 0.0007340000000000005, 346: 0.0003000000000000004, 347: 0.0004559999999999984, 348: -0.0005140000000000004, 349: 0.0009320000000000006, 350: 0.0021559999999999978, 351: 4.000000000000923e-06, 352: 0.0005199999999999996, 353: -0.0004420000000000017, 354: 5.799999999999989e-05, 355: 0.0005579999999999997, 356: 0.00016799999999999915, 357: 0.00020600000000000056, 358: 0.0004520000000000004, 359: 0.0016060000000000017, 360: 6.200000000000007e-05, 361: -0.00016199999999999976, 362: 0.0009359999999999991, 363: 0.0006120000000000001, 364: 0.0017500000000000018, 365: 0.0013259999999999997, 366: 0.0003240000000000001, 367: -0.00046199999999999876, 368: 0.0010839999999999997, 369: 0.0023219999999999985, 370: 0.0010140000000000017, 371: 0.0005819999999999997, 372: 0.0075200000000000015, 373: 0.0007119999999999991, 374: -0.0013439999999999997, 375: -0.0004059999999999995, 376: 0.0013739999999999996, 377: 0.0006420000000000008, 378: 0.0013979999999999989, 379: 0.0008700000000000016, 380: 0.00043600000000000095, 381: 0.000597999999999999, 382: 0.0018020000000000004, 383: 0.001669999999999998, 384: 0.0009239999999999997, 385: 0.0005499999999999988, 386: 0.0007980000000000002, 387: 0.0009400000000000016, 388: -0.0004599999999999988, 389: 0.0005259999999999999, 390: -0.001255999999999997, 391: 0.003610000000000004, 392: 0.0012, 393: 0.0007460000000000004, 394: 0.003530000000000004, 395: 0.0008659999999999997, 396: 0.001234000000000001, 397: 0.0004799999999999986, 398: 0.0009160000000000012, 399: 0.0005220000000000002, 400: 0.0011839999999999993, 401: 0.0005260000000000005, 402: 0.0007159999999999998, 403: 0.0017199999999999958, 404: 0.0014899999999999987, 405: 0.0008440000000000003, 406: 0.00033600000000000134, 407: -0.0003320000000000001, 408: 0.0006980000000000028, 409: 0.0011299999999999988, 410: 0.0008500000000000001, 411: 0.002822000000000002, 412: 0.0009999999999999994, 413: 0.0014319999999999992, 414: 0.0004460000000000003, 415: 0.0017299999999999991, 416: 0.0006979999999999996, 417: 0.0013620000000000017, 418: -0.0010059999999999995, 419: 0.0006860000000000019, 420: 0.0005480000000000012, 421: 4.799999999999975e-05, 422: -0.0008000000000000004, 423: -7.399999999999983e-05, 424: 0.0012659999999999998, 425: 0.001389999999999999, 426: -1.1999999999999387e-05, 427: 0.0003320000000000015, 428: 0.0015460000000000003, 429: 0.0021919999999999995, 430: -0.000639999999999999, 431: 0.0007819999999999996, 432: 0.0025279999999999994, 433: -0.000717999999999998, 434: 0.0014640000000000015, 435: 0.0023100000000000004, 436: 0.0006899999999999988, 437: 0.002360000000000003, 438: 0.00017000000000000017, 439: 0.001976, 440: 0.0011439999999999998, 441: 0.0007999999999999996, 442: 0.0005400000000000003, 443: 0.00148, 444: 0.0023640000000000037, 445: -0.0008540000000000008, 446: -0.00038599999999999984, 447: 0.0011240000000000002, 448: 4.000000000000026e-05, 449: 0.0011659999999999986, 450: 0.002243999999999999, 451: -3.799999999999912e-05, 452: 0.0016359999999999997, 453: -0.00099, 454: -0.0002340000000000008, 455: 0.0011600000000000013, 456: 0.00025000000000000076, 457: 0.000977999999999996, 458: 0.00086, 459: 0.00027800000000000107, 460: 0.0008380000000000005, 461: -0.0018999999999999987, 462: 0.00207, 463: -2.4000000000000038e-05, 464: 0.00045600000000000073, 465: -7.000000000000005e-05, 466: 0.00219, 467: -0.00043000000000000015, 468: 0.0006840000000000007, 469: 0.0009400000000000016, 470: 0.0018879999999999997, 471: -0.0004959999999999993, 472: 0.0008799999999999999, 473: 0.002205999999999996, 474: 0.0019500000000000032, 475: -0.0005760000000000001, 476: 0.000423999999999999, 477: 0.0008759999999999999, 478: -0.001375999999999999, 479: 0.0020220000000000012, 480: 0.001691999999999998, 481: 0.0004000000000000002, 482: 0.0004840000000000002, 483: 0.0016920000000000019, 484: 0.002316000000000002, 485: 0.0006540000000000014, 486: 0.0008580000000000001, 487: -0.000572, 488: 0.0012839999999999987, 489: -0.0007339999999999998, 490: -3.999999999999562e-06, 491: 0.0010479999999999988, 492: 0.0009699999999999997, 493: 0.0007560000000000008, 494: 0.0014100000000000026, 495: 0.0018180000000000006, 496: 0.001740000000000002, 497: 0.0005620000000000007, 498: 0.0009979999999999993, 499: -0.001475999999999999, 500: 0.0006380000000000005, 501: -0.00022200000000000035, 502: 0.001120000000000001, 503: 0.001862000000000002, 504: 0.0009200000000000001, 505: 0.0015559999999999994, 506: 0.00164, 507: 0.0014120000000000018, 508: 0.0003079999999999986, 509: 0.0004720000000000012, 510: 0.0025099999999999966, 511: 0.0017800000000000008, 512: -0.0008179999999999985, 513: 0.0007419999999999991, 514: 0.00044799999999999994, 515: -1.0000000000000207e-05, 516: 0.0005519999999999989, 517: -0.0001419999999999992, 518: -0.00044999999999999966, 519: 0.000498, 520: 0.0004960000000000005, 521: 0.0007179999999999982, 522: -0.0006599999999999997, 523: -0.0001339999999999996, 524: -3.599999999999952e-05, 525: 0.0010700000000000002, 526: 9.800000000000051e-05, 527: 0.0003899999999999999, 528: 0.0009999999999999998, 529: 0.002786000000000002, 530: 8.200000000000067e-05, 531: 0.001379999999999998, 532: 0.001246, 533: 0.0009639999999999988, 534: 0.0020240000000000015, 535: -7.200000000000007e-05, 536: -0.0009539999999999976, 537: 0.001168, 538: 0.000831999999999999, 539: 0.00028599999999999974, 540: -0.00016599999999999945, 541: 0.0015559999999999997, 542: 0.0011520000000000007, 543: 0.00075, 544: 0.0005859999999999998, 545: 0.0017940000000000002, 546: 0.0018480000000000003, 547: -0.0006919999999999991, 548: 0.001533999999999998, 549: 0.0007939999999999959, 550: 0.00029600000000000015, 551: 0.0006539999999999985, 552: 0.0006139999999999989, 553: 0.0011419999999999972, 554: 0.0001019999999999995, 555: 0.001031999999999999, 556: 0.0007199999999999981, 557: 0.0036379999999999984, 558: -0.0012380000000000017, 559: 0.0010239999999999993, 560: -0.00048800000000000064, 561: 0.0004900000000000006, 562: 0.0019899999999999974, 563: 0.0008119999999999998, 564: -0.0003960000000000003, 565: 0.00043600000000000035, 566: -0.00021999999999999995, 567: 0.003, 568: 0.0018140000000000003, 569: 0.0006160000000000006, 570: 0.0008040000000000002, 571: -0.00024200000000000038, 572: -0.000807999999999998, 573: -0.00015200000000000014, 574: -0.0014019999999999998, 575: -0.0005740000000000003, 576: -0.0007299999999999987, 577: 0.001105999999999999, 578: -0.00020799999999999982, 579: 0.0007200000000000003, 580: 9.999999999999961e-05, 581: 0.0012740000000000002, 582: 0.0017820000000000023, 583: -0.0002579999999999998, 584: 0.001974000000000002, 585: 0.001978000000000002, 586: 0.0007300000000000002, 587: -7.800000000000074e-05, 588: 0.000712000000000003, 589: 0.000984, 590: 0.0003060000000000003, 591: -0.0006379999999999997, 592: 5.60000000000008e-05, 593: 0.0012120000000000026, 594: 0.00020599999999999953, 595: 0.000639999999999999, 596: 0.0025460000000000005, 597: 0.00030400000000000045, 598: 0.0007500000000000006, 599: 0.0017660000000000013, 600: 2.800000000000034e-05, 601: 0.0006779999999999998, 602: 0.0003440000000000001, 603: 0.0007979999999999984, 604: 0.00027799999999999966, 605: 0.0012679999999999992, 606: 0.0005979999999999997, 607: 0.0017760000000000005, 608: 0.00017000000000000072, 609: 6.400000000000105e-05, 610: 0.0020340000000000002, 611: 0.0015980000000000007, 612: 0.002334, 613: 0.0011959999999999996, 614: 0.0004500000000000013, 615: 0.00011999999999999973, 616: 0.0013039999999999996, 617: 0.0008360000000000004, 618: 0.0004479999999999997, 619: 0.0005939999999999988, 620: 0.0019740000000000005, 621: 0.0006720000000000006, 622: 0.0017840000000000028, 623: -0.0010859999999999993, 624: 0.0005420000000000007, 625: 0.0012239999999999994, 626: 0.003086000000000001, 627: 0.00020999999999999936, 628: 0.0008500000000000002, 629: 0.000607999999999999, 630: 0.0007319999999999984, 631: 0.002672000000000001, 632: 3.000000000000028e-05, 633: 0.001577999999999999, 634: 0.0009679999999999995, 635: 0.00039000000000000075, 636: -0.00027600000000000026, 637: 0.0012960000000000029, 638: 0.0010099999999999994, 639: 0.00047400000000000084, 640: 0.0007279999999999982, 641: 0.0009540000000000012, 642: -0.0006999999999999989, 643: 0.0010219999999999988, 644: 0.0008680000000000004, 645: 0.002398000000000001, 646: 4.6000000000000535e-05, 647: 0.0007459999999999987, 648: 0.00020599999999999885, 649: 0.0018059999999999977, 650: 0.002792000000000001, 651: 0.0001720000000000008, 652: -6.000000000000058e-05, 653: 0.0006260000000000003, 654: -0.0004779999999999992, 655: 0.0018339999999999984, 656: 0.001177999999999998, 657: -0.00037199999999999977, 658: 0.0012879999999999988, 659: -0.00025199999999999946, 660: 0.0014300000000000003, 661: -0.00014200000000000033, 662: 0.001038000000000001, 663: 0.0009479999999999992, 664: 0.0014259999999999998, 665: 0.0007879999999999989, 666: 0.0006199999999999991, 667: 0.0021919999999999986, 668: -0.0008560000000000027, 669: 0.0003179999999999989, 670: -0.000597999999999999, 671: -0.0008500000000000001, 672: 0.0004100000000000006, 673: 0.0015659999999999988, 674: -0.0017699999999999994, 675: 0.0004159999999999996, 676: 0.000738, 677: -7.79999999999991e-05, 678: -0.0005759999999999991, 679: -3.999999999999735e-06, 680: -0.0008839999999999981, 681: 0.0021040000000000013, 682: 0.0011120000000000001, 683: 0.0008420000000000003, 684: 0.003653999999999998, 685: 0.0010739999999999997, 686: 0.0021759999999999982, 687: 0.0017120000000000013, 688: 0.0005939999999999987, 689: 0.0017119999999999982, 690: 0.0015900000000000003, 691: 0.0015379999999999984, 692: 0.0017439999999999995, 693: 0.0008380000000000006, 694: 0.0010239999999999987, 695: 0.0009160000000000004, 696: 0.0015020000000000003, 697: 0.0012800000000000016, 698: 0.00023800000000000118, 699: 0.00078, 700: 1.1999999999999741e-05, 701: -0.00033600000000000134, 702: 0.0006400000000000004, 703: 0.0017820000000000023, 704: 0.000640000000000001, 705: 0.00029200000000000016, 706: -0.0005640000000000002, 707: -0.00018199999999999927, 708: 0.0009819999999999978, 709: -0.0005599999999999997, 710: 0.002087999999999997, 711: 0.0016939999999999998, 712: 0.0003040000000000005, 713: 0.0005039999999999992, 714: 0.001268000000000001, 715: 0.0005040000000000006, 716: 0.0009780000000000014, 717: 0.0016279999999999995, 718: -0.00038799999999999935, 719: -0.0013640000000000006, 720: 0.0016260000000000005, 721: 0.0006519999999999997, 722: 0.0015760000000000019, 723: 0.0010719999999999992, 724: 0.0009980000000000006, 725: 0.0011200000000000016, 726: 0.00044399999999999925, 727: -0.0003940000000000002, 728: 0.0005560000000000008, 729: -0.0007339999999999995, 730: 0.0015060000000000004, 731: 0.0011599999999999994, 732: 0.0008579999999999997, 733: -0.0011439999999999988, 734: 0.0018580000000000016, 735: 0.00040599999999999984, 736: 0.000888, 737: 0.0015799999999999985, 738: 0.0013260000000000004, 739: 0.003487999999999998, 740: 0.003684, 741: 0.0011299999999999967, 742: 0.002544000000000001, 743: 0.000925999999999999, 744: 0.001048, 745: 0.0010160000000000008, 746: 0.0007279999999999984, 747: 9.00000000000007e-05, 748: 0.0006640000000000007, 749: -7.000000000000001e-05, 750: 0.0006640000000000006, 751: 0.00019599999999999972, 752: -0.0004980000000000006, 753: 0.001224, 754: 0.0017640000000000006, 755: 0.0015940000000000006, 756: 0.0009179999999999984, 757: -0.0009439999999999986, 758: 0.0025720000000000022, 759: 0.0018559999999999985, 760: 0.001088000000000001, 761: 0.00014400000000000063, 762: 0.0008559999999999996, 763: -0.00041600000000000025, 764: 0.002273999999999999, 765: -0.0011119999999999986, 766: -0.001316000000000001, 767: 0.0020300000000000014, 768: 0.0004680000000000013, 769: 0.0016299999999999986, 770: 0.002243999999999995, 771: 0.0004480000000000003, 772: 0.0008520000000000018, 773: 0.0003440000000000013, 774: -0.00028, 775: 0.0004819999999999998, 776: -9.999999999999948e-05, 777: 0.000208, 778: 0.0017200000000000023, 779: 0.001769999999999997, 780: 0.0002580000000000006, 781: 0.000501999999999999, 782: 0.001941999999999999, 783: 0.0018459999999999974, 784: 0.0011260000000000007, 785: 0.0007959999999999993, 786: -7.400000000000074e-05, 787: 0.0018499999999999975, 788: -0.0003520000000000005, 789: -0.0008059999999999993, 790: 0.0004999999999999988, 791: 0.0009680000000000005, 792: -0.0005880000000000006, 793: -0.0004279999999999995, 794: 0.001003999999999999, 795: 0.0003959999999999995, 796: 0.0007359999999999999, 797: 0.0007680000000000014, 798: -0.0002599999999999999, 799: 0.001697999999999999, 800: -0.0002640000000000009, 801: 0.000964, 802: 0.0009620000000000042, 803: 0.0010140000000000014, 804: 0.0018179999999999997, 805: 0.0010400000000000001, 806: 0.0005280000000000011, 807: -0.00014999999999999896, 808: 0.0009640000000000002, 809: 0.001754, 810: 0.0012700000000000014, 811: -0.0007859999999999996, 812: -0.00048599999999999804, 813: 0.0007980000000000006, 814: 0.0014380000000000007, 815: 0.001971999999999999, 816: 0.0013220000000000005, 817: -0.00017599999999999975, 818: -0.00024399999999999964, 819: 0.0007479999999999996, 820: 0.0015219999999999984, 821: -0.0004699999999999994, 822: 0.000140000000000001, 823: 0.0004900000000000018, 824: 0.001022000000000004, 825: 0.000961999999999998, 826: 0.00022600000000000015, 827: -0.000728, 828: 0.003075999999999998, 829: 0.001, 830: 0.0006159999999999991, 831: 0.0009420000000000003, 832: 0.0007580000000000005, 833: -2.9999999999999997e-05, 834: 0.0020280000000000025, 835: 0.0011359999999999988, 836: 0.00015199999999999868, 837: 0.0014560000000000003, 838: 0.000272000000000001, 839: 0.00036800000000000027, 840: -6.199999999999973e-05, 841: 6.799999999999969e-05, 842: -0.0010960000000000002, 843: 6.0000000000000056e-05, 844: -0.0008720000000000005, 845: 0.0005719999999999989, 846: 0.0001319999999999983, 847: 0.0006140000000000001, 848: -0.00010399999999999934, 849: -0.00013600000000000013, 850: -2.200000000000001e-05, 851: 0.0023159999999999973, 852: 0.0007880000000000005, 853: -0.001234, 854: -0.0005379999999999987, 855: 0.0014199999999999985, 856: 0.0013620000000000006, 857: 0.001108000000000001, 858: 0.0005100000000000016, 859: 0.0006779999999999997, 860: 0.0013499999999999994, 861: 2.4000000000001024e-05, 862: 0.0015720000000000013, 863: 0.0009800000000000006, 864: 0.0011659999999999997, 865: 0.0011639999999999997, 866: 0.0009100000000000004, 867: -0.00039600000000000144, 868: 0.00043800000000000067, 869: -0.0005579999999999991, 870: 0.000501999999999999, 871: 0.001598, 872: 0.0018159999999999993, 873: 0.0006380000000000003, 874: 0.0005440000000000001, 875: 0.0020880000000000057, 876: -0.0007379999999999983, 877: 0.001407999999999999, 878: 0.002513999999999999, 879: -0.0014159999999999976, 880: -0.0005819999999999999, 881: 0.001933999999999998, 882: -0.001285999999999998, 883: 0.0013380000000000026, 884: -0.0007199999999999996, 885: 0.0027259999999999984, 886: 0.001004000000000001, 887: 0.0009700000000000008, 888: 0.00024000000000000017, 889: 0.002374000000000001, 890: 0.0005280000000000009, 891: 0.0007139999999999997, 892: 0.0006980000000000006, 893: 0.0007579999999999991, 894: 0.001130000000000002, 895: -0.002116, 896: -0.000903999999999999, 897: -0.00042799999999999994, 898: 0.0039480000000000045, 899: 0.00025799999999999993, 900: -0.00040199999999999915, 901: -7.599999999999994e-05, 902: 0.000645999999999999, 903: 0.0008079999999999988, 904: 0.00185, 905: 0.001268000000000002, 906: 0.0025960000000000015, 907: -0.00018999999999999906, 908: 0.0008220000000000006, 909: 0.001427999999999999, 910: 0.0016900000000000018, 911: -0.0004199999999999997, 912: 0.0007659999999999994, 913: 0.0016340000000000011, 914: 0.0004959999999999985, 915: 0.0015979999999999996, 916: 0.0011320000000000002, 917: 0.0018499999999999999, 918: 0.0011739999999999993, 919: 9.600000000000002e-05, 920: 0.00022400000000000065, 921: -0.00011599999999999999, 922: 0.002907999999999998, 923: 0.0012639999999999993, 924: -0.0001279999999999994, 925: 0.000248, 926: 0.0006600000000000014, 927: 0.0006319999999999987, 928: -0.0006299999999999988, 929: 0.0010579999999999997, 930: 0.00017400000000000136, 931: 0.0025979999999999996, 932: -0.0008760000000000005, 933: 0.001652000000000001, 934: -0.00021399999999999924, 935: 0.0018540000000000008, 936: -0.0009840000000000005, 937: 0.0011600000000000009, 938: 0.00015199999999999933, 939: 0.0015380000000000005, 940: 0.0006640000000000012, 941: 0.0004980000000000006, 942: 0.0008199999999999999, 943: -0.00022600000000000002, 944: 7.600000000000103e-05, 945: 0.0024339999999999987, 946: 0.000646, 947: 0.0007839999999999988, 948: 0.00038400000000000104, 949: 0.0002539999999999999, 950: 0.00040800000000000114, 951: 0.0009000000000000013, 952: 0.0018819999999999991, 953: 0.0005399999999999994, 954: -0.0003879999999999994, 955: 0.0019160000000000008, 956: 0.001254, 957: 0.002186000000000002, 958: -9.599999999999831e-05, 959: 0.0008919999999999989, 960: 0.0008340000000000001, 961: 0.0022600000000000003, 962: 0.0007980000000000001, 963: 0.0014899999999999998, 964: 0.00026199999999999997, 965: -0.0003139999999999991, 966: 0.00026999999999999957, 967: 0.0008439999999999981, 968: 0.0016960000000000005, 969: -0.0013439999999999984, 970: 0.0016379999999999997, 971: 0.0014999999999999985, 972: 0.0011959999999999994, 973: -6.199999999999993e-05, 974: 0.0007640000000000017, 975: -0.00033200000000000026, 976: 0.00048400000000000054, 977: 0.0007939999999999988, 978: -0.0007219999999999989, 979: 0.0016919999999999997, 980: 0.0002779999999999987, 981: -0.0014939999999999997, 982: 0.0002559999999999999, 983: 0.0019819999999999994, 984: 0.0014739999999999987, 985: 0.00038000000000000094, 986: -0.0002659999999999994, 987: 0.0020919999999999975, 988: 0.0004519999999999996, 989: 0.0011700000000000013, 990: 0.0018619999999999995, 991: 0.0005899999999999999, 992: 0.002895999999999999, 993: 0.0006299999999999994, 994: 0.0021259999999999972, 995: -0.000652000000000001, 996: -0.0004300000000000004, 997: 0.0007259999999999999, 998: -0.0002960000000000007, 999: 0.001158}\n"
     ]
    }
   ],
   "source": [
    "with open('vals_mt.pkl', 'rb') as f:\n",
    "    accs = pickle.load(f)\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(372, 0.0075200000000000015), (287, 0.0044579999999999976), (898, 0.0039480000000000045), (307, 0.003934000000000007), (740, 0.003684), (684, 0.003653999999999998), (557, 0.0036379999999999984), (391, 0.003610000000000004), (394, 0.003530000000000004), (739, 0.003487999999999998), (270, 0.0032039999999999985), (626, 0.003086000000000001), (828, 0.003075999999999998), (230, 0.0030720000000000022), (178, 0.0030559999999999993), (149, 0.003), (567, 0.003), (249, 0.0029219999999999966), (922, 0.002907999999999998), (992, 0.002895999999999999), (89, 0.002871999999999996), (411, 0.002822000000000002), (166, 0.002816), (650, 0.002792000000000001), (529, 0.002786000000000002), (885, 0.0027259999999999984), (631, 0.002672000000000001), (213, 0.002654), (70, 0.0026360000000000008), (331, 0.002618), (931, 0.0025979999999999996), (906, 0.0025960000000000015), (758, 0.0025720000000000022), (343, 0.002572), (226, 0.002569999999999996), (596, 0.0025460000000000005), (742, 0.002544000000000001), (432, 0.0025279999999999994), (297, 0.002516), (878, 0.002513999999999999), (216, 0.0025120000000000025), (510, 0.0025099999999999966), (945, 0.0024339999999999987), (645, 0.002398000000000001), (889, 0.002374000000000001), (444, 0.0023640000000000037), (10, 0.002362000000000002), (437, 0.002360000000000003), (150, 0.002352), (612, 0.002334), (369, 0.0023219999999999985), (484, 0.002316000000000002), (851, 0.0023159999999999973), (175, 0.002314000000000002), (435, 0.0023100000000000004), (764, 0.002273999999999999), (206, 0.002261999999999998), (961, 0.0022600000000000003), (85, 0.002243999999999999), (450, 0.002243999999999999), (770, 0.002243999999999995), (300, 0.002221999999999999), (95, 0.002217999999999997), (473, 0.002205999999999996), (341, 0.0021960000000000013), (429, 0.0021919999999999995), (667, 0.0021919999999999986), (466, 0.00219), (957, 0.002186000000000002), (686, 0.0021759999999999982), (266, 0.002173999999999999), (332, 0.0021700000000000014), (224, 0.0021639999999999962), (350, 0.0021559999999999978), (106, 0.002144000000000001), (994, 0.0021259999999999972), (681, 0.0021040000000000013), (91, 0.0021039999999999987), (987, 0.0020919999999999975), (81, 0.0020899999999999994), (875, 0.0020880000000000057), (710, 0.002087999999999997), (462, 0.00207), (92, 0.0020579999999999956), (160, 0.0020519999999999974), (257, 0.0020499999999999993), (201, 0.0020460000000000035), (235, 0.002046000000000003), (311, 0.002035999999999998), (610, 0.0020340000000000002), (767, 0.0020300000000000014), (834, 0.0020280000000000025), (303, 0.0020240000000000032), (534, 0.0020240000000000015), (479, 0.0020220000000000012), (318, 0.002006), (162, 0.0019899999999999983), (562, 0.0019899999999999974), (983, 0.0019819999999999994), (585, 0.001978000000000002), (253, 0.001976000000000002), (439, 0.001976), (4, 0.001975999999999998), (584, 0.001974000000000002), (620, 0.0019740000000000005), (815, 0.001971999999999999), (199, 0.0019679999999999997), (193, 0.001957999999999999), (474, 0.0019500000000000032), (782, 0.001941999999999999), (74, 0.001933999999999998), (881, 0.001933999999999998), (72, 0.0019279999999999976), (9, 0.0019260000000000006), (955, 0.0019160000000000008), (470, 0.0018879999999999997), (136, 0.0018840000000000005), (952, 0.0018819999999999991), (277, 0.0018779999999999984), (51, 0.0018699999999999993), (503, 0.001862000000000002), (990, 0.0018619999999999995), (734, 0.0018580000000000016), (759, 0.0018559999999999985), (935, 0.0018540000000000008), (904, 0.00185), (917, 0.0018499999999999999), (787, 0.0018499999999999975), (546, 0.0018480000000000003), (783, 0.0018459999999999974), (59, 0.0018379999999999978), (263, 0.0018340000000000008), (655, 0.0018339999999999984), (495, 0.0018180000000000006), (804, 0.0018179999999999997), (58, 0.0018160000000000006), (872, 0.0018159999999999993), (568, 0.0018140000000000003), (649, 0.0018059999999999977), (76, 0.0018020000000000009), (382, 0.0018020000000000004), (545, 0.0017940000000000002), (77, 0.0017920000000000028), (15, 0.0017879999999999975), (622, 0.0017840000000000028), (289, 0.001784), (582, 0.0017820000000000023), (703, 0.0017820000000000023), (511, 0.0017800000000000008), (607, 0.0017760000000000005), (5, 0.001773999999999998), (779, 0.001769999999999997), (599, 0.0017660000000000013), (754, 0.0017640000000000006), (256, 0.0017580000000000017), (809, 0.001754), (364, 0.0017500000000000018), (111, 0.001746000000000004), (692, 0.0017439999999999995), (45, 0.0017439999999999986), (496, 0.001740000000000002), (61, 0.0017319999999999994), (415, 0.0017299999999999991), (265, 0.0017260000000000005), (254, 0.0017259999999999986), (778, 0.0017200000000000023), (403, 0.0017199999999999958), (687, 0.0017120000000000013), (689, 0.0017119999999999982), (24, 0.0017059999999999996), (799, 0.001697999999999999), (968, 0.0016960000000000005), (711, 0.0016939999999999998), (483, 0.0016920000000000019), (979, 0.0016919999999999997), (480, 0.001691999999999998), (910, 0.0016900000000000018), (101, 0.0016879999999999996), (183, 0.001686000000000001), (279, 0.0016859999999999985), (227, 0.0016859999999999978), (340, 0.0016760000000000006), (41, 0.0016740000000000004), (383, 0.001669999999999998), (334, 0.0016659999999999995), (47, 0.0016620000000000016), (80, 0.0016560000000000008), (151, 0.0016539999999999988), (933, 0.001652000000000001), (262, 0.0016500000000000006), (165, 0.0016499999999999998), (506, 0.00164), (970, 0.0016379999999999997), (158, 0.0016360000000000003), (452, 0.0016359999999999997), (913, 0.0016340000000000011), (769, 0.0016299999999999986), (717, 0.0016279999999999995), (720, 0.0016260000000000005), (25, 0.0016219999999999993), (103, 0.001620000000000001), (33, 0.0016099999999999999), (359, 0.0016060000000000017), (144, 0.0016019999999999993), (37, 0.0016019999999999988), (611, 0.0015980000000000007), (871, 0.001598), (915, 0.0015979999999999996), (755, 0.0015940000000000006), (690, 0.0015900000000000003), (200, 0.0015880000000000022), (52, 0.0015800000000000022), (737, 0.0015799999999999985), (633, 0.001577999999999999), (722, 0.0015760000000000019), (862, 0.0015720000000000013), (673, 0.0015659999999999988), (541, 0.0015559999999999997), (505, 0.0015559999999999994), (19, 0.0015500000000000008), (163, 0.001548), (428, 0.0015460000000000003), (939, 0.0015380000000000005), (691, 0.0015379999999999984), (152, 0.0015339999999999993), (548, 0.001533999999999998), (306, 0.001524000000000001), (208, 0.0015220000000000025), (820, 0.0015219999999999984), (49, 0.0015139999999999989), (730, 0.0015060000000000004), (35, 0.0015059999999999993), (696, 0.0015020000000000003), (971, 0.0014999999999999985), (169, 0.0014980000000000002), (963, 0.0014899999999999998), (87, 0.0014899999999999991), (404, 0.0014899999999999987), (443, 0.00148), (214, 0.0014780000000000004), (984, 0.0014739999999999987), (434, 0.0014640000000000015), (126, 0.0014639999999999994), (79, 0.0014599999999999993), (837, 0.0014560000000000003), (814, 0.0014380000000000007), (413, 0.0014319999999999992), (225, 0.0014300000000000011), (660, 0.0014300000000000003), (909, 0.001427999999999999), (664, 0.0014259999999999998), (204, 0.0014199999999999992), (855, 0.0014199999999999985), (507, 0.0014120000000000018), (494, 0.0014100000000000026), (877, 0.001407999999999999), (305, 0.0014000000000000004), (44, 0.0013999999999999993), (378, 0.0013979999999999989), (145, 0.0013920000000000007), (425, 0.001389999999999999), (71, 0.001387999999999999), (187, 0.001382), (531, 0.001379999999999998), (310, 0.0013779999999999995), (376, 0.0013739999999999996), (68, 0.0013739999999999987), (325, 0.0013659999999999998), (417, 0.0013620000000000017), (856, 0.0013620000000000006), (860, 0.0013499999999999994), (883, 0.0013380000000000026), (738, 0.0013260000000000004), (365, 0.0013259999999999997), (816, 0.0013220000000000005), (616, 0.0013039999999999996), (637, 0.0012960000000000029), (658, 0.0012879999999999988), (488, 0.0012839999999999987), (12, 0.0012820000000000002), (697, 0.0012800000000000016), (196, 0.001276), (581, 0.0012740000000000002), (810, 0.0012700000000000014), (905, 0.001268000000000002), (714, 0.001268000000000001), (605, 0.0012679999999999992), (424, 0.0012659999999999998), (923, 0.0012639999999999993), (956, 0.001254), (532, 0.001246), (99, 0.0012400000000000015), (396, 0.001234000000000001), (753, 0.001224), (625, 0.0012239999999999994), (593, 0.0012120000000000026), (392, 0.0012), (613, 0.0011959999999999996), (972, 0.0011959999999999994), (146, 0.0011920000000000001), (400, 0.0011839999999999993), (268, 0.0011800000000000005), (656, 0.001177999999999998), (309, 0.001174000000000001), (918, 0.0011739999999999993), (989, 0.0011700000000000013), (537, 0.001168), (864, 0.0011659999999999997), (449, 0.0011659999999999986), (865, 0.0011639999999999997), (455, 0.0011600000000000013), (937, 0.0011600000000000009), (731, 0.0011599999999999994), (999, 0.001158), (298, 0.0011560000000000008), (542, 0.0011520000000000007), (440, 0.0011439999999999998), (553, 0.0011419999999999972), (27, 0.0011360000000000025), (835, 0.0011359999999999988), (264, 0.0011340000000000011), (916, 0.0011320000000000002), (894, 0.001130000000000002), (409, 0.0011299999999999988), (741, 0.0011299999999999967), (784, 0.0011260000000000007), (447, 0.0011240000000000002), (725, 0.0011200000000000016), (502, 0.001120000000000001), (240, 0.0011159999999999985), (682, 0.0011120000000000001), (857, 0.001108000000000001), (577, 0.001105999999999999), (42, 0.0011020000000000005), (290, 0.0010920000000000018), (760, 0.001088000000000001), (215, 0.0010879999999999998), (368, 0.0010839999999999997), (172, 0.001078000000000001), (195, 0.001074000000000001), (685, 0.0010739999999999997), (723, 0.0010719999999999992), (525, 0.0010700000000000002), (929, 0.0010579999999999997), (238, 0.0010560000000000003), (744, 0.001048), (491, 0.0010479999999999988), (805, 0.0010400000000000001), (171, 0.0010399999999999997), (662, 0.001038000000000001), (134, 0.0010339999999999995), (555, 0.001031999999999999), (140, 0.0010280000000000003), (559, 0.0010239999999999993), (694, 0.0010239999999999987), (48, 0.0010239999999999984), (824, 0.001022000000000004), (308, 0.0010220000000000003), (643, 0.0010219999999999988), (16, 0.0010200000000000007), (745, 0.0010160000000000008), (50, 0.0010160000000000002), (370, 0.0010140000000000017), (803, 0.0010140000000000014), (113, 0.0010119999999999992), (638, 0.0010099999999999994), (153, 0.0010060000000000021), (886, 0.001004000000000001), (794, 0.001003999999999999), (829, 0.001), (528, 0.0009999999999999998), (412, 0.0009999999999999994), (724, 0.0009980000000000006), (498, 0.0009979999999999993), (2, 0.0009899999999999993), (589, 0.000984), (315, 0.0009839999999999988), (56, 0.0009839999999999985), (708, 0.0009819999999999978), (863, 0.0009800000000000006), (716, 0.0009780000000000014), (457, 0.000977999999999996), (176, 0.0009720000000000007), (887, 0.0009700000000000008), (492, 0.0009699999999999997), (791, 0.0009680000000000005), (634, 0.0009679999999999995), (808, 0.0009640000000000002), (801, 0.000964), (533, 0.0009639999999999988), (802, 0.0009620000000000042), (825, 0.000961999999999998), (167, 0.0009599999999999985), (641, 0.0009540000000000012), (663, 0.0009479999999999992), (141, 0.0009440000000000017), (831, 0.0009420000000000003), (387, 0.0009400000000000016), (469, 0.0009400000000000016), (362, 0.0009359999999999991), (260, 0.0009340000000000007), (349, 0.0009320000000000006), (743, 0.000925999999999999), (384, 0.0009239999999999997), (504, 0.0009200000000000001), (756, 0.0009179999999999984), (398, 0.0009160000000000012), (21, 0.0009160000000000006), (695, 0.0009160000000000004), (109, 0.000915999999999997), (866, 0.0009100000000000004), (177, 0.0009039999999999994), (18, 0.0009019999999999995), (951, 0.0009000000000000013), (66, 0.0008959999999999991), (34, 0.0008940000000000005), (959, 0.0008919999999999989), (736, 0.000888), (472, 0.0008799999999999999), (250, 0.0008799999999999998), (477, 0.0008759999999999999), (379, 0.0008700000000000016), (644, 0.0008680000000000004), (395, 0.0008659999999999997), (458, 0.00086), (107, 0.0008580000000000024), (219, 0.000858000000000001), (486, 0.0008580000000000001), (732, 0.0008579999999999997), (762, 0.0008559999999999996), (772, 0.0008520000000000018), (628, 0.0008500000000000002), (410, 0.0008500000000000001), (244, 0.0008499999999999993), (301, 0.0008440000000000011), (405, 0.0008440000000000003), (220, 0.0008439999999999989), (967, 0.0008439999999999981), (683, 0.0008420000000000003), (693, 0.0008380000000000006), (460, 0.0008380000000000005), (93, 0.0008380000000000001), (118, 0.0008380000000000001), (617, 0.0008360000000000004), (960, 0.0008340000000000001), (538, 0.000831999999999999), (133, 0.0008240000000000014), (36, 0.0008240000000000006), (908, 0.0008220000000000006), (942, 0.0008199999999999999), (563, 0.0008119999999999998), (903, 0.0008079999999999988), (570, 0.0008040000000000002), (83, 0.000800000000000001), (441, 0.0007999999999999996), (813, 0.0007980000000000006), (386, 0.0007980000000000002), (962, 0.0007980000000000001), (603, 0.0007979999999999984), (785, 0.0007959999999999993), (17, 0.0007939999999999988), (977, 0.0007939999999999988), (549, 0.0007939999999999959), (852, 0.0007880000000000005), (665, 0.0007879999999999989), (313, 0.0007859999999999998), (159, 0.0007859999999999996), (947, 0.0007839999999999988), (55, 0.0007820000000000007), (431, 0.0007819999999999996), (699, 0.00078), (7, 0.0007699999999999997), (797, 0.0007680000000000014), (912, 0.0007659999999999994), (974, 0.0007640000000000017), (117, 0.0007639999999999995), (67, 0.0007580000000000009), (832, 0.0007580000000000005), (893, 0.0007579999999999991), (493, 0.0007560000000000008), (229, 0.0007539999999999998), (598, 0.0007500000000000006), (543, 0.00075), (57, 0.0007499999999999998), (63, 0.0007499999999999996), (819, 0.0007479999999999996), (393, 0.0007460000000000004), (647, 0.0007459999999999987), (513, 0.0007419999999999991), (676, 0.000738), (173, 0.0007379999999999997), (796, 0.0007359999999999999), (209, 0.0007359999999999991), (345, 0.0007340000000000005), (630, 0.0007319999999999984), (269, 0.0007300000000000007), (233, 0.0007300000000000002), (586, 0.0007300000000000002), (746, 0.0007279999999999984), (640, 0.0007279999999999982), (130, 0.0007260000000000004), (997, 0.0007259999999999999), (86, 0.0007259999999999994), (114, 0.0007259999999999992), (579, 0.0007200000000000003), (556, 0.0007199999999999981), (521, 0.0007179999999999982), (402, 0.0007159999999999998), (891, 0.0007139999999999997), (588, 0.000712000000000003), (11, 0.0007120000000000026), (373, 0.0007119999999999991), (60, 0.0007080000000000011), (408, 0.0006980000000000028), (892, 0.0006980000000000006), (115, 0.0006980000000000005), (416, 0.0006979999999999996), (436, 0.0006899999999999988), (3, 0.000688), (419, 0.0006860000000000019), (468, 0.0006840000000000007), (194, 0.0006819999999999993), (601, 0.0006779999999999998), (859, 0.0006779999999999997), (217, 0.0006759999999999994), (621, 0.0006720000000000006), (293, 0.0006679999999999988), (940, 0.0006640000000000012), (275, 0.0006640000000000011), (748, 0.0006640000000000007), (750, 0.0006640000000000006), (116, 0.0006639999999999983), (926, 0.0006600000000000014), (135, 0.0006560000000000001), (273, 0.0006559999999999999), (485, 0.0006540000000000014), (321, 0.0006539999999999999), (551, 0.0006539999999999985), (721, 0.0006519999999999997), (946, 0.000646), (84, 0.0006459999999999992), (902, 0.000645999999999999), (122, 0.0006439999999999997), (377, 0.0006420000000000008), (26, 0.0006419999999999998), (148, 0.0006419999999999995), (704, 0.000640000000000001), (702, 0.0006400000000000004), (595, 0.000639999999999999), (500, 0.0006380000000000005), (873, 0.0006380000000000003), (927, 0.0006319999999999987), (993, 0.0006299999999999994), (131, 0.0006279999999999999), (653, 0.0006260000000000003), (274, 0.0006220000000000008), (192, 0.0006200000000000007), (666, 0.0006199999999999991), (333, 0.0006179999999999994), (569, 0.0006160000000000006), (830, 0.0006159999999999991), (847, 0.0006140000000000001), (552, 0.0006139999999999989), (363, 0.0006120000000000001), (180, 0.0006119999999999999), (223, 0.0006119999999999999), (629, 0.000607999999999999), (132, 0.0006040000000000007), (606, 0.0005979999999999997), (381, 0.000597999999999999), (619, 0.0005939999999999988), (688, 0.0005939999999999987), (991, 0.0005899999999999999), (544, 0.0005859999999999998), (205, 0.0005820000000000011), (371, 0.0005819999999999997), (280, 0.0005799999999999989), (212, 0.0005759999999999989), (845, 0.0005719999999999989), (43, 0.0005680000000000001), (228, 0.0005640000000000007), (14, 0.0005640000000000003), (497, 0.0005620000000000007), (110, 0.0005619999999999991), (327, 0.0005599999999999986), (355, 0.0005579999999999997), (728, 0.0005560000000000008), (312, 0.0005519999999999991), (516, 0.0005519999999999989), (385, 0.0005499999999999988), (420, 0.0005480000000000012), (127, 0.0005459999999999998), (874, 0.0005440000000000001), (624, 0.0005420000000000007), (442, 0.0005400000000000003), (953, 0.0005399999999999994), (271, 0.000534), (806, 0.0005280000000000011), (890, 0.0005280000000000009), (401, 0.0005260000000000005), (234, 0.0005259999999999999), (389, 0.0005259999999999999), (399, 0.0005220000000000002), (352, 0.0005199999999999996), (147, 0.0005140000000000021), (858, 0.0005100000000000016), (715, 0.0005040000000000006), (713, 0.0005039999999999992), (781, 0.000501999999999999), (870, 0.000501999999999999), (790, 0.0004999999999999988), (941, 0.0004980000000000006), (519, 0.000498), (520, 0.0004960000000000005), (914, 0.0004959999999999985), (823, 0.0004900000000000018), (561, 0.0004900000000000006), (323, 0.00048800000000000075), (976, 0.00048400000000000054), (482, 0.0004840000000000002), (22, 0.00048200000000000033), (775, 0.0004819999999999998), (397, 0.0004799999999999986), (639, 0.00047400000000000084), (509, 0.0004720000000000012), (236, 0.0004700000000000005), (768, 0.0004680000000000013), (8, 0.0004660000000000004), (179, 0.00046599999999999946), (112, 0.000458), (292, 0.0004579999999999986), (464, 0.00045600000000000073), (347, 0.0004559999999999984), (284, 0.0004539999999999998), (121, 0.0004539999999999992), (358, 0.0004520000000000004), (988, 0.0004519999999999996), (614, 0.0004500000000000013), (326, 0.0004499999999999992), (771, 0.0004480000000000003), (514, 0.00044799999999999994), (618, 0.0004479999999999997), (414, 0.0004460000000000003), (139, 0.00044599999999999994), (726, 0.00044399999999999925), (868, 0.00043800000000000067), (380, 0.00043600000000000095), (565, 0.00043600000000000035), (282, 0.00042999999999999934), (156, 0.0004279999999999999), (476, 0.000423999999999999), (157, 0.000416), (675, 0.0004159999999999996), (30, 0.00041400000000000106), (248, 0.00041200000000000167), (672, 0.0004100000000000006), (62, 0.0004099999999999996), (950, 0.00040800000000000114), (735, 0.00040599999999999984), (481, 0.0004000000000000002), (189, 0.00039799999999999965), (795, 0.0003959999999999995), (124, 0.00039399999999999955), (635, 0.00039000000000000075), (527, 0.0003899999999999999), (948, 0.00038400000000000104), (985, 0.00038000000000000094), (314, 0.0003720000000000017), (839, 0.00036800000000000027), (239, 0.00036400000000000007), (337, 0.00036200000000000007), (190, 0.0003540000000000002), (294, 0.0003499999999999996), (302, 0.00034600000000000066), (773, 0.0003440000000000013), (602, 0.0003440000000000001), (406, 0.00033600000000000134), (427, 0.0003320000000000015), (366, 0.0003240000000000001), (261, 0.00032000000000000084), (202, 0.00031999999999999976), (100, 0.00031799999999999927), (669, 0.0003179999999999989), (291, 0.00031400000000000004), (108, 0.0003100000000000004), (210, 0.0003099999999999997), (508, 0.0003079999999999986), (181, 0.0003060000000000011), (590, 0.0003060000000000003), (712, 0.0003040000000000005), (597, 0.00030400000000000045), (182, 0.0003040000000000001), (221, 0.00030199999999999953), (346, 0.0003000000000000004), (211, 0.0002980000000000013), (550, 0.00029600000000000015), (154, 0.0002959999999999995), (28, 0.00029400000000000064), (705, 0.00029200000000000016), (304, 0.0002880000000000005), (539, 0.00028599999999999974), (243, 0.0002859999999999991), (120, 0.00028000000000000095), (459, 0.00027800000000000107), (604, 0.00027799999999999966), (980, 0.0002779999999999987), (838, 0.000272000000000001), (966, 0.00026999999999999957), (155, 0.00026600000000000034), (964, 0.00026199999999999997), (780, 0.0002580000000000006), (899, 0.00025799999999999993), (982, 0.0002559999999999999), (949, 0.0002539999999999999), (456, 0.00025000000000000076), (925, 0.000248), (888, 0.00024000000000000017), (698, 0.00023800000000000118), (826, 0.00022600000000000015), (920, 0.00022400000000000065), (627, 0.00020999999999999936), (777, 0.000208), (39, 0.00020799999999999963), (357, 0.00020600000000000056), (594, 0.00020599999999999953), (648, 0.00020599999999999885), (324, 0.00020200000000000009), (751, 0.00019599999999999972), (191, 0.00018999999999999969), (20, 0.0001859999999999987), (322, 0.00017800000000000067), (185, 0.0001759999999999991), (930, 0.00017400000000000136), (241, 0.0001740000000000001), (651, 0.0001720000000000008), (608, 0.00017000000000000072), (438, 0.00017000000000000017), (356, 0.00016799999999999915), (246, 0.0001640000000000006), (316, 0.0001519999999999996), (938, 0.00015199999999999933), (836, 0.00015199999999999868), (761, 0.00014400000000000063), (82, 0.00014200000000000112), (822, 0.000140000000000001), (846, 0.0001319999999999983), (615, 0.00011999999999999973), (104, 0.00011799999999999964), (554, 0.0001019999999999995), (580, 9.999999999999961e-05), (526, 9.800000000000051e-05), (919, 9.600000000000002e-05), (747, 9.00000000000007e-05), (530, 8.200000000000067e-05), (944, 7.600000000000103e-05), (841, 6.799999999999969e-05), (609, 6.400000000000105e-05), (360, 6.200000000000007e-05), (843, 6.0000000000000056e-05), (354, 5.799999999999989e-05), (592, 5.60000000000008e-05), (421, 4.799999999999975e-05), (646, 4.6000000000000535e-05), (29, 4.199999999999945e-05), (448, 4.000000000000026e-05), (54, 3.1999999999998637e-05), (632, 3.000000000000028e-05), (600, 2.800000000000034e-05), (861, 2.4000000000001024e-05), (700, 1.1999999999999741e-05), (351, 4.000000000000923e-06), (344, -1.999999999999815e-06), (490, -3.999999999999562e-06), (679, -3.999999999999735e-06), (317, -5.999999999998748e-06), (319, -6.0000000000001645e-06), (515, -1.0000000000000207e-05), (426, -1.1999999999999387e-05), (105, -2.1999999999999593e-05), (850, -2.200000000000001e-05), (463, -2.4000000000000038e-05), (833, -2.9999999999999997e-05), (259, -3.199999999999978e-05), (524, -3.599999999999952e-05), (451, -3.799999999999912e-05), (129, -3.800000000000069e-05), (138, -4.19999999999991e-05), (97, -5.3999999999998914e-05), (296, -5.400000000000008e-05), (255, -5.9999999999999365e-05), (652, -6.000000000000058e-05), (840, -6.199999999999973e-05), (973, -6.199999999999993e-05), (90, -6.999999999999944e-05), (749, -7.000000000000001e-05), (465, -7.000000000000005e-05), (123, -7.1999999999999e-05), (535, -7.200000000000007e-05), (423, -7.399999999999983e-05), (786, -7.400000000000074e-05), (901, -7.599999999999994e-05), (677, -7.79999999999991e-05), (587, -7.800000000000074e-05), (958, -9.599999999999831e-05), (73, -9.599999999999892e-05), (776, -9.999999999999948e-05), (237, -0.00010199999999999939), (848, -0.00010399999999999934), (330, -0.00011400000000000045), (921, -0.00011599999999999999), (251, -0.00011600000000000022), (207, -0.0001160000000000008), (245, -0.0001240000000000004), (924, -0.0001279999999999994), (523, -0.0001339999999999996), (849, -0.00013600000000000013), (272, -0.00013799999999999937), (517, -0.0001419999999999992), (661, -0.00014200000000000033), (94, -0.00014399999999999778), (807, -0.00014999999999999896), (329, -0.00015199999999999966), (573, -0.00015200000000000014), (361, -0.00016199999999999976), (540, -0.00016599999999999945), (286, -0.00016600000000000005), (38, -0.0001720000000000005), (817, -0.00017599999999999975), (707, -0.00018199999999999927), (907, -0.00018999999999999906), (338, -0.00018999999999999993), (320, -0.00019999999999999968), (578, -0.00020799999999999982), (934, -0.00021399999999999924), (242, -0.00021599999999999885), (566, -0.00021999999999999995), (501, -0.00022200000000000035), (203, -0.00022599999999999918), (335, -0.00022599999999999986), (943, -0.00022600000000000002), (454, -0.0002340000000000008), (65, -0.00023600000000000018), (571, -0.00024200000000000038), (258, -0.00024399999999999926), (818, -0.00024399999999999964), (119, -0.00024800000000000034), (96, -0.00024999999999999984), (659, -0.00025199999999999946), (583, -0.0002579999999999998), (798, -0.0002599999999999999), (800, -0.0002640000000000009), (986, -0.0002659999999999994), (636, -0.00027600000000000026), (774, -0.00028), (267, -0.0002860000000000005), (998, -0.0002960000000000007), (965, -0.0003139999999999991), (276, -0.000326), (407, -0.0003320000000000001), (975, -0.00033200000000000026), (701, -0.00033600000000000134), (197, -0.000344), (186, -0.00034999999999999935), (788, -0.0003520000000000005), (75, -0.0003660000000000005), (657, -0.00037199999999999977), (222, -0.00037600000000000025), (13, -0.0003839999999999993), (446, -0.00038599999999999984), (718, -0.00038799999999999935), (954, -0.0003879999999999994), (727, -0.0003940000000000002), (40, -0.0003959999999999986), (564, -0.0003960000000000003), (867, -0.00039600000000000144), (69, -0.0003979999999999992), (900, -0.00040199999999999915), (184, -0.0004020000000000002), (375, -0.0004059999999999995), (143, -0.00041000000000000026), (763, -0.00041600000000000025), (911, -0.0004199999999999997), (793, -0.0004279999999999995), (897, -0.00042799999999999994), (467, -0.00043000000000000015), (996, -0.0004300000000000004), (353, -0.0004420000000000017), (518, -0.00044999999999999966), (0, -0.0004520000000000002), (388, -0.0004599999999999988), (174, -0.00045999999999999985), (161, -0.0004600000000000003), (367, -0.00046199999999999876), (821, -0.0004699999999999994), (654, -0.0004779999999999992), (342, -0.000479999999999999), (812, -0.00048599999999999804), (560, -0.00048800000000000064), (64, -0.0004900000000000003), (471, -0.0004959999999999993), (328, -0.0004959999999999997), (752, -0.0004980000000000006), (46, -0.0005060000000000002), (348, -0.0005140000000000004), (32, -0.0005200000000000003), (168, -0.0005279999999999987), (854, -0.0005379999999999987), (278, -0.0005420000000000005), (53, -0.000551999999999999), (869, -0.0005579999999999991), (709, -0.0005599999999999997), (706, -0.0005640000000000002), (336, -0.0005719999999999988), (487, -0.000572), (125, -0.0005720000000000002), (170, -0.0005739999999999971), (575, -0.0005740000000000003), (678, -0.0005759999999999991), (142, -0.0005759999999999999), (475, -0.0005760000000000001), (880, -0.0005819999999999999), (792, -0.0005880000000000006), (670, -0.000597999999999999), (288, -0.0006020000000000001), (928, -0.0006299999999999988), (591, -0.0006379999999999997), (430, -0.000639999999999999), (995, -0.000652000000000001), (218, -0.0006599999999999984), (522, -0.0006599999999999997), (164, -0.0006880000000000007), (547, -0.0006919999999999991), (642, -0.0006999999999999989), (433, -0.000717999999999998), (884, -0.0007199999999999996), (978, -0.0007219999999999989), (827, -0.000728), (576, -0.0007299999999999987), (729, -0.0007339999999999995), (489, -0.0007339999999999998), (876, -0.0007379999999999983), (285, -0.0007379999999999997), (339, -0.0007419999999999997), (6, -0.0007680000000000006), (811, -0.0007859999999999996), (422, -0.0008000000000000004), (789, -0.0008059999999999993), (572, -0.000807999999999998), (188, -0.0008119999999999998), (512, -0.0008179999999999985), (98, -0.0008380000000000002), (23, -0.0008460000000000009), (671, -0.0008500000000000001), (445, -0.0008540000000000008), (668, -0.0008560000000000027), (247, -0.0008579999999999994), (844, -0.0008720000000000005), (932, -0.0008760000000000005), (680, -0.0008839999999999981), (283, -0.0008919999999999984), (896, -0.000903999999999999), (102, -0.0009040000000000006), (137, -0.0009139999999999979), (231, -0.0009379999999999992), (757, -0.0009439999999999986), (252, -0.0009479999999999995), (536, -0.0009539999999999976), (299, -0.0009840000000000005), (936, -0.0009840000000000005), (453, -0.00099), (418, -0.0010059999999999995), (623, -0.0010859999999999993), (842, -0.0010960000000000002), (281, -0.0011019999999999992), (765, -0.0011119999999999986), (198, -0.0011279999999999994), (733, -0.0011439999999999988), (78, -0.0011700000000000016), (128, -0.001205999999999997), (853, -0.001234), (558, -0.0012380000000000017), (390, -0.001255999999999997), (232, -0.0012600000000000016), (882, -0.001285999999999998), (766, -0.001316000000000001), (969, -0.0013439999999999984), (374, -0.0013439999999999997), (719, -0.0013640000000000006), (478, -0.001375999999999999), (295, -0.0013839999999999996), (574, -0.0014019999999999998), (879, -0.0014159999999999976), (499, -0.001475999999999999), (981, -0.0014939999999999997), (88, -0.0015699999999999987), (674, -0.0017699999999999994), (461, -0.0018999999999999987), (31, -0.001978000000000001), (895, -0.002116), (1, -0.002193999999999999)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "sorted_dct = sorted(accs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(sorted_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted accuracy drop is 0.452\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from eval import data_removal_f1\n",
    "from metrics import weighted_acc_drop\n",
    "acc = data_removal_f1(accs, X_train_scaled, y_train_balanced, X_test_scaled, y_test_balanced)\n",
    "plt.plot(range(len(acc)), acc)\n",
    "res = weighted_acc_drop(acc)\n",
    "print(\"The weighted accuracy drop is {:.3f}\".format(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7594814520847701, 0.7592399840937211, 0.7592399840937211, 0.7595227571735715, 0.760046797169232, 0.760046797169232, 0.760046797169232, 0.7603690115851012, 0.760872438031351, 0.760872438031351, 0.7611340569662554, 0.7602057097838649, 0.7609509020832248, 0.7601862796480878, 0.760428169218241, 0.760428169218241, 0.7601862796480878, 0.759499033467873, 0.7597218157306604, 0.7600021735652206, 0.7592570173466388, 0.7592760320992962, 0.7590339626286324, 0.7585871918937603, 0.7588479267742609, 0.7589028546316828, 0.7586605054704876, 0.7593875167754609, 0.7586785235952075, 0.7579512724241463, 0.7579512724241463, 0.7579512724241463, 0.7577088303569721, 0.7579512724241463, 0.7582291921914562, 0.7575190302017586, 0.7555607879559549, 0.7555607879559549, 0.7558034368405185, 0.7562886938807608, 0.7555778583433014, 0.7550924290592252, 0.7553520506597081, 0.7553520506597081, 0.7546237074813209, 0.7553520506597081, 0.7555948035637191, 0.755837542540033, 0.7556116236687757, 0.7558544078452116, 0.7556116236687757, 0.7551754497474069, 0.755482747098617, 0.7545257125531234, 0.7550277049609901, 0.7550121477326952, 0.755482747098617, 0.7562437590023969, 0.7560316897767239, 0.7560161963075611, 0.755529697368857, 0.7558037241688872, 0.7550431372549019, 0.7548150946317744, 0.7540698306168506, 0.7545868175480966, 0.7543433889232831, 0.7535372549019608, 0.7535679323119868, 0.7538564836516858, 0.7538264645960863, 0.7533396846900791, 0.7541441758762251, 0.7544023042031011, 0.7549184228912715, 0.7549184228912715, 0.7537137928552422, 0.7534699635565657, 0.7534699635565657, 0.7539853711576436, 0.7537276900359527, 0.7539853711576436, 0.7539990667762764, 0.7539990667762764, 0.7540260833900472, 0.7537820283947992, 0.7532805661308375, 0.7535772504924225, 0.7511707098607251, 0.7514152665607676, 0.7519043222777889, 0.7512178394096122, 0.7514855128830461, 0.7515079235656408, 0.7517639382763914, 0.7493130369554782, 0.7495686946844203, 0.7500798842854839, 0.7498345846925436, 0.7500798842854839, 0.7478811343421135, 0.7466531681872483, 0.7471813853408757, 0.7459519858013439, 0.7466896948762235, 0.7459519858013439, 0.7462236375412824, 0.745739637388686, 0.7447778929613496, 0.7450243305965987, 0.7442849436375993, 0.7440529047399563, 0.7440599485155195, 0.7440736507499698, 0.744093241116011, 0.7440803092500521, 0.7433521708504099, 0.7426108650857579, 0.7413859512044813, 0.7428639750427977, 0.7428698278503494, 0.7418861718017647, 0.740401287589606, 0.7396679791426787, 0.7394246937364898, 0.7396725363090856, 0.7396679791426787, 0.740424982820035, 0.7409377602969112, 0.7416938022578536, 0.741949422086729, 0.741701165376579, 0.741701165376579, 0.741949422086729, 0.741701165376579, 0.7414563061157335, 0.7414687177148435, 0.740720036959271, 0.7402301113679016, 0.7399812136426857, 0.7392362936076827, 0.7387397945232236, 0.7387380962545106, 0.7394889934099715, 0.7394958319333109, 0.7392480280632123, 0.73924633315156, 0.7397441279768875, 0.73924633315156, 0.7392486799464423, 0.7389994127486789, 0.7392499837031239, 0.7395, 0.7392498533280426, 0.7379989519958079, 0.7379994104986737, 0.7379983624897656, 0.7374967843356081, 0.7372440715693647, 0.7369920440093313, 0.7357379595607825, 0.7362396968631588, 0.7352360833466308, 0.7352340974979811, 0.7349785332611942, 0.7344734473447345, 0.7347133738651794, 0.7339515126631829, 0.7339478537793408, 0.7329358378350398, 0.7336959236284918, 0.7326836888475647, 0.7316654578483134, 0.7329035781917271, 0.732392957182873, 0.7311168860534019, 0.7298443903688524, 0.729337534855297, 0.7283092051393092, 0.7270398035887511, 0.7272937659104698, 0.7260235413333834, 0.7265079424358021, 0.7270164455076596, 0.7264997301593289, 0.72777105545764, 0.7275006460599353, 0.7279757825873344, 0.7277034690778257, 0.7277034690778257, 0.7271938432906329, 0.7271561485361927, 0.7258797835151236, 0.7256141448912533, 0.7258696269742784, 0.7253375772057731, 0.7250710420934263, 0.7247816988594654, 0.7247702573567139, 0.7247816988594654, 0.7250377885225064, 0.7255498331137649, 0.7258057882398102, 0.7255272893741246, 0.72527113040975, 0.7257129287715928, 0.7253933940073624, 0.7251100389901465, 0.724812030075188, 0.7235238180282899, 0.7235095412708697, 0.7227063161534102, 0.7224630265657664, 0.7239224402598313, 0.7231317411402158, 0.7233272791840336, 0.7217732676669741, 0.7222750291319572, 0.7209787123318124, 0.7209787123318124, 0.7212040796801753, 0.7203902288181794, 0.7193504594820385, 0.7195375432706556, 0.719760711807792, 0.7173947488553074, 0.7165712355986528, 0.7170938858863267, 0.7160278885743695, 0.7144149314734102, 0.7151369451479594, 0.7145686487862158, 0.7150935360117181, 0.7139762055623076, 0.7144337809992667, 0.7133577130274075, 0.7114884251724084, 0.7106718827145904, 0.7098540807814597, 0.7087958759853329, 0.7082664147378366, 0.7070742821303212, 0.7064886434555666, 0.7072039738549037, 0.7060834234898254, 0.7061115985075767, 0.7067693630006024, 0.7065312875056579, 0.7056908160970478, 0.7054247495164602, 0.7043879721216669, 0.7048077730925474, 0.7048077730925474, 0.705578845676074, 0.7063778810724471, 0.7056630541871921, 0.7050458418433853, 0.7042458635427762, 0.7040365889497784, 0.7040365889497784, 0.7041587405441865, 0.7035353535353536, 0.7042177699825871, 0.7036516595791286, 0.7025793283409871, 0.7027539993615278, 0.7026271224799482, 0.7034016830626999, 0.7031647592347617, 0.7038112298669572, 0.7029709984323752, 0.7007800224339039, 0.7021293638000752, 0.7029709984323752, 0.7013199632674179, 0.6981109903048547, 0.6981109903048547, 0.6976997625245389, 0.6968501821433605, 0.6958890296786563, 0.6939446725055729, 0.6941794934153102, 0.6938685370977519, 0.6938685370977519, 0.6932839337655794, 0.6935955439602497, 0.6939446725055729, 0.6922683176522205, 0.6925415622684264, 0.6922683176522205, 0.6919169275456808, 0.6912505553201818, 0.6915643353859295, 0.6910958375164105, 0.6928937090266959, 0.6925012230064994, 0.6920659103549449, 0.6906919648080714, 0.6904169482093883, 0.6904169482093883, 0.6882140653507137, 0.6873439743316533, 0.6871107779962499, 0.6855990077769659, 0.6861523114041602, 0.6869383030922483, 0.6860646599777035, 0.6863414372199711, 0.6849121883919204, 0.6818114344901224, 0.6812547487204731, 0.6829238350783043, 0.6817651494430984, 0.6812082132147873, 0.6797671033478894, 0.6766432450294604, 0.6774832448456737, 0.6751418824223365, 0.6726111798930533, 0.6717143781932307, 0.6703034687495016, 0.6688371673377072, 0.6678258774029959, 0.6683929850882635, 0.6683929850882635, 0.6675521460182856, 0.6678364266386971, 0.6686887304317677, 0.6694858734721099, 0.6674413314827321, 0.6652191207707879, 0.6652191207707879, 0.6650462658061117, 0.664754324398856, 0.6650411981947497, 0.6645172926906254, 0.6661225701142602, 0.6666968030453713, 0.6677857223059482, 0.668300783201598, 0.6650808282558852, 0.665126973119345, 0.6632088755825402, 0.6635379833831179, 0.6623145056863549, 0.6628950029095988, 0.6626048033829491, 0.6627043044517859, 0.662413660838646, 0.6634751080136346, 0.6639920433315861, 0.6632216291382095, 0.6620586579226285, 0.6629310345385101, 0.6608941005927882, 0.6587198535060802, 0.6579338882679557, 0.6588124391384651, 0.6558118479633198, 0.6551548635109182, 0.6530223217757928, 0.6517698136051765, 0.6517698136051765, 0.6523610102201833, 0.6528919770690137, 0.6519379449124061, 0.6517917088895283, 0.6530618207315648, 0.6515715590782932, 0.6515701284332357, 0.6510436832535275, 0.6488672173852067, 0.649390515123828, 0.6474290215159977, 0.6480315430779013, 0.6464449006065277, 0.6438637788139496, 0.6418213959722239, 0.6409643392955331, 0.6389141414490075, 0.6383881605916863, 0.6388296053713641, 0.6384392258178602, 0.6386598438381537, 0.6400501642287558, 0.638574617790355, 0.638574617790355, 0.6375703623163783, 0.6375703623163783, 0.6354669070158377, 0.6335293212186787, 0.6334399913800297, 0.6307831628658844, 0.6277997554737113, 0.6312752346011319, 0.6312752346011319, 0.6281113885854566, 0.6297324817180412, 0.6296075509190264, 0.6246763745514933, 0.6208133102765505, 0.6225051485476111, 0.6175740184688313, 0.6140903460937056, 0.6152512741801943, 0.6164198685711593, 0.6148305187598075, 0.6139866053885975, 0.6126092869905496, 0.6084359291906463, 0.6084359291906463, 0.6067085962877155, 0.6044416131451061, 0.6031430159585812, 0.6040051893051311, 0.6011745168277712, 0.6005215831405034, 0.6038656469436431, 0.6022341406853678, 0.6022341406853678, 0.6022341406853678, 0.5999220616882142, 0.5947715254141984, 0.5944410302387471, 0.5935372971475087, 0.5912141934007912, 0.5921232697125477, 0.5896742083679902, 0.5900070935818293, 0.5829331511624514, 0.5796330867471561, 0.5779379403712522, 0.5775984177950036, 0.5761040686361802, 0.5786291807757962, 0.5786291807757962, 0.5788353568537308, 0.5789852888481196, 0.5811931757376242, 0.5814001021972407, 0.581868938248784, 0.583111546085635, 0.5806805745495646, 0.5802788192764677, 0.580556624342696, 0.5808975740480398, 0.5783704016021924, 0.5734074410696829, 0.571042208330914, 0.5704642441484546, 0.5727888006045577, 0.5727888006045577, 0.5753463426082515, 0.5740102285921654, 0.5736608679567987, 0.573864392505461, 0.5737182206898945, 0.5742714741348136, 0.5742714741348136, 0.5707654707838913, 0.5685537304693519, 0.5678968476685639, 0.5666810550064747, 0.567035736027778, 0.5636812265071178, 0.5674388856168631, 0.5679952343170344, 0.5662662382907379, 0.5664674255715678, 0.5678420800904388, 0.5666686243212739, 0.566090382200659, 0.5657326777432761, 0.5632642569494596, 0.5632642569494596, 0.5629046524910097, 0.5632642569494596, 0.5633045972828589, 0.561141827135279, 0.558247235045869, 0.5535166367452009, 0.5547308996464902, 0.5543646578194944, 0.5545901718304517, 0.5518456133569222, 0.5513508973088201, 0.5506107637217159, 0.552459548793225, 0.5545002261420172, 0.5515694749687313, 0.5515694749687313, 0.550478021246791, 0.5475169709558192, 0.5492045569539351, 0.543224844006437, 0.543988920556325, 0.5443653562561088, 0.5447532382848835, 0.5458414184089595, 0.5482805058661349, 0.5505363873751297, 0.5466194528163847, 0.5466194528163847, 0.5466194528163847, 0.5441935011326307, 0.5428678787520579, 0.5396530331436226, 0.5385167696947315, 0.5368017133460835, 0.5345056983925729, 0.5348889288922691, 0.5318120005044459, 0.5292896080297654, 0.5259673508928351, 0.5263574283810794, 0.5267472714067539, 0.5216418273601293, 0.5178331842576028, 0.5161880050225405, 0.5127401263846758, 0.513542666483843, 0.513542666483843, 0.5132915052968351, 0.5071927511464531, 0.5018903614679016, 0.4957171470375158, 0.49530347320753354, 0.4864848391346778, 0.4864848391346778, 0.4856441633009791, 0.4864848391346778, 0.48774377907656447, 0.4881628742465106, 0.4994282185602713, 0.49447532157788554, 0.492161579235104, 0.487568060619836, 0.4856441633009791, 0.484381068022655, 0.47871812865497076, 0.4777825243009495, 0.4777825243009495, 0.4773550115559865, 0.47021259790167746, 0.46587031322146855, 0.46994918488113663, 0.4638554103014301, 0.4638554103014301, 0.46155883867301384, 0.4585291683226801, 0.45202135774218166, 0.45202135774218166, 0.4478560655031243, 0.4511658295112044, 0.4453342271832118, 0.44850170167863823, 0.44488043436877867, 0.44549450468474244, 0.44504054897068007, 0.44504054897068007, 0.44504054897068007, 0.44338092272681845, 0.439583206587997, 0.43405236377127515, 0.4385071200073796, 0.43605797101449273, 0.4369807831838336, 0.4342123079583028, 0.4343680601125731, 0.4339037698412699, 0.4275207938129286, 0.43513952425369673, 0.430644157179797, 0.4262634377961348, 0.42688672575072883, 0.4262634377961348, 0.42688672575072883, 0.4309528843128923, 0.4304855017836187, 0.4251539164432195, 0.4235653607796248, 0.4156059600651506, 0.4076798337333601, 0.39744036294907586, 0.4130124804127555, 0.41204624606022144, 0.4038559107052258, 0.3957327732621159, 0.3941545500239494, 0.3920624106551152, 0.38308150145720815, 0.37136468791234417, 0.367444607249596, 0.3679683632691332, 0.3679683632691332, 0.367444607249596, 0.3632389628562861, 0.3632389628562861, 0.36348952741873364, 0.35990701462514246, 0.35320436899592056, 0.3468387715117635, 0.34368687704890893, 0.3398513470691716, 0.3398513470691716, 0.3398513470691716, 0.3398513470691716, 0.34368687704890893, 0.3458683217719362, 0.3453236583042709, 0.3511701139530142, 0.34368687704890893, 0.3414979628214922, 0.3414979628214922, 0.33930153215642683, 0.3404006899913104, 0.3404006899913104, 0.3404006899913104, 0.3404006899913104, 0.33941548233754254, 0.33610510837168966, 0.3349978367449659, 0.3349978367449659, 0.3344434824928323, 0.3338886482746291, 0.3338886482746291, 0.3338886482746291, 0.3338886482746291, 0.3333333333333333, 0.3333333333333333, 0.3338886482746291, 0.3338886482746291, 0.3338886482746291, 0.3349978367449659, 0.3349978367449659, 0.3349978367449659, 0.3349978367449659, 0.3338886482746291, 0.3338886482746291, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
