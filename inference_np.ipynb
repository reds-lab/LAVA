{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc3e35bbb4864fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:44:00.657220Z",
     "start_time": "2024-05-15T18:44:00.651415Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\utils.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import lava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T20:54:06.004457Z",
     "start_time": "2024-05-15T20:54:05.931845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from preact_resnet import PreActResNet18\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as n\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537970a917d28b24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:14:46.804668Z",
     "start_time": "2024-05-16T00:14:45.374590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#from preact_resnet import PreActResNet18\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56be66a2f085e903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:16:35.514864Z",
     "start_time": "2024-05-16T00:16:35.500286Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe997c54393baffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T00:16:41.576883Z",
     "start_time": "2024-05-16T00:16:41.520494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c712589afa2ff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T20:52:00.418149Z",
     "start_time": "2024-05-15T20:51:59.938637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.0\n",
      "2.3.0\n",
      "Cuda device:  0\n",
      "cude devices:  1\n"
     ]
    }
   ],
   "source": [
    "cuda_num = 0\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(cuda_num)\n",
    "#print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "#torch.cuda.set_device(cuda_num)\n",
    "print(\"Cuda device: \", torch.cuda.current_device())\n",
    "print(\"cude devices: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "825a489cd5206aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:35:03.922180Z",
     "start_time": "2024-05-15T18:35:03.914980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:' + str(cuda_num) if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "training_size = 50\n",
    "valid_size = 20\n",
    "resize = 32\n",
    "portion = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d3c8b14127bb226",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:35:19.946464Z",
     "start_time": "2024-05-15T18:35:19.690173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_test = PreActResNet18()\n",
    "net_test = net_test.to(device)\n",
    "feature_extractor_name = 'preact_resnet18_test_mnist.pth'\n",
    "net_test.load_state_dict(torch.load('checkpoint/'+feature_extractor_name, map_location=torch.device('cpu')))\n",
    "net_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e6439994689b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:35:33.478036Z",
     "start_time": "2024-05-15T18:35:33.470130Z"
    }
   },
   "outputs": [],
   "source": [
    "def modify_for_mnist(model):\n",
    "    model.linear = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f68232230f8b6740",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:35:42.670701Z",
     "start_time": "2024-05-15T18:35:42.652868Z"
    }
   },
   "outputs": [],
   "source": [
    "modify_for_mnist(net_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33dc85661655622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:35:55.181194Z",
     "start_time": "2024-05-15T18:35:55.168070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_test.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c8935a2c67bc29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:36:35.229992Z",
     "start_time": "2024-05-15T18:36:32.404108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST\n",
      "Currrent label: 9\n",
      "New label: 2 \n",
      "TRAINNNN label:  tensor(2)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 2)\n",
      "Currrent label: 9\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 7\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 6\n",
      "New label: 5 \n",
      "TRAINNNN label:  tensor(5)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 5)\n",
      "Currrent label: 6\n",
      "New label: 2 \n",
      "TRAINNNN label:  tensor(2)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 2)\n",
      "Currrent label: 3\n",
      "New label: 2 \n",
      "TRAINNNN label:  tensor(2)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 2)\n",
      "Currrent label: 5\n",
      "New label: 8 \n",
      "TRAINNNN label:  tensor(8)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 8)\n",
      "Currrent label: 5\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 6\n",
      "New label: 9 \n",
      "TRAINNNN label:  tensor(9)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 9)\n",
      "Currrent label: 3\n",
      "New label: 7 \n",
      "TRAINNNN label:  tensor(7)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 7)\n",
      "Currrent label: 5\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 3\n",
      "New label: 2 \n",
      "TRAINNNN label:  tensor(2)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 2)\n",
      "Currrent label: 6\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 1\n",
      "New label: 8 \n",
      "TRAINNNN label:  tensor(8)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 8)\n",
      "Currrent label: 0\n",
      "New label: 3 \n",
      "TRAINNNN label:  tensor(3)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 3)\n"
     ]
    }
   ],
   "source": [
    "loaders, shuffle_ind = lava.load_data_corrupted(corrupt_type='shuffle', dataname='MNIST', resize=resize,\n",
    "                                        training_size=training_size, test_size=valid_size, currupt_por=portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3b1312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x00000275C41CA890>\n"
     ]
    }
   ],
   "source": [
    "print(loaders['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4799164",
   "metadata": {},
   "outputs": [],
   "source": [
    "ktr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3621d447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch in loaders['train']:\n",
    "    print(batch[0][0].size())\n",
    "    ktr = batch[0][0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a3bd5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\21520\\anaconda3\\lib\\site-packages (10.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a525b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c67357fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = ktr.numpy()\n",
    "np_array = np_array.squeeze()\n",
    "image = Image.fromarray(np.uint8(np_array * 255), 'L')\n",
    "image.save('output_image.png')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4fa7fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(np_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "748b12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = ktr.repeat(3,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6abe713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(rgb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30b1eaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1024)\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "for i in range(0,32):\n",
    "    for j in range(0,32):\n",
    "        #print(ktr[0,i,j])\n",
    "        count = count + (ktr[0,i,j]!=tensor(-0.4242))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c00aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "604b8fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_test = PreActResNet18()\n",
    "net_test = net_test.to(device)\n",
    "net_test.load_state_dict(torch.load('checkpoint/'+'preact_resnet18_test_mnist.pth', map_location='cuda:0'))\n",
    "net_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd5d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = net_test.to(device)\n",
    "embedder.fc = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9abc5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreActResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): PreActBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): PreActBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): PreActBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): PreActBlock(\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): PreActBlock(\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): PreActBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): PreActBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): PreActBlock(\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
      "  (fc): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18c9b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in embedder.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2481c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from otdd.pytorch.distance_fast import DatasetDistance, FeatureCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab8cfeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cost = FeatureCost(src_embedding = embedder,\n",
    "                               src_dim = (1, resize,resize),\n",
    "                               tgt_embedding = embedder,\n",
    "                               tgt_dim = (1, resize,resize),\n",
    "                               p = 2,\n",
    "                               device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d04203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1024])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"save_x1y1x2y2.txt\", \"rb\") as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "X1, Y1, X2, Y2 = loaded_data\n",
    "\n",
    "# Now you can use X1, Y1, X2, and Y2 in your code\n",
    "print(X1.shape)  # Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50540c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4242)\n"
     ]
    }
   ],
   "source": [
    "print(X1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bfe3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch_shape(b):\n",
    "    if b.ndim == 3: return b.shape\n",
    "    elif b.ndim == 2: return (1,*b.shape)\n",
    "    elif b.ndim == 1: return (1,1,b.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e54879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "B1, N1, D1 = _get_batch_shape(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da47d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 50 1024\n"
     ]
    }
   ],
   "source": [
    "print(B1, N1, D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "521d88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dim = (1, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91d5249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X1.view(-1, *src_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b14697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd264433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_rgb = X_test.repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05451076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(X_test_rgb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccb69085",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_rgb = X_test_rgb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e37de6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 32, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_rgb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1c4520f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08ec46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_emb = embedder(X_test_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fbf2d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 100])\n"
     ]
    }
   ],
   "source": [
    "print(X_test_emb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97f11e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_X1 = X_test_emb.reshape(B1, N1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4953bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  4.5680,  35.7887,  -0.4968,  ..., -24.5336, -30.4800, -24.2358],\n",
      "         [ -1.0291,  14.3661,   0.8592,  ..., -15.6310, -18.3019, -15.5440],\n",
      "         [  1.1676,  18.7959,  -5.3507,  ..., -21.4980, -27.0970, -21.4995],\n",
      "         ...,\n",
      "         [  0.4916,  14.6977,  -0.4040,  ..., -14.9157, -17.9280, -14.8729],\n",
      "         [ -1.3570,  20.5337,   2.7240,  ..., -18.8162, -21.8036, -18.5932],\n",
      "         [ -9.1459,  11.4589,  17.1523,  ...,  -9.8075,  -6.5931,  -9.3166]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(flattened_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "469c0f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 100])\n"
     ]
    }
   ],
   "source": [
    "print(flattened_X1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ac46d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee69c666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 17 1024\n",
      "torch.Size([17, 1, 32, 32])\n",
      "torch.Size([17, 3, 32, 32])\n",
      "torch.Size([17, 3, 32, 32])\n",
      "torch.Size([17, 100])\n",
      "torch.Size([1, 17, 100])\n"
     ]
    }
   ],
   "source": [
    "B2, N2, D2 = _get_batch_shape(X2)\n",
    "print(B2, N2, D2)\n",
    "tgt_dim = (1, 32, 32)\n",
    "X_test_1 = X2.view(-1, *tgt_dim)\n",
    "print(X_test_1.shape)\n",
    "X_test_rgb_1 = X_test_1.repeat(1, 3, 1, 1)\n",
    "print(X_test_rgb_1.size())\n",
    "X_test_rgb_1 = X_test_rgb_1.to(device)\n",
    "print(X_test_rgb_1.size())\n",
    "X_test_emb_1 = embedder(X_test_rgb_1)\n",
    "print(X_test_emb_1.size())\n",
    "flattened_X2 = X_test_emb_1.reshape(B2, N2, -1)\n",
    "print(flattened_X2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "826855f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geomloss\n",
    "c = geomloss.utils.squared_distances(flattened_X1, flattened_X2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8062eec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.2867e+03, 1.0123e+04, 3.3894e+03, 2.2643e+04, 1.8316e+04,\n",
       "          1.6929e+03, 1.5964e+03, 1.4539e+04, 5.8768e+03, 6.0583e+03,\n",
       "          1.0668e+04, 9.8217e+03, 2.2888e+03, 1.4502e+04, 3.2149e+03,\n",
       "          2.4505e+03, 2.2549e+04],\n",
       "         [1.1393e+04, 8.6758e+00, 2.4432e+03, 2.4312e+03, 1.7133e+03,\n",
       "          4.2093e+03, 6.8048e+03, 3.6053e+02, 1.0514e+03, 9.3430e+02,\n",
       "          6.5640e+02, 1.1870e+02, 5.7243e+03, 3.0782e+03, 6.9086e+03,\n",
       "          3.9213e+03, 3.0672e+03],\n",
       "         [4.9633e+03, 3.0817e+03, 1.3474e+03, 1.1285e+04, 9.3767e+03,\n",
       "          1.2645e+02, 1.4237e+03, 5.6676e+03, 1.5117e+03, 1.6831e+03,\n",
       "          4.5970e+03, 3.3720e+03, 5.6225e+02, 8.8407e+03, 1.1159e+03,\n",
       "          1.7912e+02, 1.2387e+04],\n",
       "         [5.2531e+02, 1.4524e+04, 5.0961e+03, 2.6884e+04, 2.0289e+04,\n",
       "          6.0954e+03, 5.6210e+03, 1.9096e+04, 9.3927e+03, 9.0594e+03,\n",
       "          1.2565e+04, 1.3099e+04, 8.2856e+03, 1.3663e+04, 1.0490e+04,\n",
       "          7.6179e+03, 2.4402e+04],\n",
       "         [1.2070e+03, 1.9307e+04, 8.1230e+03, 3.4301e+04, 2.7247e+04,\n",
       "          7.6277e+03, 6.9201e+03, 2.4853e+04, 1.3225e+04, 1.2738e+04,\n",
       "          1.7761e+04, 1.7965e+04, 9.4412e+03, 1.9753e+04, 1.1302e+04,\n",
       "          9.4261e+03, 3.2035e+04],\n",
       "         [1.1570e+03, 5.5217e+03, 7.0455e+02, 1.4841e+04, 1.0829e+04,\n",
       "          1.0254e+03, 1.8253e+03, 8.7075e+03, 2.7213e+03, 2.4557e+03,\n",
       "          5.1649e+03, 4.9254e+03, 2.5725e+03, 7.5647e+03, 4.0291e+03,\n",
       "          1.8139e+03, 1.4077e+04],\n",
       "         [4.8594e+03, 5.9823e+03, 2.7054e+03, 1.5536e+04, 1.3111e+04,\n",
       "          1.3160e+03, 2.5164e+01, 9.3540e+03, 2.3649e+03, 4.5292e+03,\n",
       "          7.9105e+03, 6.4558e+03, 5.4466e+02, 1.2515e+04, 1.1528e+03,\n",
       "          5.5979e+02, 1.7034e+04],\n",
       "         [1.6105e+04, 3.6851e+03, 6.2770e+03, 2.6880e+03, 6.9095e+02,\n",
       "          1.1958e+04, 1.5877e+04, 2.8058e+03, 5.9928e+03, 4.4634e+03,\n",
       "          1.4917e+03, 2.6201e+03, 1.5914e+04, 4.3482e+02, 1.8530e+04,\n",
       "          1.2530e+04, 7.8442e+02],\n",
       "         [2.3353e+04, 1.3619e+04, 1.4768e+04, 1.1020e+04, 7.0062e+03,\n",
       "          2.3692e+04, 2.9704e+04, 1.2111e+04, 1.6961e+04, 1.2932e+04,\n",
       "          8.2063e+03, 1.1116e+04, 3.0617e+04, 3.7019e+03, 3.4625e+04,\n",
       "          2.5891e+04, 6.0525e+03],\n",
       "         [7.9460e+03, 1.2250e+03, 1.4591e+03, 6.2360e+03, 4.8835e+03,\n",
       "          2.1979e+03, 2.5055e+03, 2.7686e+03, 4.7787e+01, 1.5269e+03,\n",
       "          2.5032e+03, 1.5597e+03, 2.4576e+03, 5.7827e+03, 3.4690e+03,\n",
       "          1.3673e+03, 7.3773e+03],\n",
       "         [1.4518e+04, 5.6414e+03, 6.5478e+03, 5.4235e+03, 2.2785e+03,\n",
       "          1.2871e+04, 1.7099e+04, 5.0859e+03, 7.5813e+03, 5.2892e+03,\n",
       "          2.3626e+03, 4.0450e+03, 1.7683e+04, 3.3618e+02, 2.0734e+04,\n",
       "          1.4107e+04, 2.4059e+03],\n",
       "         [1.9868e+03, 4.0631e+03, 2.6373e+02, 1.1680e+04, 7.7702e+03,\n",
       "          1.6596e+03, 3.5463e+03, 6.5340e+03, 2.3761e+03, 1.3100e+03,\n",
       "          3.0275e+03, 3.2074e+03, 4.0665e+03, 4.5300e+03, 5.8210e+03,\n",
       "          2.7550e+03, 1.0344e+04],\n",
       "         [1.4242e+03, 1.5820e+04, 6.5825e+03, 3.0760e+04, 2.5228e+04,\n",
       "          4.3417e+03, 4.4063e+03, 2.1149e+04, 1.0683e+04, 1.0096e+04,\n",
       "          1.5794e+04, 1.5156e+04, 5.3804e+03, 1.9487e+04, 6.4031e+03,\n",
       "          5.8388e+03, 2.9934e+04],\n",
       "         [1.3714e+04, 4.1768e+03, 5.4367e+03, 4.2029e+03, 1.4694e+03,\n",
       "          1.1141e+04, 1.5121e+04, 3.7014e+03, 6.0333e+03, 4.0992e+03,\n",
       "          1.4900e+03, 2.8440e+03, 1.5456e+04, 1.0554e+02, 1.8236e+04,\n",
       "          1.2117e+04, 1.7474e+03],\n",
       "         [1.9276e+04, 5.2338e+03, 8.5280e+03, 3.0358e+03, 1.1259e+03,\n",
       "          1.4981e+04, 1.9397e+04, 3.8781e+03, 8.1703e+03, 6.3200e+03,\n",
       "          2.6343e+03, 4.0242e+03, 1.9398e+04, 9.8788e+02, 2.2225e+04,\n",
       "          1.5647e+04, 8.0939e+02],\n",
       "         [3.7154e+03, 8.9819e+03, 3.7874e+03, 2.0321e+04, 1.7019e+04,\n",
       "          2.0613e+03, 2.2074e+02, 1.3098e+04, 4.3112e+03, 6.5025e+03,\n",
       "          1.0640e+04, 9.2697e+03, 1.2752e+03, 1.5217e+04, 1.9700e+03,\n",
       "          1.5574e+03, 2.1450e+04],\n",
       "         [8.0948e+03, 3.5465e+02, 1.1155e+03, 4.6314e+03, 3.2951e+03,\n",
       "          2.2364e+03, 3.9889e+03, 1.4869e+03, 2.3166e+02, 4.9382e+02,\n",
       "          1.1056e+03, 4.5151e+02, 3.3748e+03, 3.8723e+03, 4.4987e+03,\n",
       "          1.9714e+03, 5.2640e+03],\n",
       "         [1.0498e+04, 3.2444e+03, 3.4972e+03, 4.7282e+03, 1.7509e+03,\n",
       "          8.4075e+03, 1.1871e+04, 3.3485e+03, 4.3467e+03, 2.6735e+03,\n",
       "          8.7857e+02, 2.0074e+03, 1.2313e+04, 3.8236e+01, 1.4909e+04,\n",
       "          9.3642e+03, 2.4957e+03],\n",
       "         [8.9926e+03, 3.7149e+03, 3.6528e+03, 1.1591e+04, 1.0988e+04,\n",
       "          1.2649e+03, 2.2517e+03, 6.1220e+03, 2.4457e+03, 3.4085e+03,\n",
       "          6.6417e+03, 4.6661e+03, 6.1797e+02, 1.2284e+04, 5.4797e+02,\n",
       "          6.1966e+02, 1.4088e+04],\n",
       "         [3.3169e+03, 4.3797e+03, 1.2942e+03, 1.3694e+04, 1.1066e+04,\n",
       "          1.0902e+01, 1.3705e+03, 7.3986e+03, 2.3422e+03, 2.1078e+03,\n",
       "          5.4489e+03, 4.4202e+03, 7.7270e+02, 9.4317e+03, 1.4616e+03,\n",
       "          4.6727e+02, 1.4283e+04],\n",
       "         [9.3901e+03, 2.8532e+02, 1.7671e+03, 4.4863e+03, 3.6774e+03,\n",
       "          2.3075e+03, 4.5027e+03, 1.3037e+03, 6.0868e+02, 6.8533e+02,\n",
       "          1.5071e+03, 5.9031e+02, 3.2782e+03, 4.8153e+03, 4.1193e+03,\n",
       "          2.0099e+03, 5.5616e+03],\n",
       "         [5.3112e+03, 1.5104e+03, 3.6620e+02, 6.3460e+03, 3.5772e+03,\n",
       "          2.5955e+03, 4.8454e+03, 2.8914e+03, 1.1000e+03, 3.4129e+02,\n",
       "          7.4604e+02, 8.8595e+02, 4.9537e+03, 2.0700e+03, 6.7116e+03,\n",
       "          3.1783e+03, 5.4592e+03],\n",
       "         [5.7487e+03, 9.7007e+02, 3.6482e+02, 6.3360e+03, 4.2637e+03,\n",
       "          1.4631e+03, 3.2356e+03, 2.5335e+03, 3.5483e+02, 2.4395e+02,\n",
       "          1.2586e+03, 8.1135e+02, 2.9237e+03, 3.7376e+03, 4.1980e+03,\n",
       "          1.5886e+03, 6.4432e+03],\n",
       "         [8.1763e+03, 1.1558e+03, 1.6981e+03, 6.8193e+03, 5.8743e+03,\n",
       "          1.3756e+03, 2.5761e+03, 2.7987e+03, 4.7145e+02, 1.2715e+03,\n",
       "          2.9199e+03, 1.6664e+03, 1.6144e+03, 6.9479e+03, 2.2275e+03,\n",
       "          8.0800e+02, 8.3483e+03],\n",
       "         [1.2715e+04, 2.4658e+02, 3.1463e+03, 1.6732e+03, 9.0599e+02,\n",
       "          5.6624e+03, 8.8996e+03, 1.8781e+02, 1.9105e+03, 1.2784e+03,\n",
       "          3.7406e+02, 1.4880e+02, 7.7673e+03, 2.1412e+03, 9.1850e+03,\n",
       "          5.6017e+03, 1.8561e+03],\n",
       "         [1.0204e+04, 6.0455e+03, 4.7656e+03, 7.9218e+03, 3.8134e+03,\n",
       "          1.0635e+04, 1.4087e+04, 6.3726e+03, 6.6917e+03, 4.5727e+03,\n",
       "          2.5511e+03, 4.2798e+03, 1.5314e+04, 5.8370e+02, 1.8435e+04,\n",
       "          1.2083e+04, 4.5737e+03],\n",
       "         [7.2003e+03, 4.0103e+03, 2.9887e+03, 1.2586e+04, 1.1424e+04,\n",
       "          6.6226e+02, 1.8393e+03, 6.6819e+03, 2.5039e+03, 3.0378e+03,\n",
       "          6.5065e+03, 4.7331e+03, 3.7738e+02, 1.1845e+04, 4.4528e+02,\n",
       "          3.8040e+02, 1.4611e+04],\n",
       "         [2.2486e+03, 2.3471e+04, 1.0838e+04, 3.9566e+04, 3.1713e+04,\n",
       "          1.0384e+04, 9.6625e+03, 2.9461e+04, 1.6854e+04, 1.6000e+04,\n",
       "          2.1404e+04, 2.1848e+04, 1.2548e+04, 2.3045e+04, 1.4592e+04,\n",
       "          1.2585e+04, 3.6719e+04],\n",
       "         [5.0328e+03, 4.3195e+03, 2.0304e+03, 1.3365e+04, 1.1410e+04,\n",
       "          3.2512e+02, 6.3590e+02, 7.3072e+03, 1.9319e+03, 2.8967e+03,\n",
       "          6.2683e+03, 4.7915e+03, 1.0913e+02, 1.1003e+04, 5.2755e+02,\n",
       "          3.0605e+01, 1.4860e+04],\n",
       "         [2.1667e+03, 3.9340e+03, 2.1377e+02, 1.1465e+04, 7.6849e+03,\n",
       "          1.6379e+03, 2.6011e+03, 6.4647e+03, 1.7577e+03, 1.5614e+03,\n",
       "          3.2067e+03, 3.2403e+03, 3.5049e+03, 4.9044e+03, 5.2380e+03,\n",
       "          2.2745e+03, 1.0467e+04],\n",
       "         [2.2469e+03, 2.3241e+04, 1.0639e+04, 3.8824e+04, 3.0829e+04,\n",
       "          1.0764e+04, 9.8306e+03, 2.9086e+04, 1.6584e+04, 1.5898e+04,\n",
       "          2.0889e+04, 2.1513e+04, 1.3051e+04, 2.2105e+04, 1.5296e+04,\n",
       "          1.2896e+04, 3.5747e+04],\n",
       "         [1.7828e+04, 1.1633e+03, 5.9568e+03, 4.3416e+02, 3.8714e+02,\n",
       "          9.2715e+03, 1.3056e+04, 2.4376e+02, 3.9243e+03, 3.2341e+03,\n",
       "          1.2625e+03, 1.1312e+03, 1.1597e+04, 2.6713e+03, 1.3098e+04,\n",
       "          8.9963e+03, 7.2548e+02],\n",
       "         [3.5606e+02, 7.8961e+03, 1.5884e+03, 1.8283e+04, 1.3439e+04,\n",
       "          2.0286e+03, 2.6848e+03, 1.1530e+04, 4.4820e+03, 3.9532e+03,\n",
       "          6.9975e+03, 7.0123e+03, 3.8949e+03, 9.0005e+03, 5.5636e+03,\n",
       "          3.1747e+03, 1.6924e+04],\n",
       "         [1.6209e+04, 8.0126e+03, 8.5351e+03, 7.3566e+03, 3.6928e+03,\n",
       "          1.5736e+04, 2.0306e+04, 7.3077e+03, 1.0108e+04, 7.3483e+03,\n",
       "          3.9380e+03, 6.0661e+03, 2.1220e+04, 1.0294e+03, 2.4641e+04,\n",
       "          1.7304e+04, 3.5912e+03],\n",
       "         [1.1837e+03, 5.8566e+03, 9.8074e+02, 1.5113e+04, 1.0875e+04,\n",
       "          1.4317e+03, 3.7095e+03, 8.8785e+03, 3.8849e+03, 2.2343e+03,\n",
       "          4.9193e+03, 4.9698e+03, 3.9036e+03, 6.8773e+03, 5.4500e+03,\n",
       "          2.9477e+03, 1.3738e+04],\n",
       "         [1.6704e+04, 1.4351e+03, 5.4628e+03, 6.7661e+02, 4.5867e+01,\n",
       "          9.5245e+03, 1.2673e+04, 6.1803e+02, 3.6437e+03, 3.2954e+03,\n",
       "          1.0058e+03, 1.1768e+03, 1.1990e+04, 1.7681e+03, 1.3834e+04,\n",
       "          9.2143e+03, 4.8446e+02],\n",
       "         [1.4248e+04, 5.2168e+02, 3.9357e+03, 1.0931e+03, 4.5415e+02,\n",
       "          6.9890e+03, 1.0054e+04, 1.7521e+02, 2.3399e+03, 1.9685e+03,\n",
       "          5.4336e+02, 4.1474e+02, 9.1089e+03, 2.0189e+03, 1.0657e+04,\n",
       "          6.7403e+03, 1.2423e+03],\n",
       "         [1.2558e+04, 3.0229e+01, 3.0136e+03, 2.1478e+03, 1.7670e+03,\n",
       "          4.6314e+03, 7.2956e+03, 2.4342e+02, 1.2956e+03, 1.2670e+03,\n",
       "          9.3813e+02, 2.6969e+02, 5.9923e+03, 3.5886e+03, 7.0610e+03,\n",
       "          4.2118e+03, 3.0540e+03],\n",
       "         [2.1474e+04, 3.8060e+03, 8.8564e+03, 9.9654e+02, 3.4846e+02,\n",
       "          1.4402e+04, 1.8580e+04, 2.1684e+03, 7.1455e+03, 6.0263e+03,\n",
       "          2.4600e+03, 3.1958e+03, 1.7872e+04, 2.0013e+03, 2.0161e+04,\n",
       "          1.4399e+04, 3.1182e+01],\n",
       "         [5.4692e+03, 1.1585e+03, 3.7383e+02, 7.0039e+03, 4.9685e+03,\n",
       "          1.0695e+03, 2.7654e+03, 2.8937e+03, 3.8691e+02, 3.2394e+02,\n",
       "          1.6637e+03, 1.0799e+03, 2.3518e+03, 4.4514e+03, 3.4993e+03,\n",
       "          1.1806e+03, 7.2881e+03],\n",
       "         [1.7009e+04, 8.8311e+02, 5.3685e+03, 5.9193e+02, 5.8169e+02,\n",
       "          8.4495e+03, 1.0973e+04, 2.4833e+02, 2.7913e+03, 3.1967e+03,\n",
       "          1.4316e+03, 1.0620e+03, 1.0017e+04, 3.2392e+03, 1.1438e+04,\n",
       "          7.6453e+03, 1.3316e+03],\n",
       "         [1.8133e+03, 4.1855e+03, 2.8478e+02, 1.2575e+04, 8.9476e+03,\n",
       "          8.5752e+02, 1.9782e+03, 6.9853e+03, 1.9320e+03, 1.5944e+03,\n",
       "          3.8817e+03, 3.6454e+03, 2.4852e+03, 6.2067e+03, 3.9347e+03,\n",
       "          1.5595e+03, 1.1917e+04],\n",
       "         [3.3097e+03, 2.9371e+03, 1.8155e+02, 9.1312e+03, 5.5834e+03,\n",
       "          2.2965e+03, 3.9747e+03, 4.9096e+03, 1.5995e+03, 9.5451e+02,\n",
       "          1.8751e+03, 2.1326e+03, 4.6526e+03, 3.0614e+03, 6.5415e+03,\n",
       "          3.0511e+03, 7.8805e+03],\n",
       "         [6.9148e+03, 1.1478e+03, 9.5200e+02, 6.4109e+03, 4.7593e+03,\n",
       "          1.7913e+03, 2.4307e+03, 2.7657e+03, 1.2160e+01, 1.0676e+03,\n",
       "          2.1072e+03, 1.3207e+03, 2.3877e+03, 5.1366e+03, 3.4933e+03,\n",
       "          1.2458e+03, 7.2144e+03],\n",
       "         [1.2104e+04, 2.3742e+02, 2.8292e+03, 1.8254e+03, 8.9231e+02,\n",
       "          5.4401e+03, 8.3782e+03, 2.7935e+02, 1.6305e+03, 1.1882e+03,\n",
       "          2.9594e+02, 1.0760e+02, 7.4860e+03, 1.9818e+03, 8.9664e+03,\n",
       "          5.3303e+03, 1.9412e+03],\n",
       "         [5.7397e+03, 4.1148e+03, 1.8115e+03, 8.3351e+03, 4.3135e+03,\n",
       "          5.8496e+03, 8.6486e+03, 5.3182e+03, 3.8020e+03, 2.1272e+03,\n",
       "          1.6337e+03, 2.7252e+03, 9.5891e+03, 1.0913e+03, 1.2149e+04,\n",
       "          7.1453e+03, 5.8180e+03],\n",
       "         [5.7689e+03, 1.6306e+04, 9.0283e+03, 3.1727e+04, 2.8128e+04,\n",
       "          4.3921e+03, 3.3217e+03, 2.1777e+04, 1.0965e+04, 1.2077e+04,\n",
       "          1.8831e+04, 1.6803e+04, 3.2660e+03, 2.5179e+04, 3.1668e+03,\n",
       "          4.6324e+03, 3.3367e+04],\n",
       "         [1.4619e+04, 2.1986e+02, 4.0482e+03, 1.5787e+03, 1.6333e+03,\n",
       "          5.7810e+03, 8.3182e+03, 1.5071e+02, 1.7368e+03, 2.0385e+03,\n",
       "          1.3872e+03, 6.1115e+02, 6.9205e+03, 4.1574e+03, 7.9300e+03,\n",
       "          5.0726e+03, 2.7947e+03],\n",
       "         [3.8991e+03, 1.9507e+03, 5.6479e+01, 8.4063e+03, 5.6486e+03,\n",
       "          1.0597e+03, 2.9861e+03, 3.9273e+03, 9.3858e+02, 3.5574e+02,\n",
       "          1.8055e+03, 1.5468e+03, 2.8703e+03, 4.0443e+03, 4.2631e+03,\n",
       "          1.6126e+03, 8.0109e+03],\n",
       "         [1.7658e+04, 5.6276e+03, 7.9834e+03, 4.0774e+03, 1.6215e+03,\n",
       "          1.4540e+04, 1.8999e+04, 4.5332e+03, 8.2564e+03, 6.1160e+03,\n",
       "          2.6259e+03, 4.2076e+03, 1.9248e+04, 6.7726e+02, 2.2221e+04,\n",
       "          1.5497e+04, 1.4148e+03]]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_orig_device = flattened_X1.device\n",
    "c.to(_orig_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f244e8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 17])\n"
     ]
    }
   ],
   "source": [
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c503bf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch in loaders['train']:\n",
    "    print(batch[0].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd7e679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch in loaders['test']:\n",
    "    print(batch[0].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c231b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = DatasetDistance(loaders['train'], loaders['test'], X1=X1, X2=X2, Y1=Y1, Y2=Y2,\n",
    "                           inner_ot_method = 'exact',\n",
    "                           debiased_loss = True,\n",
    "                           feature_cost = c,\n",
    "                           _x=1.0, _y=1.0,\n",
    "                           sqrt_method = 'spectral',\n",
    "                           sqrt_niters=10,\n",
    "                           precision='single',\n",
    "                           p = 2, entreg = 1e-1,\n",
    "                           device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e01dca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8f36d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from otdd.pytorch.wasserstein import pwdist_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f18a1bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_tasks = False\n",
    "inner_ot_p = 2\n",
    "inner_ot_loss='sinkhorn'\n",
    "inner_ot_debiased = False\n",
    "inner_ot_entreg = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41a1c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91645b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwdist = partial(pwdist_exact,\n",
    "                symmetric=symmetric_tasks,\n",
    "                p = inner_ot_p,\n",
    "                loss = inner_ot_loss,\n",
    "                debias=inner_ot_debiased,\n",
    "                entreg = inner_ot_entreg,\n",
    "                cost_function = c,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d4e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14, 13, 15, 12, 12, 14, 11, 12, 11, 14, 15, 13, 10, 15, 10, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "print(Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "51b7ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = torch.unique(Y1)\n",
    "c2 = torch.unique(Y1)\n",
    "n1, n2 = len(c1), len(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9455b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "print(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afa1340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 13, 14, 15])\n"
     ]
    }
   ],
   "source": [
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "183d1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_label = X1[Y1 == c1[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c0fe000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "        [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]])\n"
     ]
    }
   ],
   "source": [
    "print(X_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6108d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_label1 = X2[Y2 == c2[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "934b4979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(X_label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9124f539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024])\n",
      "torch.Size([3, 1024])\n",
      "torch.Size([7, 1024])\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([9, 1024])\n",
      "torch.Size([7, 1024])\n",
      "torch.Size([3, 1024])\n",
      "torch.Size([2, 1024])\n",
      "torch.Size([5, 1024])\n",
      "torch.Size([5, 1024])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    X_label = X1[Y1 == c1[i]]\n",
    "    print(X_label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9db360",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.feature_cost = partial(FeatureCost,\n",
    "                                   src_emb = self.src_embedding,\n",
    "                                   src_dim = (3,32,32),\n",
    "                                   tgt_emb = self.tgt_embedding,\n",
    "                                   tgt_dim = (3,32,32),\n",
    "                                   p = self.p, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwdist = partial(pwdist_exact,\n",
    "                             symmetric=self.symmetric_tasks,\n",
    "                             p = self.inner_ot_p,\n",
    "                             loss = self.inner_ot_loss,\n",
    "                             debias=self.inner_ot_debiased,\n",
    "                             entreg = self.inner_ot_entreg,\n",
    "                             cost_function = self.feature_cost,\n",
    "                             device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "128a01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = geomloss.SamplesLoss(\n",
    "            loss='sinkhorn', p=2,\n",
    "            cost=c,\n",
    "            debias=False,\n",
    "            blur=0.1**(1 / p),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94be7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "pairs = list(itertools.combinations(range(n1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e55d88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6004a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc9fcd0e773426ba6e436aae1b58d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(pairs, leave=False)\n",
    "D = torch.zeros((n1, n2), device = device, dtype=X1.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d3db0c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d50ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = geomloss.SamplesLoss(\n",
    "            loss='sinkhorn', p=2,\n",
    "            debias=False,\n",
    "            blur=0.1**(1 / p),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d98c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f6feca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x as NumPy array: [[ 0.23637404 -1.0222394   0.23795809]\n",
      " [-0.4118512   0.5532744   1.4319055 ]\n",
      " [-0.64027804  0.26863298  1.0842471 ]\n",
      " [ 1.1066191   0.6303161  -0.01383621]\n",
      " [-0.17073889  1.0128652   0.88475263]\n",
      " [-0.6653725  -0.7384063   0.8844492 ]\n",
      " [ 0.21793933 -0.9558697   0.60141236]\n",
      " [-0.2026215   1.4778774  -0.69676685]\n",
      " [-1.1358181  -0.5682623   2.5295017 ]\n",
      " [ 0.5794923   0.1590776   1.1077752 ]]\n",
      "y as NumPy array: [[-2.0398054  -0.10267979 -0.96329546]\n",
      " [ 2.6238248   1.0282892   0.89757043]\n",
      " [-1.9531504   2.2250667   1.2123191 ]\n",
      " [ 0.39188144  0.47344908  0.8685519 ]\n",
      " [ 0.13422154  0.5251658  -1.1500561 ]\n",
      " [-0.8348974  -1.806435    0.7070032 ]\n",
      " [ 0.6554972   0.5630011   0.6749198 ]\n",
      " [-1.2917346   2.5334623  -0.978294  ]\n",
      " [ 0.28702423  0.1525463  -0.35922626]\n",
      " [-0.19794202 -0.04627462 -1.2972354 ]\n",
      " [-0.6789175  -1.1342194  -0.6889773 ]\n",
      " [-0.68138576 -1.0734137   1.4142936 ]\n",
      " [-1.2484815   0.4062807  -0.12571701]\n",
      " [-0.18478991  1.9796231   1.2416492 ]\n",
      " [-1.2980149  -0.3421969   0.88671124]\n",
      " [-1.6540742   1.6433208   0.20565724]\n",
      " [-0.3878218  -0.40253115  0.48630565]\n",
      " [-0.128175    1.6245605   1.1429846 ]\n",
      " [-0.4061261  -0.57052696 -0.11721523]\n",
      " [-1.4464471  -1.7155226  -1.873302  ]]\n",
      "x as PyTorch tensor on CUDA: tensor([[ 0.2364, -1.0222,  0.2380],\n",
      "        [-0.4119,  0.5533,  1.4319],\n",
      "        [-0.6403,  0.2686,  1.0842],\n",
      "        [ 1.1066,  0.6303, -0.0138],\n",
      "        [-0.1707,  1.0129,  0.8848],\n",
      "        [-0.6654, -0.7384,  0.8844],\n",
      "        [ 0.2179, -0.9559,  0.6014],\n",
      "        [-0.2026,  1.4779, -0.6968],\n",
      "        [-1.1358, -0.5683,  2.5295],\n",
      "        [ 0.5795,  0.1591,  1.1078]], requires_grad=True)\n",
      "y as PyTorch tensor on CUDA: tensor([[-2.0398, -0.1027, -0.9633],\n",
      "        [ 2.6238,  1.0283,  0.8976],\n",
      "        [-1.9532,  2.2251,  1.2123],\n",
      "        [ 0.3919,  0.4734,  0.8686],\n",
      "        [ 0.1342,  0.5252, -1.1501],\n",
      "        [-0.8349, -1.8064,  0.7070],\n",
      "        [ 0.6555,  0.5630,  0.6749],\n",
      "        [-1.2917,  2.5335, -0.9783],\n",
      "        [ 0.2870,  0.1525, -0.3592],\n",
      "        [-0.1979, -0.0463, -1.2972],\n",
      "        [-0.6789, -1.1342, -0.6890],\n",
      "        [-0.6814, -1.0734,  1.4143],\n",
      "        [-1.2485,  0.4063, -0.1257],\n",
      "        [-0.1848,  1.9796,  1.2416],\n",
      "        [-1.2980, -0.3422,  0.8867],\n",
      "        [-1.6541,  1.6433,  0.2057],\n",
      "        [-0.3878, -0.4025,  0.4863],\n",
      "        [-0.1282,  1.6246,  1.1430],\n",
      "        [-0.4061, -0.5705, -0.1172],\n",
      "        [-1.4464, -1.7155, -1.8733]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example tensors on a CUDA device\n",
    "x = torch.randn(10, 3, requires_grad=True)\n",
    "y = torch.randn(20, 3)\n",
    "\n",
    "# Convert CUDA tensors to NumPy arrays\n",
    "x_cpu = x.cpu().detach().numpy()  # Use .detach() for tensors that require gradients\n",
    "y_cpu = y.cpu().numpy()\n",
    "\n",
    "print(\"x as NumPy array:\", x_cpu)\n",
    "print(\"y as NumPy array:\", y_cpu)\n",
    "\n",
    "# Convert NumPy arrays back to CUDA tensors\n",
    "x_tensor_cuda = torch.tensor(x_cpu, requires_grad=True)\n",
    "y_tensor_cuda = torch.tensor(y_cpu)\n",
    "\n",
    "print(\"x as PyTorch tensor on CUDA:\", x_tensor_cuda)\n",
    "print(\"y as PyTorch tensor on CUDA:\", y_tensor_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "04445c8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m co \u001b[38;5;241m=\u001b[39m distance(x_tensor_cuda\u001b[38;5;241m.\u001b[39mto(device),y_tensor_cuda\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\samples_loss.py:265\u001b[0m, in \u001b[0;36mSamplesLoss.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    262\u001b[0m     , x, , y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Run --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m values \u001b[38;5;241m=\u001b[39m routines[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss][backend](\n\u001b[0;32m    266\u001b[0m     ,\n\u001b[0;32m    267\u001b[0m     x,\n\u001b[0;32m    268\u001b[0m     ,\n\u001b[0;32m    269\u001b[0m     y,\n\u001b[0;32m    270\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp,\n\u001b[0;32m    271\u001b[0m     blur\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblur,\n\u001b[0;32m    272\u001b[0m     reach\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreach,\n\u001b[0;32m    273\u001b[0m     diameter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiameter,\n\u001b[0;32m    274\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    275\u001b[0m     truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate,\n\u001b[0;32m    276\u001b[0m     cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost,\n\u001b[0;32m    277\u001b[0m     kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel,\n\u001b[0;32m    278\u001b[0m     cluster_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_scale,\n\u001b[0;32m    279\u001b[0m     debias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias,\n\u001b[0;32m    280\u001b[0m     potentials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotentials,\n\u001b[0;32m    281\u001b[0m     labels_x\u001b[38;5;241m=\u001b[39ml_x,\n\u001b[0;32m    282\u001b[0m     labels_y\u001b[38;5;241m=\u001b[39ml_y,\n\u001b[0;32m    283\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Make sure that the output has the correct shape ------------------------------------\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotentials\n\u001b[0;32m    289\u001b[0m ):  \u001b[38;5;66;03m# Return some dual potentials (= test functions) sampled on the input measures\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\sinkhorn_samples.py:191\u001b[0m, in \u001b[0;36msinkhorn_tensorized\u001b[1;34m(a, x, b, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m C_yy \u001b[38;5;241m=\u001b[39m cost(y, y\u001b[38;5;241m.\u001b[39mdetach()) \u001b[38;5;28;01mif\u001b[39;00m debias \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# (B,M,M) torch Tensor\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Compute the relevant values of the diameter of the configuration,\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# target temperature epsilon, temperature schedule across itereations\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# and strength of the marginal constraints:\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m diameter, eps, eps_list, rho \u001b[38;5;241m=\u001b[39m scaling_parameters(\n\u001b[0;32m    192\u001b[0m     x, y, p, blur, reach, diameter, scaling\n\u001b[0;32m    193\u001b[0m )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Use an optimal transport solver to retrieve the dual potentials:\u001b[39;00m\n\u001b[0;32m    196\u001b[0m f_aa, g_bb, g_ab, f_ba \u001b[38;5;241m=\u001b[39m sinkhorn_loop(\n\u001b[0;32m    197\u001b[0m     softmin_tensorized,\n\u001b[0;32m    198\u001b[0m     log_weights(a),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m     debias\u001b[38;5;241m=\u001b[39mdebias,\n\u001b[0;32m    207\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\sinkhorn_divergence.py:163\u001b[0m, in \u001b[0;36mscaling_parameters\u001b[1;34m(x, y, p, blur, reach, diameter, scaling)\u001b[0m\n\u001b[0;32m    161\u001b[0m eps \u001b[38;5;241m=\u001b[39m blur \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m p\n\u001b[0;32m    162\u001b[0m rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m reach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m reach \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m p\n\u001b[1;32m--> 163\u001b[0m eps_list \u001b[38;5;241m=\u001b[39m epsilon_schedule(p, diameter, blur, scaling)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m diameter, eps, eps_list, rho\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\sinkhorn_divergence.py:147\u001b[0m, in \u001b[0;36mepsilon_schedule\u001b[1;34m(p, diameter, blur, scaling)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mepsilon_schedule\u001b[39m(p, diameter, blur, scaling):\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a list of values for the temperature \"epsilon\" across Sinkhorn iterations.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    We use an aggressive strategy with an exponential cooling\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m        list of float: list of values for the temperature epsilon.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     eps_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    143\u001b[0m         [diameter \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m p]\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m    145\u001b[0m             np\u001b[38;5;241m.\u001b[39mexp(e)\n\u001b[0;32m    146\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m--> 147\u001b[0m                 p \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(diameter), p \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(blur), p \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(scaling)\n\u001b[0;32m    148\u001b[0m             )\n\u001b[0;32m    149\u001b[0m         ]\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;241m+\u001b[39m [blur \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m p]\n\u001b[0;32m    151\u001b[0m     )\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m eps_list\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:1087\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "co = distance(x_tensor_cuda.to(device),y_tensor_cuda.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fb1b2c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i,j)\n\u001b[1;32m----> 3\u001b[0m     D[i, j] \u001b[38;5;241m=\u001b[39m distance(X1[Y1\u001b[38;5;241m==\u001b[39mc1[i]]\u001b[38;5;241m.\u001b[39mto(device), X1[Y1\u001b[38;5;241m==\u001b[39mc2[j]]\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\samples_loss.py:265\u001b[0m, in \u001b[0;36mSamplesLoss.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    262\u001b[0m     , x, , y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Run --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m values \u001b[38;5;241m=\u001b[39m routines[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss][backend](\n\u001b[0;32m    266\u001b[0m     ,\n\u001b[0;32m    267\u001b[0m     x,\n\u001b[0;32m    268\u001b[0m     ,\n\u001b[0;32m    269\u001b[0m     y,\n\u001b[0;32m    270\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp,\n\u001b[0;32m    271\u001b[0m     blur\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblur,\n\u001b[0;32m    272\u001b[0m     reach\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreach,\n\u001b[0;32m    273\u001b[0m     diameter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiameter,\n\u001b[0;32m    274\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    275\u001b[0m     truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate,\n\u001b[0;32m    276\u001b[0m     cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost,\n\u001b[0;32m    277\u001b[0m     kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel,\n\u001b[0;32m    278\u001b[0m     cluster_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_scale,\n\u001b[0;32m    279\u001b[0m     debias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias,\n\u001b[0;32m    280\u001b[0m     potentials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotentials,\n\u001b[0;32m    281\u001b[0m     labels_x\u001b[38;5;241m=\u001b[39ml_x,\n\u001b[0;32m    282\u001b[0m     labels_y\u001b[38;5;241m=\u001b[39ml_y,\n\u001b[0;32m    283\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Make sure that the output has the correct shape ------------------------------------\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotentials\n\u001b[0;32m    289\u001b[0m ):  \u001b[38;5;66;03m# Return some dual potentials (= test functions) sampled on the input measures\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\sinkhorn_samples.py:180\u001b[0m, in \u001b[0;36msinkhorn_tensorized\u001b[1;34m(a, x, b, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     cost \u001b[38;5;241m=\u001b[39m cost_routines[p]\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Compute the relevant cost matrices C(x_i, y_j), C(y_j, x_i), etc.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Note that we \"detach\" the gradients of the \"right-hand sides\":\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# this is coherent with the way we compute our gradients\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# in the `sinkhorn_loop(...)` routine, in the `sinkhorn_divergence.py` file.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Please refer to the comments in this file for more details.\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m C_xy \u001b[38;5;241m=\u001b[39m cost(x, y\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,N,M) torch Tensor\u001b[39;00m\n\u001b[0;32m    181\u001b[0m C_yx \u001b[38;5;241m=\u001b[39m cost(y, x\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,M,N) torch Tensor\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# N.B.: The \"auto-correlation\" matrices C(x_i, x_j) and C(y_i, y_j)\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m#       are only used by the \"debiased\" Sinkhorn algorithm.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "for i, j in pbar:\n",
    "    print(i,j)\n",
    "    D[i, j] = distance(X1[Y1==c1[i]].to(device), X1[Y1==c2[j]].to(device)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53cdb7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024]) torch.Size([3, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(X1[Y1==c1[0]].size(), X1[Y1==c2[1]].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dee0b8db",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m distance(X1[Y1\u001b[38;5;241m==\u001b[39mc1[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mto(device), X1[Y1\u001b[38;5;241m==\u001b[39mc2[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\samples_loss.py:265\u001b[0m, in \u001b[0;36mSamplesLoss.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    262\u001b[0m     , x, , y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Run --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m values \u001b[38;5;241m=\u001b[39m routines[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss][backend](\n\u001b[0;32m    266\u001b[0m     ,\n\u001b[0;32m    267\u001b[0m     x,\n\u001b[0;32m    268\u001b[0m     ,\n\u001b[0;32m    269\u001b[0m     y,\n\u001b[0;32m    270\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp,\n\u001b[0;32m    271\u001b[0m     blur\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblur,\n\u001b[0;32m    272\u001b[0m     reach\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreach,\n\u001b[0;32m    273\u001b[0m     diameter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiameter,\n\u001b[0;32m    274\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    275\u001b[0m     truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate,\n\u001b[0;32m    276\u001b[0m     cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost,\n\u001b[0;32m    277\u001b[0m     kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel,\n\u001b[0;32m    278\u001b[0m     cluster_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_scale,\n\u001b[0;32m    279\u001b[0m     debias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias,\n\u001b[0;32m    280\u001b[0m     potentials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotentials,\n\u001b[0;32m    281\u001b[0m     labels_x\u001b[38;5;241m=\u001b[39ml_x,\n\u001b[0;32m    282\u001b[0m     labels_y\u001b[38;5;241m=\u001b[39ml_y,\n\u001b[0;32m    283\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Make sure that the output has the correct shape ------------------------------------\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotentials\n\u001b[0;32m    289\u001b[0m ):  \u001b[38;5;66;03m# Return some dual potentials (= test functions) sampled on the input measures\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\sinkhorn_samples.py:180\u001b[0m, in \u001b[0;36msinkhorn_tensorized\u001b[1;34m(a, x, b, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     cost \u001b[38;5;241m=\u001b[39m cost_routines[p]\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Compute the relevant cost matrices C(x_i, y_j), C(y_j, x_i), etc.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Note that we \"detach\" the gradients of the \"right-hand sides\":\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# this is coherent with the way we compute our gradients\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# in the `sinkhorn_loop(...)` routine, in the `sinkhorn_divergence.py` file.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Please refer to the comments in this file for more details.\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m C_xy \u001b[38;5;241m=\u001b[39m cost(x, y\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,N,M) torch Tensor\u001b[39;00m\n\u001b[0;32m    181\u001b[0m C_yx \u001b[38;5;241m=\u001b[39m cost(y, x\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,M,N) torch Tensor\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# N.B.: The \"auto-correlation\" matrices C(x_i, x_j) and C(y_i, y_j)\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m#       are only used by the \"debiased\" Sinkhorn algorithm.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "distance(X1[Y1==c1[0]].to(device), X1[Y1==c2[1]].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9480c4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (5, 6), (5, 7), (5, 8), (5, 9), (6, 7), (6, 8), (6, 9), (7, 8), (7, 9), (8, 9)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9f4854faa7471bb1573125b2209f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "This is awkward. Distance computation failed. Geomloss is hard to debugBut here's a few things that might be happening:  1. Too many samples with this label, causing memory issues 2. Datatype errors, e.g., if the two datasets have different type\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\wasserstein.py:337\u001b[0m, in \u001b[0;36mpwdist_exact\u001b[1;34m(X1, Y1, X2, Y2, symmetric, loss, cost_function, p, debias, entreg, device)\u001b[0m\n\u001b[0;32m    336\u001b[0m     D[i, j] \u001b[38;5;241m=\u001b[39m distance(X1[Y1\u001b[38;5;241m==\u001b[39mc1[i]]\u001b[38;5;241m.\u001b[39mto(device), X2[Y2\u001b[38;5;241m==\u001b[39mc2[j]]\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is awkward. Distance computation failed. Geomloss is hard to debug\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    339\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a few things that might be happening: \u001b[39m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m    340\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 1. Too many samples with this label, causing memory issues\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    341\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 2. Datatype errors, e.g., if the two datasets have different type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\samples_loss.py:265\u001b[0m, in \u001b[0;36mSamplesLoss.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Run --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m values \u001b[38;5;241m=\u001b[39m routines[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss][backend](\n\u001b[0;32m    266\u001b[0m     ,\n\u001b[0;32m    267\u001b[0m     x,\n\u001b[0;32m    268\u001b[0m     ,\n\u001b[0;32m    269\u001b[0m     y,\n\u001b[0;32m    270\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp,\n\u001b[0;32m    271\u001b[0m     blur\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblur,\n\u001b[0;32m    272\u001b[0m     reach\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreach,\n\u001b[0;32m    273\u001b[0m     diameter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiameter,\n\u001b[0;32m    274\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    275\u001b[0m     truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate,\n\u001b[0;32m    276\u001b[0m     cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost,\n\u001b[0;32m    277\u001b[0m     kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel,\n\u001b[0;32m    278\u001b[0m     cluster_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_scale,\n\u001b[0;32m    279\u001b[0m     debias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias,\n\u001b[0;32m    280\u001b[0m     potentials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotentials,\n\u001b[0;32m    281\u001b[0m     labels_x\u001b[38;5;241m=\u001b[39ml_x,\n\u001b[0;32m    282\u001b[0m     labels_y\u001b[38;5;241m=\u001b[39ml_y,\n\u001b[0;32m    283\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Make sure that the output has the correct shape ------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\sinkhorn_samples.py:180\u001b[0m, in \u001b[0;36msinkhorn_tensorized\u001b[1;34m(a, x, b, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Compute the relevant cost matrices C(x_i, y_j), C(y_j, x_i), etc.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Note that we \"detach\" the gradients of the \"right-hand sides\":\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# this is coherent with the way we compute our gradients\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# in the `sinkhorn_loop(...)` routine, in the `sinkhorn_divergence.py` file.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Please refer to the comments in this file for more details.\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m C_xy \u001b[38;5;241m=\u001b[39m cost(x, y\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,N,M) torch Tensor\u001b[39;00m\n\u001b[0;32m    181\u001b[0m C_yx \u001b[38;5;241m=\u001b[39m cost(y, x\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,M,N) torch Tensor\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m DYY1 \u001b[38;5;241m=\u001b[39m pwdist(X1, Y1)\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\wasserstein.py:343\u001b[0m, in \u001b[0;36mpwdist_exact\u001b[1;34m(X1, Y1, X2, Y2, symmetric, loss, cost_function, p, debias, entreg, device)\u001b[0m\n\u001b[0;32m    342\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistance computation failed. Aborting.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m symmetric:\n\u001b[0;32m    344\u001b[0m     D[j, i] \u001b[38;5;241m=\u001b[39m D[i, j]\n",
      "\u001b[1;31mSystemExit\u001b[0m: Distance computation failed. Aborting.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2121\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2119\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2120\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2121\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mInteractiveTB\u001b[38;5;241m.\u001b[39mget_exception_only(etype,\n\u001b[0;32m   2122\u001b[0m                                                      value))\n\u001b[0;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m    569\u001b[0m             etype,\n\u001b[0;32m    570\u001b[0m             evalue,\n\u001b[0;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[0;32m    573\u001b[0m             context,\n\u001b[0;32m    574\u001b[0m         )\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FormattedTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;28mself\u001b[39m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1437\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VerboseTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1327\u001b[0m         \u001b[38;5;28mself\u001b[39m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1328\u001b[0m     )\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1171\u001b[0m ):\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m   1174\u001b[0m                                                            tb_offset)\n\u001b[0;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1064\u001b[0m )\n\u001b[0;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(cf\u001b[38;5;241m.\u001b[39mtb_frame)\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "DYY1 = pwdist(X1, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c0e4294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate the same thing again...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652cfe77ed10430ea68915533f824f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is awkward. Distance computation failed. Geomloss is hard to debugBut here's a few things that might be happening:  1. Too many samples with this label, causing memory issues 2. Datatype errors, e.g., if the two datasets have different type\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\wasserstein.py:336\u001b[0m, in \u001b[0;36mpwdist_exact\u001b[1;34m(X1, Y1, X2, Y2, symmetric, loss, cost_function, p, debias, entreg, device)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     D[i, j] \u001b[38;5;241m=\u001b[39m distance(X1[Y1\u001b[38;5;241m==\u001b[39mc1[i]]\u001b[38;5;241m.\u001b[39mto(device), X2[Y2\u001b[38;5;241m==\u001b[39mc2[j]]\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\samples_loss.py:265\u001b[0m, in \u001b[0;36mSamplesLoss.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Run --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m values \u001b[38;5;241m=\u001b[39m routines[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss][backend](\n\u001b[0;32m    266\u001b[0m     ,\n\u001b[0;32m    267\u001b[0m     x,\n\u001b[0;32m    268\u001b[0m     ,\n\u001b[0;32m    269\u001b[0m     y,\n\u001b[0;32m    270\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp,\n\u001b[0;32m    271\u001b[0m     blur\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblur,\n\u001b[0;32m    272\u001b[0m     reach\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreach,\n\u001b[0;32m    273\u001b[0m     diameter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiameter,\n\u001b[0;32m    274\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    275\u001b[0m     truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate,\n\u001b[0;32m    276\u001b[0m     cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost,\n\u001b[0;32m    277\u001b[0m     kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel,\n\u001b[0;32m    278\u001b[0m     cluster_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_scale,\n\u001b[0;32m    279\u001b[0m     debias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias,\n\u001b[0;32m    280\u001b[0m     potentials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotentials,\n\u001b[0;32m    281\u001b[0m     labels_x\u001b[38;5;241m=\u001b[39ml_x,\n\u001b[0;32m    282\u001b[0m     labels_y\u001b[38;5;241m=\u001b[39ml_y,\n\u001b[0;32m    283\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Make sure that the output has the correct shape ------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\sinkhorn_samples.py:180\u001b[0m, in \u001b[0;36msinkhorn_tensorized\u001b[1;34m(a, x, b, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Compute the relevant cost matrices C(x_i, y_j), C(y_j, x_i), etc.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Note that we \"detach\" the gradients of the \"right-hand sides\":\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# this is coherent with the way we compute our gradients\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# in the `sinkhorn_loop(...)` routine, in the `sinkhorn_divergence.py` file.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Please refer to the comments in this file for more details.\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m C_xy \u001b[38;5;241m=\u001b[39m cost(x, y\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,N,M) torch Tensor\u001b[39;00m\n\u001b[0;32m    181\u001b[0m C_yx \u001b[38;5;241m=\u001b[39m cost(y, x\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,M,N) torch Tensor\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dual_sol \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mdual_sol(maxsamples \u001b[38;5;241m=\u001b[39m training_size, return_coupling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\distance_fast.py:918\u001b[0m, in \u001b[0;36mDatasetDistance.dual_sol\u001b[1;34m(self, maxsamples, return_coupling)\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculate the same thing again...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 918\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_label_distances()\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(device_dists))\n\u001b[0;32m    920\u001b[0m to_save \u001b[38;5;241m=\u001b[39m [W]\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\distance_fast.py:551\u001b[0m, in \u001b[0;36mDatasetDistance._get_label_distances\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# Exact\u001b[39;00m\n\u001b[1;32m--> 551\u001b[0m         DYY1 \u001b[38;5;241m=\u001b[39m pwdist(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY1)\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\wasserstein.py:342\u001b[0m, in \u001b[0;36mpwdist_exact\u001b[1;34m(X1, Y1, X2, Y2, symmetric, loss, cost_function, p, debias, entreg, device)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is awkward. Distance computation failed. Geomloss is hard to debug\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    339\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a few things that might be happening: \u001b[39m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m    340\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 1. Too many samples with this label, causing memory issues\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    341\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 2. Datatype errors, e.g., if the two datasets have different type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 342\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistance computation failed. Aborting.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m symmetric:\n",
      "\u001b[1;31mSystemExit\u001b[0m: Distance computation failed. Aborting.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2121\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2119\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2120\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2121\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mInteractiveTB\u001b[38;5;241m.\u001b[39mget_exception_only(etype,\n\u001b[0;32m   2122\u001b[0m                                                      value))\n\u001b[0;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m    569\u001b[0m             etype,\n\u001b[0;32m    570\u001b[0m             evalue,\n\u001b[0;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[0;32m    573\u001b[0m             context,\n\u001b[0;32m    574\u001b[0m         )\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FormattedTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;28mself\u001b[39m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1437\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VerboseTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1327\u001b[0m         \u001b[38;5;28mself\u001b[39m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1328\u001b[0m     )\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1171\u001b[0m ):\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m   1174\u001b[0m                                                            tb_offset)\n\u001b[0;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1064\u001b[0m )\n\u001b[0;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(cf\u001b[38;5;241m.\u001b[39mtb_frame)\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "dual_sol = dist.dual_sol(maxsamples = training_size, return_coupling = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "974313d4b1133b94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:51:11.300391Z",
     "start_time": "2024-05-15T18:51:00.814095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cfb6cf3da24061b6b831e07af5dfc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1092bcde660748658e303f4811f6dbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (5, 6), (5, 7), (5, 8), (5, 9), (6, 7), (6, 8), (6, 9), (7, 8), (7, 9), (8, 9)]\n",
      "cost function: <otdd.pytorch.distance_fast.FeatureCost object at 0x000001379CC37750>\n",
      "cost function: <otdd.pytorch.distance_fast.FeatureCost object at 0x000001379CC37750>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a3cb5e000b49f0af8d385cc87c92d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 4 1024\n",
      "Batchifying feature distance computation\n",
      "This is awkward. Distance computation failed. Geomloss is hard to debugBut here's a few things that might be happening:  1. Too many samples with this label, causing memory issues 2. Datatype errors, e.g., if the two datasets have different type\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\distance_fast.py:1457\u001b[0m, in \u001b[0;36mFeatureCost.__call__\u001b[1;34m(self, X1, X2)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_emb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m-> 1457\u001b[0m X1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_emb(X1\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mreshape(B1, N1, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28mprint\u001b[39m(X1\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 3, 32, 32]' is invalid for input of size 4096",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\wasserstein.py:338\u001b[0m, in \u001b[0;36mpwdist_exact\u001b[1;34m(X1, Y1, X2, Y2, symmetric, loss, cost_function, p, debias, entreg, device)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is awkward. Distance computation failed. Geomloss is hard to debug\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    339\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms a few things that might be happening: \u001b[39m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m    340\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 1. Too many samples with this label, causing memory issues\u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[0;32m    341\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m 2. Datatype errors, e.g., if the two datasets have different type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    342\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistance computation failed. Aborting.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\samples_loss.py:265\u001b[0m, in \u001b[0;36mSamplesLoss.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Run --------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m values \u001b[38;5;241m=\u001b[39m routines[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss][backend](\n\u001b[0;32m    266\u001b[0m     ,\n\u001b[0;32m    267\u001b[0m     x,\n\u001b[0;32m    268\u001b[0m     ,\n\u001b[0;32m    269\u001b[0m     y,\n\u001b[0;32m    270\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp,\n\u001b[0;32m    271\u001b[0m     blur\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblur,\n\u001b[0;32m    272\u001b[0m     reach\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreach,\n\u001b[0;32m    273\u001b[0m     diameter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiameter,\n\u001b[0;32m    274\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[0;32m    275\u001b[0m     truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate,\n\u001b[0;32m    276\u001b[0m     cost\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost,\n\u001b[0;32m    277\u001b[0m     kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel,\n\u001b[0;32m    278\u001b[0m     cluster_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_scale,\n\u001b[0;32m    279\u001b[0m     debias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebias,\n\u001b[0;32m    280\u001b[0m     potentials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotentials,\n\u001b[0;32m    281\u001b[0m     labels_x\u001b[38;5;241m=\u001b[39ml_x,\n\u001b[0;32m    282\u001b[0m     labels_y\u001b[38;5;241m=\u001b[39ml_y,\n\u001b[0;32m    283\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Make sure that the output has the correct shape ------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\geomloss\\sinkhorn_samples.py:180\u001b[0m, in \u001b[0;36msinkhorn_tensorized\u001b[1;34m(a, x, b, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Compute the relevant cost matrices C(x_i, y_j), C(y_j, x_i), etc.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Note that we \"detach\" the gradients of the \"right-hand sides\":\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# this is coherent with the way we compute our gradients\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# in the `sinkhorn_loop(...)` routine, in the `sinkhorn_divergence.py` file.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Please refer to the comments in this file for more details.\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m C_xy \u001b[38;5;241m=\u001b[39m cost(x, y\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,N,M) torch Tensor\u001b[39;00m\n\u001b[0;32m    181\u001b[0m C_yx \u001b[38;5;241m=\u001b[39m cost(y, x\u001b[38;5;241m.\u001b[39mdetach())  \u001b[38;5;66;03m# (B,M,N) torch Tensor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\distance_fast.py:1461\u001b[0m, in \u001b[0;36mFeatureCost.__call__\u001b[1;34m(self, X1, X2)\u001b[0m\n\u001b[0;32m   1460\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatchifying feature distance computation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1461\u001b[0m         X1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batchify_computation(X1\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B1, N1, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_emb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 3, 32, 32]' is invalid for input of size 4096",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[104], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dual_sol, trained_with_flag \u001b[38;5;241m=\u001b[39m lava\u001b[38;5;241m.\u001b[39mcompute_dual(net_test, loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      2\u001b[0m                                                 training_size, shuffle_ind, resize\u001b[38;5;241m=\u001b[39mresize, device \u001b[38;5;241m=\u001b[39m device)\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\lava.py:166\u001b[0m, in \u001b[0;36mcompute_dual\u001b[1;34m(feature_extractor, trainloader, testloader, training_size, shuffle_ind, p, resize, device)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# to return 1\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# OT Dual calculation\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m dual_sol \u001b[38;5;241m=\u001b[39m get_OT_dual_sol(feature_extractor, trainloader, testloader, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, resize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dual_sol, trained_with_flag\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\lava.py:114\u001b[0m, in \u001b[0;36mget_OT_dual_sol\u001b[1;34m(feature_extractor, trainloader, testloader, training_size, p, resize, device)\u001b[0m\n\u001b[0;32m    113\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m--> 114\u001b[0m dual_sol \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mdual_sol(maxsamples \u001b[38;5;241m=\u001b[39m training_size, return_coupling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    116\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\distance_fast.py:851\u001b[0m, in \u001b[0;36mDatasetDistance.dual_sol\u001b[1;34m(self, maxsamples, return_coupling)\u001b[0m\n\u001b[0;32m    850\u001b[0m s \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m--> 851\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_label_distances()\n\u001b[0;32m    852\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/* Time to precompute label distances: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m */\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time() \u001b[38;5;241m-\u001b[39m s))\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\distance_fast.py:551\u001b[0m, in \u001b[0;36mDatasetDistance._get_label_distances\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# Exact\u001b[39;00m\n\u001b[1;32m--> 551\u001b[0m         DYY1 \u001b[38;5;241m=\u001b[39m pwdist(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY1)\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\21520\\PycharmProjects\\LAVA\\LAVA\\otdd\\pytorch\\wasserstein.py:345\u001b[0m, in \u001b[0;36mpwdist_exact\u001b[1;34m(X1, Y1, X2, Y2, symmetric, loss, cost_function, p, debias, entreg, device)\u001b[0m\n\u001b[0;32m    344\u001b[0m         D[j, i] \u001b[38;5;241m=\u001b[39m D[i, j]\n\u001b[1;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m D\n",
      "\u001b[1;31mSystemExit\u001b[0m: Distance computation failed. Aborting.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2121\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2119\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2120\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2121\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mInteractiveTB\u001b[38;5;241m.\u001b[39mget_exception_only(etype,\n\u001b[0;32m   2122\u001b[0m                                                      value))\n\u001b[0;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m    569\u001b[0m             etype,\n\u001b[0;32m    570\u001b[0m             evalue,\n\u001b[0;32m    571\u001b[0m             (etb, chained_exc_ids),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    572\u001b[0m             chained_exceptions_tb_offset,\n\u001b[0;32m    573\u001b[0m             context,\n\u001b[0;32m    574\u001b[0m         )\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FormattedTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;28mself\u001b[39m, etype, evalue, etb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1437\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VerboseTB\u001b[38;5;241m.\u001b[39mstructured_traceback(\n\u001b[0;32m   1327\u001b[0m         \u001b[38;5;28mself\u001b[39m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1328\u001b[0m     )\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1171\u001b[0m ):\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m   1174\u001b[0m                                                            tb_offset)\n\u001b[0;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1064\u001b[0m )\n\u001b[0;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\21520\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(cf\u001b[38;5;241m.\u001b[39mtb_frame)\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "dual_sol, trained_with_flag = lava.compute_dual(net_test, loaders['train'], loaders['test'],\n",
    "                                                training_size, shuffle_ind, resize=resize, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b0e35b2a233fc80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:50:07.982095Z",
     "start_time": "2024-05-15T18:50:07.952972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\21520\\\\PycharmProjects\\\\LAVA'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf7ab8aa67755222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:50:51.336347Z",
     "start_time": "2024-05-15T18:50:51.191860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lava' from 'C:\\\\Users\\\\21520\\\\PycharmProjects\\\\LAVA\\\\lava.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import lava\n",
    "importlib.reload(lava)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f8ac057cb273194",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T18:46:37.843738Z",
     "start_time": "2024-05-15T18:46:37.623845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is BB40-D6E0\n",
      "\n",
      " Directory of C:\\Users\\21520\\PycharmProjects\\LAVA\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140bf08cee7487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.repeat(1, 3, 1, 1)  # Convert grayscale to RGB\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c071dfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1024])\n",
      "torch.Size([17, 1024])\n",
      "torch.Size([50])\n",
      "torch.Size([17])\n",
      "10 6\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"save_x1y1x2y2.txt\", \"rb\") as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "X1, Y1, X2, Y2 = loaded_data\n",
    "\n",
    "# Now you can use X1, Y1, X2, and Y2 in your code\n",
    "print(X1.shape)  # Example usage\n",
    "print(X2.shape)\n",
    "print(Y1.shape)\n",
    "print(Y2.shape)\n",
    "c1 = torch.unique(Y1)\n",
    "c2 = torch.unique(Y2)\n",
    "n1, n2 = len(c1), len(c2)\n",
    "print(n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e4d787d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST\n",
      "Currrent label: 9\n",
      "New label: 2 \n",
      "TRAINNNN label:  tensor(2)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 2)\n",
      "Currrent label: 9\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 7\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 6\n",
      "New label: 5 \n",
      "TRAINNNN label:  tensor(5)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 5)\n",
      "Currrent label: 6\n",
      "New label: 2 \n",
      "TRAINNNN label:  tensor(2)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 2)\n",
      "Currrent label: 3\n",
      "New label: 2 \n",
      "TRAINNNN label:  tensor(2)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 2)\n",
      "Currrent label: 5\n",
      "New label: 8 \n",
      "TRAINNNN label:  tensor(8)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 8)\n",
      "Currrent label: 5\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 6\n",
      "New label: 9 \n",
      "TRAINNNN label:  tensor(9)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 9)\n",
      "Currrent label: 3\n",
      "New label: 7 \n",
      "TRAINNNN label:  tensor(7)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 7)\n",
      "Currrent label: 5\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 3\n",
      "New label: 2 \n",
      "TRAINNNN label:  tensor(2)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 2)\n",
      "Currrent label: 6\n",
      "New label: 4 \n",
      "TRAINNNN label:  tensor(4)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 4)\n",
      "Currrent label: 1\n",
      "New label: 8 \n",
      "TRAINNNN label:  tensor(8)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 8)\n",
      "Currrent label: 0\n",
      "New label: 3 \n",
      "TRAINNNN label:  tensor(3)\n",
      "TRAINNNN:  (tensor([[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         ...,\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]), 3)\n"
     ]
    }
   ],
   "source": [
    "loaders, shuffle_ind = lava.load_data_corrupted(corrupt_type='shuffle', dataname='MNIST', resize=resize,\n",
    "                                        training_size=training_size, test_size=valid_size, currupt_por=portion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f52872ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "for batch in loaders['train']:\n",
    "    print(len(batch[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ba596ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "for batch in loaders['test']:\n",
    "    print(len(batch[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e167629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cost = FeatureCost(src_embedding = embedder,\n",
    "                               src_dim = (1, resize,resize),\n",
    "                               tgt_embedding = embedder,\n",
    "                               tgt_dim = (1, resize,resize),\n",
    "                               p = 2,\n",
    "                               device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "22d2a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = DatasetDistance(loaders['train'], loaders['test'],\n",
    "                           inner_ot_method = 'exact',\n",
    "                           debiased_loss = True,\n",
    "                           feature_cost = feature_cost,\n",
    "                           _x=1.0, _y=1.0,\n",
    "                           sqrt_method = 'spectral',\n",
    "                           sqrt_niters=10,\n",
    "                           precision='single',\n",
    "                           p = 2, entreg = 1e-1,\n",
    "                           device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c0124f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) tensor([0, 1, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "print(dist.V1, dist.V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dbb6ddd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9dc78168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0566fe75836e47d9ba3ae26f7533aeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load full dataset: torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b05c67adae3462f94a5fbef57b49648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load full dataset: torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "dist._load_datasets(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cb4f12d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1024]) torch.Size([17, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(dist.X1.shape, dist.X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3f3dfa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50]) torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "print(dist.Y1.shape, dist.Y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f3bb6666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14, 13, 15, 12, 12, 14, 11, 12, 11, 14, 15, 13, 10, 15, 10, 10, 12])\n"
     ]
    }
   ],
   "source": [
    "print(dist.Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6dd15672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for batch in dist.D1:\n",
    "    print(len(batch[0]))\n",
    "for batch in dist.D2:\n",
    "    print(len(batch[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "591b444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (5, 6), (5, 7), (5, 8), (5, 9), (6, 7), (6, 8), (6, 9), (7, 8), (7, 9), (8, 9)]\n",
      "cost function:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5359fd6c8bb44a4297f57c206be9a6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "huhu: torch.Size([4, 1024]) torch.Size([3, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "was: tensor(727.9841, device='cuda:0')\n",
      "0 2\n",
      "huhu: torch.Size([4, 1024]) torch.Size([7, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 4, 7])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 7, 4])\n",
      "was: tensor(5487.4429, device='cuda:0')\n",
      "0 3\n",
      "huhu: torch.Size([4, 1024]) torch.Size([5, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 4, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 5, 4])\n",
      "was: tensor(9170.8467, device='cuda:0')\n",
      "0 4\n",
      "huhu: torch.Size([4, 1024]) torch.Size([9, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 4, 9])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 9, 4])\n",
      "was: tensor(4899.0498, device='cuda:0')\n",
      "0 5\n",
      "huhu: torch.Size([4, 1024]) torch.Size([7, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 4, 7])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 7, 4])\n",
      "was: tensor(3434.0608, device='cuda:0')\n",
      "0 6\n",
      "huhu: torch.Size([4, 1024]) torch.Size([3, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "was: tensor(3555.6626, device='cuda:0')\n",
      "0 7\n",
      "huhu: torch.Size([4, 1024]) torch.Size([2, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 4, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 2, 4])\n",
      "was: tensor(7825.9287, device='cuda:0')\n",
      "0 8\n",
      "huhu: torch.Size([4, 1024]) torch.Size([5, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 4, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 5, 4])\n",
      "was: tensor(1373.7068, device='cuda:0')\n",
      "0 9\n",
      "huhu: torch.Size([4, 1024]) torch.Size([5, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 4, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 5, 4])\n",
      "was: tensor(2698.0107, device='cuda:0')\n",
      "1 2\n",
      "huhu: torch.Size([3, 1024]) torch.Size([7, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "was: tensor(5782.2881, device='cuda:0')\n",
      "1 3\n",
      "huhu: torch.Size([3, 1024]) torch.Size([5, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "was: tensor(9831.0566, device='cuda:0')\n",
      "1 4\n",
      "huhu: torch.Size([3, 1024]) torch.Size([9, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 3, 9])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 9, 3])\n",
      "was: tensor(4518.3906, device='cuda:0')\n",
      "1 5\n",
      "huhu: torch.Size([3, 1024]) torch.Size([7, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "was: tensor(3782.0122, device='cuda:0')\n",
      "1 6\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(3873.9246, device='cuda:0')\n",
      "1 7\n",
      "huhu: torch.Size([3, 1024]) torch.Size([2, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "was: tensor(8309.8398, device='cuda:0')\n",
      "1 8\n",
      "huhu: torch.Size([3, 1024]) torch.Size([5, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "was: tensor(997.8967, device='cuda:0')\n",
      "1 9\n",
      "huhu: torch.Size([3, 1024]) torch.Size([5, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "was: tensor(2574.4294, device='cuda:0')\n",
      "2 3\n",
      "huhu: torch.Size([7, 1024]) torch.Size([5, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 7, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 5, 7])\n",
      "was: tensor(1967.5338, device='cuda:0')\n",
      "2 4\n",
      "huhu: torch.Size([7, 1024]) torch.Size([9, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 7, 9])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 9, 7])\n",
      "was: tensor(3708.6943, device='cuda:0')\n",
      "2 5\n",
      "huhu: torch.Size([7, 1024]) torch.Size([7, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 7, 7])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 7, 7])\n",
      "was: tensor(6503.2144, device='cuda:0')\n",
      "2 6\n",
      "huhu: torch.Size([7, 1024]) torch.Size([3, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "was: tensor(13226.7168, device='cuda:0')\n",
      "2 7\n",
      "huhu: torch.Size([7, 1024]) torch.Size([2, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 7, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 2, 7])\n",
      "was: tensor(2192.8142, device='cuda:0')\n",
      "2 8\n",
      "huhu: torch.Size([7, 1024]) torch.Size([5, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 7, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 5, 7])\n",
      "was: tensor(4360.4814, device='cuda:0')\n",
      "2 9\n",
      "huhu: torch.Size([7, 1024]) torch.Size([5, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 7, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 5, 7])\n",
      "was: tensor(2390.7476, device='cuda:0')\n",
      "3 4\n",
      "huhu: torch.Size([5, 1024]) torch.Size([9, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 5, 9])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 9, 5])\n",
      "was: tensor(8097.4526, device='cuda:0')\n",
      "3 5\n",
      "huhu: torch.Size([5, 1024]) torch.Size([7, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 5, 7])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 7, 5])\n",
      "was: tensor(10033.0410, device='cuda:0')\n",
      "3 6\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(17845.8867, device='cuda:0')\n",
      "3 7\n",
      "huhu: torch.Size([5, 1024]) torch.Size([2, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 5, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 2, 5])\n",
      "was: tensor(2920.1614, device='cuda:0')\n",
      "3 8\n",
      "huhu: torch.Size([5, 1024]) torch.Size([5, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 5, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 5, 5])\n",
      "was: tensor(7288.6279, device='cuda:0')\n",
      "3 9\n",
      "huhu: torch.Size([5, 1024]) torch.Size([5, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 5, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 5, 5])\n",
      "was: tensor(6288.4907, device='cuda:0')\n",
      "4 5\n",
      "huhu: torch.Size([9, 1024]) torch.Size([7, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 9, 7])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 7, 9])\n",
      "was: tensor(3303.1108, device='cuda:0')\n",
      "4 6\n",
      "huhu: torch.Size([9, 1024]) torch.Size([3, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 9, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 3, 9])\n",
      "was: tensor(7672.2500, device='cuda:0')\n",
      "4 7\n",
      "huhu: torch.Size([9, 1024]) torch.Size([2, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 9, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 2, 9])\n",
      "was: tensor(9422.2441, device='cuda:0')\n",
      "4 8\n",
      "huhu: torch.Size([9, 1024]) torch.Size([5, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 9, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 5, 9])\n",
      "was: tensor(3425.3037, device='cuda:0')\n",
      "4 9\n",
      "huhu: torch.Size([9, 1024]) torch.Size([5, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 9, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 5, 9])\n",
      "was: tensor(1588.4110, device='cuda:0')\n",
      "5 6\n",
      "huhu: torch.Size([7, 1024]) torch.Size([3, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "was: tensor(3526.7598, device='cuda:0')\n",
      "5 7\n",
      "huhu: torch.Size([7, 1024]) torch.Size([2, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 7, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 2, 7])\n",
      "was: tensor(11779.8691, device='cuda:0')\n",
      "5 8\n",
      "huhu: torch.Size([7, 1024]) torch.Size([5, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 7, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 5, 7])\n",
      "was: tensor(1906.3737, device='cuda:0')\n",
      "5 9\n",
      "huhu: torch.Size([7, 1024]) torch.Size([5, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 7, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 5, 7])\n",
      "was: tensor(3078.3845, device='cuda:0')\n",
      "6 7\n",
      "huhu: torch.Size([3, 1024]) torch.Size([2, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "was: tensor(19059.5039, device='cuda:0')\n",
      "6 8\n",
      "huhu: torch.Size([3, 1024]) torch.Size([5, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "was: tensor(3158.9441, device='cuda:0')\n",
      "6 9\n",
      "huhu: torch.Size([3, 1024]) torch.Size([5, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "was: tensor(5708.8438, device='cuda:0')\n",
      "7 8\n",
      "huhu: torch.Size([2, 1024]) torch.Size([5, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 2, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 5, 2])\n",
      "was: tensor(7515.7266, device='cuda:0')\n",
      "7 9\n",
      "huhu: torch.Size([2, 1024]) torch.Size([5, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 2, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 5, 2])\n",
      "was: tensor(7236.1860, device='cuda:0')\n",
      "8 9\n",
      "huhu: torch.Size([5, 1024]) torch.Size([5, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 5, 5])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 5, 5])\n",
      "was: tensor(1678.0430, device='cuda:0')\n",
      "6 6\n",
      "[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n",
      "cost function:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9155963836f447581446ce7142e3ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "huhu: torch.Size([3, 1024]) torch.Size([2, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "was: tensor(1421.4941, device='cuda:0')\n",
      "0 2\n",
      "huhu: torch.Size([3, 1024]) torch.Size([4, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "was: tensor(12837.8008, device='cuda:0')\n",
      "0 3\n",
      "huhu: torch.Size([3, 1024]) torch.Size([2, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "was: tensor(5481.5630, device='cuda:0')\n",
      "0 4\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(3272.4238, device='cuda:0')\n",
      "0 5\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(7547.8379, device='cuda:0')\n",
      "1 2\n",
      "huhu: torch.Size([2, 1024]) torch.Size([4, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 2, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 4, 2])\n",
      "was: tensor(9108.9844, device='cuda:0')\n",
      "1 3\n",
      "huhu: torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 2, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 2, 2])\n",
      "was: tensor(3887.0498, device='cuda:0')\n",
      "1 4\n",
      "huhu: torch.Size([2, 1024]) torch.Size([3, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "was: tensor(2251.4348, device='cuda:0')\n",
      "1 5\n",
      "huhu: torch.Size([2, 1024]) torch.Size([3, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "was: tensor(3856.8416, device='cuda:0')\n",
      "2 3\n",
      "huhu: torch.Size([4, 1024]) torch.Size([2, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 4, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 2, 4])\n",
      "was: tensor(1845.0972, device='cuda:0')\n",
      "2 4\n",
      "huhu: torch.Size([4, 1024]) torch.Size([3, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "was: tensor(10727.9453, device='cuda:0')\n",
      "2 5\n",
      "huhu: torch.Size([4, 1024]) torch.Size([3, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "was: tensor(3080.3672, device='cuda:0')\n",
      "3 4\n",
      "huhu: torch.Size([2, 1024]) torch.Size([3, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "was: tensor(4948.5522, device='cuda:0')\n",
      "3 5\n",
      "huhu: torch.Size([2, 1024]) torch.Size([3, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "was: tensor(1653.2192, device='cuda:0')\n",
      "4 5\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(3720.0244, device='cuda:0')\n",
      "10 6\n",
      "[(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (5, 0), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (7, 0), (7, 1), (7, 2), (7, 3), (7, 4), (7, 5), (8, 0), (8, 1), (8, 2), (8, 3), (8, 4), (8, 5), (9, 0), (9, 1), (9, 2), (9, 3), (9, 4), (9, 5)]\n",
      "cost function:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcb31e92a68431cbc2647c9c91fd532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "huhu: torch.Size([4, 1024]) torch.Size([3, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "was: tensor(1134.1279, device='cuda:0')\n",
      "0 1\n",
      "huhu: torch.Size([4, 1024]) torch.Size([2, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 4, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 2, 4])\n",
      "was: tensor(876.6338, device='cuda:0')\n",
      "0 2\n",
      "huhu: torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 4, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 4, 4])\n",
      "was: tensor(7223.5078, device='cuda:0')\n",
      "0 3\n",
      "huhu: torch.Size([4, 1024]) torch.Size([2, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 4, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 2, 4])\n",
      "was: tensor(2394.4346, device='cuda:0')\n",
      "0 4\n",
      "huhu: torch.Size([4, 1024]) torch.Size([3, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "was: tensor(2170.8447, device='cuda:0')\n",
      "0 5\n",
      "huhu: torch.Size([4, 1024]) torch.Size([3, 1024])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "was: tensor(3522.3743, device='cuda:0')\n",
      "1 0\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(1594.6130, device='cuda:0')\n",
      "1 1\n",
      "huhu: torch.Size([3, 1024]) torch.Size([2, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "was: tensor(116.0882, device='cuda:0')\n",
      "1 2\n",
      "huhu: torch.Size([3, 1024]) torch.Size([4, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "was: tensor(7542.1660, device='cuda:0')\n",
      "1 3\n",
      "huhu: torch.Size([3, 1024]) torch.Size([2, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "was: tensor(2896.0273, device='cuda:0')\n",
      "1 4\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(2664.2661, device='cuda:0')\n",
      "1 5\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(3446.1055, device='cuda:0')\n",
      "2 0\n",
      "huhu: torch.Size([7, 1024]) torch.Size([3, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "was: tensor(11127.5049, device='cuda:0')\n",
      "2 1\n",
      "huhu: torch.Size([7, 1024]) torch.Size([2, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 7, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 2, 7])\n",
      "was: tensor(6598.5044, device='cuda:0')\n",
      "2 2\n",
      "huhu: torch.Size([7, 1024]) torch.Size([4, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 7, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 4, 7])\n",
      "was: tensor(2583.0576, device='cuda:0')\n",
      "2 3\n",
      "huhu: torch.Size([7, 1024]) torch.Size([2, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 7, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 2, 7])\n",
      "was: tensor(2822.5771, device='cuda:0')\n",
      "2 4\n",
      "huhu: torch.Size([7, 1024]) torch.Size([3, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "was: tensor(5852.9790, device='cuda:0')\n",
      "2 5\n",
      "huhu: torch.Size([7, 1024]) torch.Size([3, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "was: tensor(1093.8008, device='cuda:0')\n",
      "3 0\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(16614.9238, device='cuda:0')\n",
      "3 1\n",
      "huhu: torch.Size([5, 1024]) torch.Size([2, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 5, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 2, 5])\n",
      "was: tensor(10142.2119, device='cuda:0')\n",
      "3 2\n",
      "huhu: torch.Size([5, 1024]) torch.Size([4, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 5, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 4, 5])\n",
      "was: tensor(4277.8389, device='cuda:0')\n",
      "3 3\n",
      "huhu: torch.Size([5, 1024]) torch.Size([2, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 5, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 2, 5])\n",
      "was: tensor(6031.0449, device='cuda:0')\n",
      "3 4\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(9948.1074, device='cuda:0')\n",
      "3 5\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(2471.9341, device='cuda:0')\n",
      "4 0\n",
      "huhu: torch.Size([9, 1024]) torch.Size([3, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 9, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 3, 9])\n",
      "was: tensor(8144.9814, device='cuda:0')\n",
      "4 1\n",
      "huhu: torch.Size([9, 1024]) torch.Size([2, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 9, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 2, 9])\n",
      "was: tensor(4519.9229, device='cuda:0')\n",
      "4 2\n",
      "huhu: torch.Size([9, 1024]) torch.Size([4, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 9, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 4, 9])\n",
      "was: tensor(9489.5557, device='cuda:0')\n",
      "4 3\n",
      "huhu: torch.Size([9, 1024]) torch.Size([2, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 9, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 2, 9])\n",
      "was: tensor(6478.4482, device='cuda:0')\n",
      "4 4\n",
      "huhu: torch.Size([9, 1024]) torch.Size([3, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 9, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 3, 9])\n",
      "was: tensor(2492.5322, device='cuda:0')\n",
      "4 5\n",
      "huhu: torch.Size([9, 1024]) torch.Size([3, 1024])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 9, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 9 1024\n",
      "torch.Size([1, 9, 100])\n",
      "torch.Size([1, 3, 9])\n",
      "was: tensor(3787.1611, device='cuda:0')\n",
      "5 0\n",
      "huhu: torch.Size([7, 1024]) torch.Size([3, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "was: tensor(4715.1406, device='cuda:0')\n",
      "5 1\n",
      "huhu: torch.Size([7, 1024]) torch.Size([2, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 7, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 2, 7])\n",
      "was: tensor(3473.4507, device='cuda:0')\n",
      "5 2\n",
      "huhu: torch.Size([7, 1024]) torch.Size([4, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 7, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 4, 7])\n",
      "was: tensor(13087.9102, device='cuda:0')\n",
      "5 3\n",
      "huhu: torch.Size([7, 1024]) torch.Size([2, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 7, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 2, 7])\n",
      "was: tensor(6975.9297, device='cuda:0')\n",
      "5 4\n",
      "huhu: torch.Size([7, 1024]) torch.Size([3, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "was: tensor(1196.1936, device='cuda:0')\n",
      "5 5\n",
      "huhu: torch.Size([7, 1024]) torch.Size([3, 1024])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 7, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 7 1024\n",
      "torch.Size([1, 7, 100])\n",
      "torch.Size([1, 3, 7])\n",
      "was: tensor(4526.0693, device='cuda:0')\n",
      "6 0\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(1975.7388, device='cuda:0')\n",
      "6 1\n",
      "huhu: torch.Size([3, 1024]) torch.Size([2, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "was: tensor(2974.5225, device='cuda:0')\n",
      "6 2\n",
      "huhu: torch.Size([3, 1024]) torch.Size([4, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 3, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 4, 3])\n",
      "was: tensor(19035.8906, device='cuda:0')\n",
      "6 3\n",
      "huhu: torch.Size([3, 1024]) torch.Size([2, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "was: tensor(10173.3643, device='cuda:0')\n",
      "6 4\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(2595.9185, device='cuda:0')\n",
      "6 5\n",
      "huhu: torch.Size([3, 1024]) torch.Size([3, 1024])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 3, 3])\n",
      "was: tensor(9574.2109, device='cuda:0')\n",
      "7 0\n",
      "huhu: torch.Size([2, 1024]) torch.Size([3, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "was: tensor(13818.4121, device='cuda:0')\n",
      "7 1\n",
      "huhu: torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 2, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 2, 2])\n",
      "was: tensor(9615.4609, device='cuda:0')\n",
      "7 2\n",
      "huhu: torch.Size([2, 1024]) torch.Size([4, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 2, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 4, 2])\n",
      "was: tensor(969.1218, device='cuda:0')\n",
      "7 3\n",
      "huhu: torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 2, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 2, 2])\n",
      "was: tensor(2544.9526, device='cuda:0')\n",
      "7 4\n",
      "huhu: torch.Size([2, 1024]) torch.Size([3, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "was: tensor(10239.4082, device='cuda:0')\n",
      "7 5\n",
      "huhu: torch.Size([2, 1024]) torch.Size([3, 1024])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 2, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 3, 2])\n",
      "was: tensor(2516.4773, device='cuda:0')\n",
      "8 0\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(2843.1790, device='cuda:0')\n",
      "8 1\n",
      "huhu: torch.Size([5, 1024]) torch.Size([2, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 5, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 2, 5])\n",
      "was: tensor(1129.9922, device='cuda:0')\n",
      "8 2\n",
      "huhu: torch.Size([5, 1024]) torch.Size([4, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 5, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 4, 5])\n",
      "was: tensor(7772.2437, device='cuda:0')\n",
      "8 3\n",
      "huhu: torch.Size([5, 1024]) torch.Size([2, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 5, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 2, 5])\n",
      "was: tensor(3154.6377, device='cuda:0')\n",
      "8 4\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(1453.3425, device='cuda:0')\n",
      "8 5\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(2256.7878, device='cuda:0')\n",
      "9 0\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(5786.1855, device='cuda:0')\n",
      "9 1\n",
      "huhu: torch.Size([5, 1024]) torch.Size([2, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 5, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 2, 5])\n",
      "was: tensor(2870.8760, device='cuda:0')\n",
      "9 2\n",
      "huhu: torch.Size([5, 1024]) torch.Size([4, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "torch.Size([1, 5, 4])\n",
      "1 4 1024\n",
      "torch.Size([1, 4, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 4, 5])\n",
      "was: tensor(6293.9639, device='cuda:0')\n",
      "9 3\n",
      "huhu: torch.Size([5, 1024]) torch.Size([2, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 5, 2])\n",
      "1 2 1024\n",
      "torch.Size([1, 2, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 2, 5])\n",
      "was: tensor(3526.7334, device='cuda:0')\n",
      "9 4\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(2264.4192, device='cuda:0')\n",
      "9 5\n",
      "huhu: torch.Size([5, 1024]) torch.Size([3, 1024])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "torch.Size([1, 5, 3])\n",
      "1 3 1024\n",
      "torch.Size([1, 3, 100])\n",
      "1 5 1024\n",
      "torch.Size([1, 5, 100])\n",
      "torch.Size([1, 3, 5])\n",
      "was: tensor(2239.1121, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "W = dist._get_label_distances().to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "20962d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9633e43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0.0000,   727.9841,  5487.4429,  9170.8467,  4899.0498,  3434.0608,\n",
       "          3555.6626,  7825.9287,  1373.7068,  2698.0107,  1134.1279,   876.6338,\n",
       "          7223.5078,  2394.4346,  2170.8447,  3522.3743],\n",
       "        [  727.9841,     0.0000,  5782.2881,  9831.0566,  4518.3906,  3782.0122,\n",
       "          3873.9246,  8309.8398,   997.8967,  2574.4294,  1594.6130,   116.0882,\n",
       "          7542.1660,  2896.0273,  2664.2661,  3446.1055],\n",
       "        [ 5487.4429,  5782.2881,     0.0000,  1967.5338,  3708.6943,  6503.2144,\n",
       "         13226.7168,  2192.8142,  4360.4814,  2390.7476, 11127.5049,  6598.5044,\n",
       "          2583.0576,  2822.5771,  5852.9790,  1093.8008],\n",
       "        [ 9170.8467,  9831.0566,  1967.5338,     0.0000,  8097.4526, 10033.0410,\n",
       "         17845.8867,  2920.1614,  7288.6279,  6288.4907, 16614.9238, 10142.2119,\n",
       "          4277.8389,  6031.0449,  9948.1074,  2471.9341],\n",
       "        [ 4899.0498,  4518.3906,  3708.6943,  8097.4526,     0.0000,  3303.1108,\n",
       "          7672.2500,  9422.2441,  3425.3037,  1588.4110,  8144.9814,  4519.9229,\n",
       "          9489.5557,  6478.4482,  2492.5322,  3787.1611],\n",
       "        [ 3434.0608,  3782.0122,  6503.2144, 10033.0410,  3303.1108,     0.0000,\n",
       "          3526.7598, 11779.8691,  1906.3737,  3078.3845,  4715.1406,  3473.4507,\n",
       "         13087.9102,  6975.9297,  1196.1936,  4526.0693],\n",
       "        [ 3555.6626,  3873.9246, 13226.7168, 17845.8867,  7672.2500,  3526.7598,\n",
       "             0.0000, 19059.5039,  3158.9441,  5708.8438,  1975.7388,  2974.5225,\n",
       "         19035.8906, 10173.3643,  2595.9185,  9574.2109],\n",
       "        [ 7825.9287,  8309.8398,  2192.8142,  2920.1614,  9422.2441, 11779.8691,\n",
       "         19059.5039,     0.0000,  7515.7266,  7236.1860, 13818.4121,  9615.4609,\n",
       "           969.1218,  2544.9526, 10239.4082,  2516.4773],\n",
       "        [ 1373.7068,   997.8967,  4360.4814,  7288.6279,  3425.3037,  1906.3737,\n",
       "          3158.9441,  7515.7266,     0.0000,  1678.0430,  2843.1790,  1129.9922,\n",
       "          7772.2437,  3154.6377,  1453.3425,  2256.7878],\n",
       "        [ 2698.0107,  2574.4294,  2390.7476,  6288.4907,  1588.4110,  3078.3845,\n",
       "          5708.8438,  7236.1860,  1678.0430,     0.0000,  5786.1855,  2870.8760,\n",
       "          6293.9639,  3526.7334,  2264.4192,  2239.1121],\n",
       "        [ 1134.1279,  1594.6130, 11127.5049, 16614.9238,  8144.9814,  4715.1406,\n",
       "          1975.7388, 13818.4121,  2843.1790,  5786.1855,     0.0000,  1421.4941,\n",
       "         12837.8008,  5481.5630,  3272.4238,  7547.8379],\n",
       "        [  876.6338,   116.0882,  6598.5044, 10142.2119,  4519.9229,  3473.4507,\n",
       "          2974.5225,  9615.4609,  1129.9922,  2870.8760,  1421.4941,     0.0000,\n",
       "          9108.9844,  3887.0498,  2251.4348,  3856.8416],\n",
       "        [ 7223.5078,  7542.1660,  2583.0576,  4277.8389,  9489.5557, 13087.9102,\n",
       "         19035.8906,   969.1218,  7772.2437,  6293.9639, 12837.8008,  9108.9844,\n",
       "             0.0000,  1845.0972, 10727.9453,  3080.3672],\n",
       "        [ 2394.4346,  2896.0273,  2822.5771,  6031.0449,  6478.4482,  6975.9297,\n",
       "         10173.3643,  2544.9526,  3154.6377,  3526.7334,  5481.5630,  3887.0498,\n",
       "          1845.0972,     0.0000,  4948.5522,  1653.2192],\n",
       "        [ 2170.8447,  2664.2661,  5852.9790,  9948.1074,  2492.5322,  1196.1936,\n",
       "          2595.9185, 10239.4082,  1453.3425,  2264.4192,  3272.4238,  2251.4348,\n",
       "         10727.9453,  4948.5522,     0.0000,  3720.0244],\n",
       "        [ 3522.3743,  3446.1055,  1093.8008,  2471.9341,  3787.1611,  4526.0693,\n",
       "          9574.2109,  2516.4773,  2256.7878,  2239.1121,  7547.8379,  3856.8416,\n",
       "          3080.3672,  1653.2192,  3720.0244,     0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6f93772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = dist._get_label_distances().to(torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "117a59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_geomloss = partial(\n",
    "                batch_augmented_cost,\n",
    "                W=W,\n",
    "                _x=dist._x,\n",
    "                _y=dist._y,\n",
    "                feature_cost=dist.feature_cost\n",
    "            )\n",
    "\n",
    "loss = geomloss.SamplesLoss(\n",
    "                loss=dist.loss, p=dist.p,\n",
    "                cost=cost_geomloss,\n",
    "                debias=dist.debiased_loss,\n",
    "                blur=dist.entreg**(1 / dist.p),\n",
    "                backend='tensorized'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2c3a853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxsamples = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2ea3920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if maxsamples and dist.X1.shape[0] > maxsamples:\n",
    "    idxs_1 = sorted(np.random.choice(\n",
    "    dist.X1.shape[0], maxsamples, replace=False))\n",
    "else:\n",
    "    idxs_1 = np.s_[:]  # hack to get a full slice\n",
    "\n",
    "if maxsamples and dist.X2.shape[0] > maxsamples:\n",
    "    idxs_2 = sorted(np.random.choice(\n",
    "    dist.X2.shape[0], maxsamples, replace=False))\n",
    "else:\n",
    "    idxs_2 = np.s_[:]  # hack to get a full slice\n",
    "Z1 = torch.cat((dist.X1[idxs_1],\n",
    "                dist.Y1[idxs_1].type(dist.X1.dtype).unsqueeze(1)), -1)\n",
    "Z2 = torch.cat((dist.X2[idxs_2],\n",
    "                dist.Y2[idxs_2].type(dist.X2.dtype).unsqueeze(1)), -1)\n",
    "Z1 = Z1.to(device)\n",
    "Z2 = Z2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2f72394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss.debias = False\n",
    "    loss.potentials = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "04f6ce11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1025]) torch.Size([17, 1025])\n"
     ]
    }
   ],
   "source": [
    "print(Z1.shape, Z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8f728ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 shape in batch:  torch.Size([1, 50, 1025])\n",
      "Z2 shape in batch:  torch.Size([1, 17, 1025])\n",
      "1 50 1024\n",
      "torch.Size([1, 50, 100])\n",
      "1 17 1024\n",
      "torch.Size([1, 17, 100])\n",
      "torch.Size([1, 50, 17])\n",
      "torch.Size([1, 50, 17])\n",
      "Gia tri M: tensor([[[110, 109, 111, 108, 108, 110, 107, 108, 107, 110, 111, 109, 106, 111,\n",
      "          106, 106, 108],\n",
      "         [158, 157, 159, 156, 156, 158, 155, 156, 155, 158, 159, 157, 154, 159,\n",
      "          154, 154, 156],\n",
      "         [ 62,  61,  63,  60,  60,  62,  59,  60,  59,  62,  63,  61,  58,  63,\n",
      "           58,  58,  60],\n",
      "         [ 78,  77,  79,  76,  76,  78,  75,  76,  75,  78,  79,  77,  74,  79,\n",
      "           74,  74,  76],\n",
      "         [ 78,  77,  79,  76,  76,  78,  75,  76,  75,  78,  79,  77,  74,  79,\n",
      "           74,  74,  76],\n",
      "         [ 94,  93,  95,  92,  92,  94,  91,  92,  91,  94,  95,  93,  90,  95,\n",
      "           90,  90,  92],\n",
      "         [ 30,  29,  31,  28,  28,  30,  27,  28,  27,  30,  31,  29,  26,  31,\n",
      "           26,  26,  28],\n",
      "         [ 78,  77,  79,  76,  76,  78,  75,  76,  75,  78,  79,  77,  74,  79,\n",
      "           74,  74,  76],\n",
      "         [ 62,  61,  63,  60,  60,  62,  59,  60,  59,  62,  63,  61,  58,  63,\n",
      "           58,  58,  60],\n",
      "         [ 30,  29,  31,  28,  28,  30,  27,  28,  27,  30,  31,  29,  26,  31,\n",
      "           26,  26,  28],\n",
      "         [ 62,  61,  63,  60,  60,  62,  59,  60,  59,  62,  63,  61,  58,  63,\n",
      "           58,  58,  60],\n",
      "         [ 94,  93,  95,  92,  92,  94,  91,  92,  91,  94,  95,  93,  90,  95,\n",
      "           90,  90,  92],\n",
      "         [158, 157, 159, 156, 156, 158, 155, 156, 155, 158, 159, 157, 154, 159,\n",
      "          154, 154, 156],\n",
      "         [ 46,  45,  47,  44,  44,  46,  43,  44,  43,  46,  47,  45,  42,  47,\n",
      "           42,  42,  44],\n",
      "         [ 62,  61,  63,  60,  60,  62,  59,  60,  59,  62,  63,  61,  58,  63,\n",
      "           58,  58,  60],\n",
      "         [142, 141, 143, 140, 140, 142, 139, 140, 139, 142, 143, 141, 138, 143,\n",
      "          138, 138, 140],\n",
      "         [ 14,  13,  15,  12,  12,  14,  11,  12,  11,  14,  15,  13,  10,  15,\n",
      "           10,  10,  12],\n",
      "         [ 46,  45,  47,  44,  44,  46,  43,  44,  43,  46,  47,  45,  42,  47,\n",
      "           42,  42,  44],\n",
      "         [ 14,  13,  15,  12,  12,  14,  11,  12,  11,  14,  15,  13,  10,  15,\n",
      "           10,  10,  12],\n",
      "         [110, 109, 111, 108, 108, 110, 107, 108, 107, 110, 111, 109, 106, 111,\n",
      "          106, 106, 108],\n",
      "         [ 78,  77,  79,  76,  76,  78,  75,  76,  75,  78,  79,  77,  74,  79,\n",
      "           74,  74,  76],\n",
      "         [142, 141, 143, 140, 140, 142, 139, 140, 139, 142, 143, 141, 138, 143,\n",
      "          138, 138, 140],\n",
      "         [142, 141, 143, 140, 140, 142, 139, 140, 139, 142, 143, 141, 138, 143,\n",
      "          138, 138, 140],\n",
      "         [ 78,  77,  79,  76,  76,  78,  75,  76,  75,  78,  79,  77,  74,  79,\n",
      "           74,  74,  76],\n",
      "         [158, 157, 159, 156, 156, 158, 155, 156, 155, 158, 159, 157, 154, 159,\n",
      "          154, 154, 156],\n",
      "         [ 46,  45,  47,  44,  44,  46,  43,  44,  43,  46,  47,  45,  42,  47,\n",
      "           42,  42,  44],\n",
      "         [ 94,  93,  95,  92,  92,  94,  91,  92,  91,  94,  95,  93,  90,  95,\n",
      "           90,  90,  92],\n",
      "         [ 94,  93,  95,  92,  92,  94,  91,  92,  91,  94,  95,  93,  90,  95,\n",
      "           90,  90,  92],\n",
      "         [ 14,  13,  15,  12,  12,  14,  11,  12,  11,  14,  15,  13,  10,  15,\n",
      "           10,  10,  12],\n",
      "         [142, 141, 143, 140, 140, 142, 139, 140, 139, 142, 143, 141, 138, 143,\n",
      "          138, 138, 140],\n",
      "         [ 78,  77,  79,  76,  76,  78,  75,  76,  75,  78,  79,  77,  74,  79,\n",
      "           74,  74,  76],\n",
      "         [ 78,  77,  79,  76,  76,  78,  75,  76,  75,  78,  79,  77,  74,  79,\n",
      "           74,  74,  76],\n",
      "         [ 94,  93,  95,  92,  92,  94,  91,  92,  91,  94,  95,  93,  90,  95,\n",
      "           90,  90,  92],\n",
      "         [ 62,  61,  63,  60,  60,  62,  59,  60,  59,  62,  63,  61,  58,  63,\n",
      "           58,  58,  60],\n",
      "         [ 46,  45,  47,  44,  44,  46,  43,  44,  43,  46,  47,  45,  42,  47,\n",
      "           42,  42,  44],\n",
      "         [ 46,  45,  47,  44,  44,  46,  43,  44,  43,  46,  47,  45,  42,  47,\n",
      "           42,  42,  44],\n",
      "         [ 78,  77,  79,  76,  76,  78,  75,  76,  75,  78,  79,  77,  74,  79,\n",
      "           74,  74,  76],\n",
      "         [ 46,  45,  47,  44,  44,  46,  43,  44,  43,  46,  47,  45,  42,  47,\n",
      "           42,  42,  44],\n",
      "         [ 46,  45,  47,  44,  44,  46,  43,  44,  43,  46,  47,  45,  42,  47,\n",
      "           42,  42,  44],\n",
      "         [ 14,  13,  15,  12,  12,  14,  11,  12,  11,  14,  15,  13,  10,  15,\n",
      "           10,  10,  12],\n",
      "         [126, 125, 127, 124, 124, 126, 123, 124, 123, 126, 127, 125, 122, 127,\n",
      "          122, 122, 124],\n",
      "         [ 94,  93,  95,  92,  92,  94,  91,  92,  91,  94,  95,  93,  90,  95,\n",
      "           90,  90,  92],\n",
      "         [158, 157, 159, 156, 156, 158, 155, 156, 155, 158, 159, 157, 154, 159,\n",
      "          154, 154, 156],\n",
      "         [ 30,  29,  31,  28,  28,  30,  27,  28,  27,  30,  31,  29,  26,  31,\n",
      "           26,  26,  28],\n",
      "         [158, 157, 159, 156, 156, 158, 155, 156, 155, 158, 159, 157, 154, 159,\n",
      "          154, 154, 156],\n",
      "         [ 94,  93,  95,  92,  92,  94,  91,  92,  91,  94,  95,  93,  90,  95,\n",
      "           90,  90,  92],\n",
      "         [110, 109, 111, 108, 108, 110, 107, 108, 107, 110, 111, 109, 106, 111,\n",
      "          106, 106, 108],\n",
      "         [ 78,  77,  79,  76,  76,  78,  75,  76,  75,  78,  79,  77,  74,  79,\n",
      "           74,  74,  76],\n",
      "         [142, 141, 143, 140, 140, 142, 139, 140, 139, 142, 143, 141, 138, 143,\n",
      "          138, 138, 140],\n",
      "         [126, 125, 127, 124, 124, 126, 123, 124, 123, 126, 127, 125, 122, 127,\n",
      "          122, 122, 124]]], device='cuda:0')\n",
      "torch.Size([1, 50, 17])\n",
      "torch.Size([1, 50, 17])\n",
      "gia tri D: tensor([[[ 2584.6584, 15209.9414,  8176.5547, 32161.0352, 27834.1055,\n",
      "           2990.8381,  3083.6206, 24056.8398,  7364.0854,  7356.2441,\n",
      "          15455.3613, 14908.4277,  3276.7131, 19288.6309,  4202.7832,\n",
      "           3438.3303, 32066.8223],\n",
      "         [12525.3623,  1772.0425,  3562.7827,  5578.2061,  4860.2495,\n",
      "           5341.5220,  8240.2461,  3507.5161,  2486.8325,  2066.5142,\n",
      "           1775.9574,  1882.0630,  8617.3662,  4197.7856,  9801.7021,\n",
      "           6814.3896,  6214.1680],\n",
      "         [ 9937.3975,  6097.2266,  2583.4045, 13423.9473, 11515.6328,\n",
      "           5100.4990,  6494.7739,  7806.5405,  6582.7778,  6657.1553,\n",
      "           5832.9492,  6387.5264,  8869.7158, 10076.7148,  9423.3174,\n",
      "           8486.5830, 14525.5605],\n",
      "         [ 1771.5786, 17763.0684,  6989.6353, 31628.3164, 25033.7852,\n",
      "           7341.6567,  7880.9888, 23841.1484, 11652.6328, 10305.6367,\n",
      "          14458.3242, 16338.4395, 12358.1348, 15556.6055, 14562.9629,\n",
      "          11690.4238, 29146.6094],\n",
      "         [ 2453.3091, 22546.2363, 10016.5527, 39045.8125, 31991.3867,\n",
      "           8873.9180,  9180.0156, 29597.5586, 15484.5742, 13984.1641,\n",
      "          19654.3125, 21204.5449, 13513.6465, 21646.5586, 15374.8418,\n",
      "          13498.5605, 36779.7773],\n",
      "         [ 1755.0695,  9009.6221,  2967.5854, 21385.2734, 17372.6602,\n",
      "           1623.4757,  3562.0535, 15251.4629,  4458.0576,  3053.7490,\n",
      "           7427.9702,  8413.3594,  4930.1172,  9827.7012,  6386.6875,\n",
      "           4171.5156, 20620.5156],\n",
      "         [ 6191.5508,  7430.3584,  4428.4941, 19306.7422, 16882.5078,\n",
      "           2648.0979,    83.2082, 13125.0596,  2422.9231,  5861.3047,\n",
      "           9633.5312,  7903.8496,  1341.9628, 14238.0234,  1950.0800,\n",
      "           1357.0956, 20804.8828],\n",
      "         [17351.4395,  6924.3169,  8170.5981,  7432.8252,  5435.7241,\n",
      "          13203.8340, 18136.5137,  7550.5845,  8252.7480,  5709.6255,\n",
      "           3385.2466,  5859.3101, 19986.4766,  2328.4048, 22602.2266,\n",
      "          16602.1641,  5529.1953],\n",
      "         [28327.1328, 16634.0625, 16003.4795, 13159.0098,  9145.0977,\n",
      "          28666.1406, 34775.5703, 14250.3691, 22031.7188, 17906.5508,\n",
      "           9442.2881, 14131.7715, 38924.5352,  4937.8232, 42932.8984,\n",
      "          34198.4219,  8191.4414],\n",
      "         [ 9278.1445,  2673.0010,  3182.1680, 10007.0977,  8654.5859,\n",
      "           3530.0276,  2563.5442,  6539.6865,   105.8312,  2859.0452,\n",
      "           4226.2363,  3007.7227,  3254.9551,  7505.7109,  4266.2832,\n",
      "           2164.6211, 11148.3965],\n",
      "         [19492.3398,  8656.9668,  7783.8047,  7562.4551,  4417.4458,\n",
      "          17845.0508, 22170.5332,  7224.8218, 12652.4551, 10263.2861,\n",
      "           3598.5706,  7060.5127, 25990.7148,  1572.1506, 29041.7578,\n",
      "          22414.8711,  4544.8428],\n",
      "         [ 2584.8662,  7551.0850,  2526.7671, 18224.3047, 14314.1084,\n",
      "           2257.7412,  5283.0303, 13077.9609,  4112.8213,  1908.1144,\n",
      "           5290.5073,  6695.3535,  6424.0547,  6793.0151,  8178.5703,\n",
      "           5112.5215, 16888.0996],\n",
      "         [ 2556.3853, 17583.6016,  7702.0679, 33906.6602, 28375.3848,\n",
      "           5473.9399,  5841.7661, 24295.5762, 12118.2383, 11228.1084,\n",
      "          16913.6016, 16919.1562,  8273.4912, 20606.5508,  9296.2100,\n",
      "           8731.8818, 33080.7031],\n",
      "         [16640.1465,  5588.0601,  5983.5723,  5494.4238,  2760.9097,\n",
      "          14067.7041, 18420.3301,  4992.9595,  9332.5068,  7025.7158,\n",
      "           2036.8564,  4255.3306, 21019.7793,   652.4385, 23799.8613,\n",
      "          17680.7520,  3038.9258],\n",
      "         [24250.0664,  8249.2734,  9763.9512,  5174.7427,  3264.8477,\n",
      "          19955.4102, 24468.1250,  6016.9746, 13241.3906, 11294.0732,\n",
      "           3870.2542,  7039.7236, 27705.6797,  2223.8518, 30532.0781,\n",
      "          23954.5195,  2948.3062],\n",
      "         [ 4442.0425, 10559.1719,  4915.7886, 24206.7031, 20904.7383,\n",
      "           2788.0112,   785.7383, 16984.2617,  4876.1641,  7229.2144,\n",
      "          11768.3291, 10846.9746,  2696.7612, 16345.7354,  3391.5425,\n",
      "           2979.0269, 25335.7402],\n",
      "         [ 9180.2500,  1551.8628,  2876.6479,  8243.1699,  6906.8701,\n",
      "           3321.8364,  4427.2153,  5098.6797,   669.9810,  1579.2388,\n",
      "           2866.7983,  1648.7251,  3941.8179,  5633.4976,  5065.7437,\n",
      "           2538.4624,  8875.7188],\n",
      "         [13424.5635,  4655.7212,  4044.1211,  6019.7549,  3042.4321,\n",
      "          11334.0068, 15170.3047,  4640.0308,  7645.9473,  5599.9678,\n",
      "           1425.4746,  3418.6401, 17876.4688,   585.1367, 20472.9414,\n",
      "          14927.9883,  3787.2021],\n",
      "         [10078.0039,  4912.1558,  5413.9839, 15202.8730, 14599.6865,\n",
      "           2350.3481,  2690.0200,  9733.7617,  2883.9692,  4493.9458,\n",
      "           8402.9199,  5863.2954,  1185.0327, 14045.5098,  1115.0327,\n",
      "           1186.7280, 17700.0254],\n",
      "         [ 4614.8691,  9466.4297,  6081.3359, 23211.9590, 20583.7148,\n",
      "           1308.8616,  2857.7690, 16916.5781,  3829.4175,  3405.7288,\n",
      "          10236.0488,  9506.8340,  1760.5725, 14218.7949,  2449.5022,\n",
      "           1455.1428, 23800.8672],\n",
      "         [10636.3711,  3524.5396,  3660.6919,  9231.1240,  8422.1289,\n",
      "           3553.7231,  6762.6958,  6048.5161,  2868.6392,  1931.6001,\n",
      "           3400.6470,  3829.5308,  7350.7173,  6708.8833,  8191.7485,\n",
      "           6082.3794, 10306.3438],\n",
      "         [ 6037.8628,  3087.7163,  1494.5912, 10232.1533,  7463.3691,\n",
      "           3322.1968,  5410.3555,  6777.5068,  1664.9824,  1067.9584,\n",
      "           1874.4369,  2463.2661,  6375.3003,  3198.3511,  8133.1519,\n",
      "           4599.9155,  9345.3691],\n",
      "         [ 6475.3940,  2547.3862,  1493.2181, 10222.1377,  8149.8359,\n",
      "           2189.7261,  3800.5703,  6419.6689,   919.8281,   970.6166,\n",
      "           2386.9780,  2388.6665,  4345.3237,  4865.9546,  5619.6089,\n",
      "           3010.1558, 10329.3711],\n",
      "         [ 9422.5234,  4395.0610,  3591.7212, 11564.1084, 10619.0703,\n",
      "           2621.8560,  4836.0903,  7543.4497,  2731.4146,  2517.7271,\n",
      "           4813.5278,  4905.5952,  5686.8540,  8841.4395,  6299.9517,\n",
      "           4880.4868, 13093.1230],\n",
      "         [13846.9131,  2009.9438,  4265.8608,  4820.2266,  4052.9712,\n",
      "           6794.5767, 10335.0859,  3334.7944,  3345.9106,  2410.6274,\n",
      "           1493.6166,  1912.1655, 10660.3857,  3260.7085, 12078.0771,\n",
      "           8494.7529,  5003.0596],\n",
      "         [13130.0986,  7456.7974,  5312.4512,  9213.3525,  5104.9067,\n",
      "          13561.8994, 17386.3418,  7664.1343,  9990.9590,  7499.2354,\n",
      "           3098.0039,  5691.1206, 20877.4121,  1130.6016, 23998.4434,\n",
      "          17647.1309,  5865.2295],\n",
      "         [ 7798.4287,  7498.2549,  5251.7495, 19130.2500, 17967.6367,\n",
      "           1260.3585,  3576.0535, 13225.8262,  4240.6357,  3635.8857,\n",
      "           8769.5332,  8221.0234,  2734.9453, 14107.9668,  2802.8516,\n",
      "           2737.9688, 21154.6582],\n",
      "         [ 2846.6553, 26959.3164, 13100.7109, 46109.6953, 38257.2500,\n",
      "          10982.1631, 11399.2725, 36004.5000, 18591.1738, 16598.4492,\n",
      "          23667.2793, 25335.7422, 14905.6797, 25307.6152, 16949.3984,\n",
      "          14942.3906, 43262.5469],\n",
      "         [ 6118.2661,  5516.6909,  3791.6206, 16976.8730, 15021.6572,\n",
      "           1410.5474,  1074.2192, 10918.9180,  2370.2192,  3982.0981,\n",
      "           8029.4937,  5988.6704,   676.1929, 12764.2559,  1094.6147,\n",
      "            597.6694, 18472.0371],\n",
      "         [ 2893.3823,  5511.3433,  1342.1674, 15351.5381, 11571.0684,\n",
      "           2364.5894,  3166.1250, 10350.8643,  2322.6641,  2288.1011,\n",
      "           4335.1382,  4817.6587,  4926.4995,  6032.7710,  6659.6011,\n",
      "           3696.0933, 14353.1416],\n",
      "         [ 3493.1528, 26480.1934, 12532.3027, 43568.4023, 35573.5430,\n",
      "          12009.8008, 12090.5625, 33831.0703, 18844.0273, 17144.6953,\n",
      "          22782.1836, 24752.4902, 17123.7090, 23998.8555, 19368.0918,\n",
      "          16968.3105, 40491.3008],\n",
      "         [19074.5859,  4402.5728,  7850.3931,  5178.9375,  5131.9214,\n",
      "          10517.7812, 15315.5195,  4988.5396,  6184.2427,  4480.3931,\n",
      "           3156.0571,  4370.3784, 15669.9082,  4564.8931, 17170.9238,\n",
      "          13068.7988,  5470.2539],\n",
      "         [  954.1554, 11384.0205,  3851.3979, 24826.7109, 19982.9609,\n",
      "           2626.6475,  4421.4834, 18074.2129,  6218.7061,  4551.3467,\n",
      "           9260.5332, 10500.2930,  6252.4219, 11263.5566,  7921.2109,\n",
      "           5532.3125, 23467.8340],\n",
      "         [21182.8047, 11028.1465,  9771.0371,  9495.5684,  5831.7603,\n",
      "          20709.6562, 25376.9316,  9446.6289, 15178.9570, 12322.3916,\n",
      "           5174.0137,  9081.6260, 29527.0547,  2265.3655, 32948.5430,\n",
      "          25611.1172,  5730.0986],\n",
      "         [ 4110.1572,  7267.8794,  1527.6387, 16404.3906, 12166.0352,\n",
      "           4358.1416,  7008.7285, 10169.9902,  7184.1191,  5160.8096,\n",
      "           5466.1836,  6381.0620,  9467.3145,  7424.2305, 11013.7598,\n",
      "           8511.4160, 15029.6230],\n",
      "         [19630.2949,  2846.3940,  6009.7402,  1968.1357,  1337.3960,\n",
      "          12450.9971, 15971.7949,  1909.5610,  6942.9316,  6221.9307,\n",
      "           1552.6533,  2588.0825, 17553.4043,  2315.0107, 19398.0293,\n",
      "          14778.0879,  1775.9912],\n",
      "         [15493.8242,  3760.9009,  5829.2563,  5837.8340,  5198.9292,\n",
      "           8235.2734, 12314.0391,  4919.9907,  4599.9106,  3214.7427,\n",
      "           2436.9360,  3653.9624, 13181.3730,  3912.5044, 14729.8965,\n",
      "          10812.8105,  5987.0918],\n",
      "         [15484.5166,  1441.5181,  3560.5078,  3439.3223,  3058.4790,\n",
      "           7557.8623, 10594.8145,  1534.9526,  4594.8359,  4193.5342,\n",
      "           1485.0293,  1680.9780, 11556.0098,  4135.4961, 12624.7988,\n",
      "           9775.5664,  4345.5459],\n",
      "         [24400.7305,  5217.3071,  9403.2988,  2288.0737,  1639.9854,\n",
      "          17328.9570, 21879.3828,  3459.8921, 10444.7695,  8952.8018,\n",
      "           3006.8594,  4607.1265, 23436.0742,  2548.2129, 25724.9062,\n",
      "          19963.1348,  1322.7104],\n",
      "         [ 6554.6685,  2355.7124,  2135.0132, 10615.7031,  8580.2197,\n",
      "           2154.9653,  3203.7310,  6505.4277,   825.2290,  1409.3657,\n",
      "           3424.9136,  2277.0708,  2918.9077,  6212.5620,  4066.3452,\n",
      "           1747.6187, 10899.8965],\n",
      "         [22128.5820,  2155.5857,  6626.7271,  1076.4940,  1066.2504,\n",
      "          13569.1846, 15780.3281,   732.8959,  7599.0547,  8316.4385,\n",
      "           2689.8628,  2334.4910, 16925.9688,  4497.4624, 18347.3164,\n",
      "          14554.4990,  1816.1473],\n",
      "         [ 2411.3779,  7673.4893,  2547.8159, 19119.2539, 15491.5322,\n",
      "           1455.6124,  3714.9714, 13529.2637,  3668.7253,  2192.5459,\n",
      "           6144.7280,  7133.3867,  4842.7539,  8469.7637,  6292.2773,\n",
      "           3917.0625, 18460.8711],\n",
      "         [ 4441.8931,  4700.4438,  1301.1088, 12278.1328,  8730.3438,\n",
      "           3428.6821,  5410.1333,  8056.5405,  3034.9282,  2086.7153,\n",
      "           2994.6968,  3895.9312,  7545.6475,  4180.9272,  9434.6240,\n",
      "           5944.1494, 11027.4639],\n",
      "         [ 8246.8828,  2595.7881,  2675.0527, 10181.9980,  8530.3867,\n",
      "           3123.4534,  2488.7356,  6536.8154,    70.2043,  2399.6995,\n",
      "           3830.2852,  2768.6699,  3185.0254,  6859.6680,  4290.5840,\n",
      "           2043.0839, 10985.4922],\n",
      "         [13235.9717,  2000.7896,  3948.7378,  4972.3652,  4039.2876,\n",
      "           6572.3442,  9813.6426,  3426.3315,  3065.9517,  2320.4575,\n",
      "           1415.4916,  1870.9663, 10379.0986,  3101.3843, 11859.5049,\n",
      "           8223.3721,  5088.2021],\n",
      "         [ 6337.7881,  7602.7334,  4074.4937, 14879.0645, 10857.4326,\n",
      "           6447.7393, 10385.3486, 11862.1621,  5538.6768,  2725.2822,\n",
      "           3896.7026,  6213.1270, 11946.7051,  3354.3159, 14507.0410,\n",
      "           9502.8379, 12361.9746],\n",
      "         [ 7066.8223, 21392.3750, 13815.4297, 41244.7266, 37645.5781,\n",
      "           5690.0098,  4808.9956, 31294.8203, 12451.9375, 13375.3652,\n",
      "          23618.5098, 21889.6191,  4253.8770, 29965.7715,  4154.6973,\n",
      "           5620.2598, 42885.4297],\n",
      "         [15864.7949,  3459.0825,  5941.7427,  6323.4854,  6378.0776,\n",
      "           7027.2290, 10578.1426,  4895.4829,  3996.7681,  3284.8149,\n",
      "           3280.8188,  3850.3784, 10992.9531,  6050.9712, 12002.5215,\n",
      "           9145.0508,  7539.4707],\n",
      "         [ 4625.7925,  3528.0015,  1184.8724, 12292.4189,  9534.7422,\n",
      "           1786.3705,  3551.0820,  7813.4053,  1503.5801,  1082.4154,\n",
      "           2933.9214,  3124.0786,  4291.9019,  5172.6714,  5684.7065,\n",
      "           3034.1929, 11897.0430],\n",
      "         [22778.0156,  6900.0283,  9241.6602,  4561.9268,  2106.0647,\n",
      "          19660.1445, 23806.6230,  5017.7271, 13064.0859, 11235.6807,\n",
      "           3884.1196,  5480.0996, 26156.9023,  1935.4945, 29130.4727,\n",
      "          22405.7891,  1899.3651]]], device='cuda:0')\n",
      "torch.Size([1, 50, 17])\n",
      "Z1 shape in batch:  torch.Size([1, 17, 1025])\n",
      "Z2 shape in batch:  torch.Size([1, 50, 1025])\n",
      "1 17 1024\n",
      "torch.Size([1, 17, 100])\n",
      "1 50 1024\n",
      "torch.Size([1, 50, 100])\n",
      "torch.Size([1, 17, 50])\n",
      "torch.Size([1, 17, 50])\n",
      "Gia tri M: tensor([[[230, 233, 227, 228, 228, 229, 225, 228, 227, 225, 227, 229, 233, 226,\n",
      "          227, 232, 224, 226, 224, 230, 228, 232, 232, 228, 233, 226, 229, 229,\n",
      "          224, 232, 228, 228, 229, 227, 226, 226, 228, 226, 226, 224, 231, 229,\n",
      "          233, 225, 233, 229, 230, 228, 232, 231],\n",
      "         [214, 217, 211, 212, 212, 213, 209, 212, 211, 209, 211, 213, 217, 210,\n",
      "          211, 216, 208, 210, 208, 214, 212, 216, 216, 212, 217, 210, 213, 213,\n",
      "          208, 216, 212, 212, 213, 211, 210, 210, 212, 210, 210, 208, 215, 213,\n",
      "          217, 209, 217, 213, 214, 212, 216, 215],\n",
      "         [246, 249, 243, 244, 244, 245, 241, 244, 243, 241, 243, 245, 249, 242,\n",
      "          243, 248, 240, 242, 240, 246, 244, 248, 248, 244, 249, 242, 245, 245,\n",
      "          240, 248, 244, 244, 245, 243, 242, 242, 244, 242, 242, 240, 247, 245,\n",
      "          249, 241, 249, 245, 246, 244, 248, 247],\n",
      "         [198, 201, 195, 196, 196, 197, 193, 196, 195, 193, 195, 197, 201, 194,\n",
      "          195, 200, 192, 194, 192, 198, 196, 200, 200, 196, 201, 194, 197, 197,\n",
      "          192, 200, 196, 196, 197, 195, 194, 194, 196, 194, 194, 192, 199, 197,\n",
      "          201, 193, 201, 197, 198, 196, 200, 199],\n",
      "         [198, 201, 195, 196, 196, 197, 193, 196, 195, 193, 195, 197, 201, 194,\n",
      "          195, 200, 192, 194, 192, 198, 196, 200, 200, 196, 201, 194, 197, 197,\n",
      "          192, 200, 196, 196, 197, 195, 194, 194, 196, 194, 194, 192, 199, 197,\n",
      "          201, 193, 201, 197, 198, 196, 200, 199],\n",
      "         [230, 233, 227, 228, 228, 229, 225, 228, 227, 225, 227, 229, 233, 226,\n",
      "          227, 232, 224, 226, 224, 230, 228, 232, 232, 228, 233, 226, 229, 229,\n",
      "          224, 232, 228, 228, 229, 227, 226, 226, 228, 226, 226, 224, 231, 229,\n",
      "          233, 225, 233, 229, 230, 228, 232, 231],\n",
      "         [182, 185, 179, 180, 180, 181, 177, 180, 179, 177, 179, 181, 185, 178,\n",
      "          179, 184, 176, 178, 176, 182, 180, 184, 184, 180, 185, 178, 181, 181,\n",
      "          176, 184, 180, 180, 181, 179, 178, 178, 180, 178, 178, 176, 183, 181,\n",
      "          185, 177, 185, 181, 182, 180, 184, 183],\n",
      "         [198, 201, 195, 196, 196, 197, 193, 196, 195, 193, 195, 197, 201, 194,\n",
      "          195, 200, 192, 194, 192, 198, 196, 200, 200, 196, 201, 194, 197, 197,\n",
      "          192, 200, 196, 196, 197, 195, 194, 194, 196, 194, 194, 192, 199, 197,\n",
      "          201, 193, 201, 197, 198, 196, 200, 199],\n",
      "         [182, 185, 179, 180, 180, 181, 177, 180, 179, 177, 179, 181, 185, 178,\n",
      "          179, 184, 176, 178, 176, 182, 180, 184, 184, 180, 185, 178, 181, 181,\n",
      "          176, 184, 180, 180, 181, 179, 178, 178, 180, 178, 178, 176, 183, 181,\n",
      "          185, 177, 185, 181, 182, 180, 184, 183],\n",
      "         [230, 233, 227, 228, 228, 229, 225, 228, 227, 225, 227, 229, 233, 226,\n",
      "          227, 232, 224, 226, 224, 230, 228, 232, 232, 228, 233, 226, 229, 229,\n",
      "          224, 232, 228, 228, 229, 227, 226, 226, 228, 226, 226, 224, 231, 229,\n",
      "          233, 225, 233, 229, 230, 228, 232, 231],\n",
      "         [246, 249, 243, 244, 244, 245, 241, 244, 243, 241, 243, 245, 249, 242,\n",
      "          243, 248, 240, 242, 240, 246, 244, 248, 248, 244, 249, 242, 245, 245,\n",
      "          240, 248, 244, 244, 245, 243, 242, 242, 244, 242, 242, 240, 247, 245,\n",
      "          249, 241, 249, 245, 246, 244, 248, 247],\n",
      "         [214, 217, 211, 212, 212, 213, 209, 212, 211, 209, 211, 213, 217, 210,\n",
      "          211, 216, 208, 210, 208, 214, 212, 216, 216, 212, 217, 210, 213, 213,\n",
      "          208, 216, 212, 212, 213, 211, 210, 210, 212, 210, 210, 208, 215, 213,\n",
      "          217, 209, 217, 213, 214, 212, 216, 215],\n",
      "         [166, 169, 163, 164, 164, 165, 161, 164, 163, 161, 163, 165, 169, 162,\n",
      "          163, 168, 160, 162, 160, 166, 164, 168, 168, 164, 169, 162, 165, 165,\n",
      "          160, 168, 164, 164, 165, 163, 162, 162, 164, 162, 162, 160, 167, 165,\n",
      "          169, 161, 169, 165, 166, 164, 168, 167],\n",
      "         [246, 249, 243, 244, 244, 245, 241, 244, 243, 241, 243, 245, 249, 242,\n",
      "          243, 248, 240, 242, 240, 246, 244, 248, 248, 244, 249, 242, 245, 245,\n",
      "          240, 248, 244, 244, 245, 243, 242, 242, 244, 242, 242, 240, 247, 245,\n",
      "          249, 241, 249, 245, 246, 244, 248, 247],\n",
      "         [166, 169, 163, 164, 164, 165, 161, 164, 163, 161, 163, 165, 169, 162,\n",
      "          163, 168, 160, 162, 160, 166, 164, 168, 168, 164, 169, 162, 165, 165,\n",
      "          160, 168, 164, 164, 165, 163, 162, 162, 164, 162, 162, 160, 167, 165,\n",
      "          169, 161, 169, 165, 166, 164, 168, 167],\n",
      "         [166, 169, 163, 164, 164, 165, 161, 164, 163, 161, 163, 165, 169, 162,\n",
      "          163, 168, 160, 162, 160, 166, 164, 168, 168, 164, 169, 162, 165, 165,\n",
      "          160, 168, 164, 164, 165, 163, 162, 162, 164, 162, 162, 160, 167, 165,\n",
      "          169, 161, 169, 165, 166, 164, 168, 167],\n",
      "         [198, 201, 195, 196, 196, 197, 193, 196, 195, 193, 195, 197, 201, 194,\n",
      "          195, 200, 192, 194, 192, 198, 196, 200, 200, 196, 201, 194, 197, 197,\n",
      "          192, 200, 196, 196, 197, 195, 194, 194, 196, 194, 194, 192, 199, 197,\n",
      "          201, 193, 201, 197, 198, 196, 200, 199]]], device='cuda:0')\n",
      "torch.Size([1, 17, 50])\n",
      "torch.Size([1, 17, 50])\n",
      "gia tri D: tensor([[[ 2584.6584, 12525.3613,  9937.3975,  1771.5786,  2453.3091,\n",
      "           1755.0695,  6191.5508, 17351.4395, 28327.1328,  9278.1426,\n",
      "          19492.3359,  2584.8682,  2556.3853, 16640.1465, 24250.0664,\n",
      "           4442.0425,  9180.2500, 13424.5615, 10078.0039,  4614.8691,\n",
      "          10636.3691,  6037.8608,  6475.3940,  9422.5234, 13846.9131,\n",
      "          13130.0986,  7798.4287,  2846.6514,  6118.2661,  2893.3823,\n",
      "           3493.1528, 19074.5859,   954.1554, 21182.8047,  4110.1572,\n",
      "          19630.2949, 15493.8242, 15484.5186, 24400.7305,  6554.6704,\n",
      "          22128.5820,  2411.3779,  4441.8950,  8246.8828, 13235.9736,\n",
      "           6337.7861,  7066.8223, 15864.7949,  4625.7944, 22778.0156],\n",
      "         [15209.9395,  1772.0425,  6097.2256, 17763.0684, 22546.2363,\n",
      "           9009.6211,  7430.3574,  6924.3169, 16634.0625,  2673.0000,\n",
      "           8656.9668,  7551.0840, 17583.6016,  5588.0601,  8249.2734,\n",
      "          10559.1699,  1551.8618,  4655.7212,  4912.1548,  9466.4277,\n",
      "           3524.5386,  3087.7153,  2547.3853,  4395.0601,  2009.9438,\n",
      "           7456.7974,  7498.2539, 26959.3164,  5516.6899,  5511.3423,\n",
      "          26480.1934,  4402.5728, 11384.0195, 11028.1465,  7267.8784,\n",
      "           2846.3940,  3760.9009,  1441.5181,  5217.3076,  2355.7114,\n",
      "           2155.5857,  7673.4883,  4700.4429,  2595.7871,  2000.7896,\n",
      "           7602.7324, 21392.3730,  3459.0825,  3528.0005,  6900.0283],\n",
      "         [ 8176.5547,  3562.7817,  2583.4045,  6989.6353, 10016.5527,\n",
      "           2967.5854,  4428.4941,  8170.5981, 16003.4795,  3182.1680,\n",
      "           7783.8047,  2526.7671,  7702.0679,  5983.5732,  9763.9512,\n",
      "           4915.7886,  2876.6479,  4044.1211,  5413.9839,  6081.3359,\n",
      "           3660.6919,  1494.5912,  1493.2181,  3591.7212,  4265.8608,\n",
      "           5312.4512,  5251.7495, 13100.7109,  3791.6206,  1342.1674,\n",
      "          12532.3027,  7850.3931,  3851.3979,  9771.0371,  1527.6387,\n",
      "           6009.7393,  5829.2563,  3560.5078,  9403.2988,  2135.0132,\n",
      "           6626.7261,  2547.8159,  1301.1088,  2675.0527,  3948.7378,\n",
      "           4074.4937, 13815.4297,  5941.7427,  1184.8724,  9241.6602],\n",
      "         [32161.0352,  5578.2056, 13423.9473, 31628.3164, 39045.8086,\n",
      "          21385.2734, 19306.7422,  7432.8252, 13159.0098, 10007.0967,\n",
      "           7562.4551, 18224.3047, 33906.6602,  5494.4238,  5174.7427,\n",
      "          24206.7031,  8243.1699,  6019.7554, 15202.8730, 23211.9590,\n",
      "           9231.1230, 10232.1533, 10222.1377, 11564.1074,  4820.2261,\n",
      "           9213.3535, 19130.2500, 46109.6953, 16976.8730, 15351.5381,\n",
      "          43568.4023,  5178.9380, 24826.7109,  9495.5684, 16404.3906,\n",
      "           1968.1357,  5837.8345,  3439.3218,  2288.0737, 10615.7031,\n",
      "           1076.4935, 19119.2539, 12278.1328, 10181.9971,  4972.3647,\n",
      "          14879.0645, 41244.7266,  6323.4849, 12292.4189,  4561.9268],\n",
      "         [27834.1055,  4860.2495, 11515.6348, 25033.7852, 31991.3867,\n",
      "          17372.6621, 16882.5078,  5435.7241,  9145.0977,  8654.5850,\n",
      "           4417.4458, 14314.1094, 28375.3848,  2760.9097,  3264.8477,\n",
      "          20904.7383,  6906.8711,  3042.4321, 14599.6875, 20583.7148,\n",
      "           8422.1270,  7463.3701,  8149.8369, 10619.0703,  4052.9712,\n",
      "           5104.9067, 17967.6387, 38257.2500, 15021.6562, 11571.0674,\n",
      "          35573.5430,  5131.9214, 19982.9629,  5831.7603, 12166.0371,\n",
      "           1337.3960,  5198.9292,  3058.4790,  1639.9854,  8580.2207,\n",
      "           1066.2504, 15491.5332,  8730.3438,  8530.3877,  4039.2876,\n",
      "          10857.4336, 37645.5781,  6378.0776,  9534.7412,  2106.0647],\n",
      "         [ 2990.8381,  5341.5210,  5100.4990,  7341.6567,  8873.9180,\n",
      "           1623.4757,  2648.0979, 13203.8340, 28666.1406,  3530.0295,\n",
      "          17845.0508,  2257.7432,  5473.9399, 14067.7051, 19955.4102,\n",
      "           2788.0112,  3321.8364, 11334.0068,  2350.3481,  1308.8616,\n",
      "           3553.7212,  3322.1968,  2189.7261,  2621.8560,  6794.5767,\n",
      "          13561.8994,  1260.3585, 10982.1631,  1410.5474,  2364.5894,\n",
      "          12009.8008, 10517.7812,  2626.6475, 20709.6562,  4358.1416,\n",
      "          12450.9980,  8235.2734,  7557.8623, 17328.9570,  2154.9673,\n",
      "          13569.1836,  1455.6124,  3428.6841,  3123.4534,  6572.3442,\n",
      "           6447.7393,  5690.0098,  7027.2290,  1786.3724, 19660.1445],\n",
      "         [ 3083.6206,  8240.2461,  6494.7739,  7880.9888,  9180.0156,\n",
      "           3562.0535,    83.2082, 18136.5137, 34775.5703,  2563.5422,\n",
      "          22170.5332,  5283.0322,  5841.7661, 18420.3320, 24468.1250,\n",
      "            785.7383,  4427.2153, 15170.3047,  2690.0200,  2857.7690,\n",
      "           6762.6938,  5410.3535,  3800.5703,  4836.0903, 10335.0859,\n",
      "          17386.3418,  3576.0535, 11399.2725,  1074.2192,  3166.1250,\n",
      "          12090.5625, 15315.5195,  4421.4834, 25376.9316,  7008.7285,\n",
      "          15971.7959, 12314.0391, 10594.8164, 21879.3828,  3203.7329,\n",
      "          15780.3271,  3714.9714,  5410.1313,  2488.7356,  9813.6426,\n",
      "          10385.3486,  4808.9956, 10578.1426,  3551.0840, 23806.6230],\n",
      "         [24056.8398,  3507.5161,  7806.5405, 23841.1484, 29597.5586,\n",
      "          15251.4629, 13125.0596,  7550.5845, 14250.3691,  6539.6865,\n",
      "           7224.8218, 13077.9609, 24295.5762,  4992.9595,  6016.9746,\n",
      "          16984.2617,  5098.6797,  4640.0308,  9733.7617, 16916.5781,\n",
      "           6048.5161,  6777.5068,  6419.6689,  7543.4497,  3334.7944,\n",
      "           7664.1343, 13225.8262, 36004.5000, 10918.9180, 10350.8643,\n",
      "          33831.0703,  4988.5396, 18074.2129,  9446.6289, 10169.9902,\n",
      "           1909.5610,  4919.9907,  1534.9526,  3459.8926,  6505.4277,\n",
      "            732.8959, 13529.2637,  8056.5405,  6536.8154,  3426.3315,\n",
      "          11862.1621, 31294.8203,  4895.4829,  7813.4053,  5017.7271],\n",
      "         [ 7364.0854,  2486.8315,  6582.7778, 11652.6328, 15484.5742,\n",
      "           4458.0576,  2422.9231,  8252.7480, 22031.7188,   105.8312,\n",
      "          12652.4551,  4112.8213, 12118.2383,  9332.5068, 13241.3916,\n",
      "           4876.1641,   669.9810,  7645.9473,  2883.9692,  3829.4175,\n",
      "           2868.6392,  1664.9824,   919.8281,  2731.4146,  3345.9106,\n",
      "           9990.9590,  4240.6357, 18591.1738,  2370.2192,  2322.6641,\n",
      "          18844.0273,  6184.2427,  6218.7061, 15178.9570,  7184.1191,\n",
      "           6942.9326,  4599.9106,  4594.8359, 10444.7705,   825.2290,\n",
      "           7599.0537,  3668.7253,  3034.9282,    70.2043,  3065.9517,\n",
      "           5538.6768, 12451.9375,  3996.7681,  1503.5801, 13064.0859],\n",
      "         [ 7356.2441,  2066.5132,  6657.1553, 10305.6367, 13984.1641,\n",
      "           3053.7490,  5861.3047,  5709.6255, 17906.5508,  2859.0452,\n",
      "          10263.2861,  1908.1144, 11228.1084,  7025.7168, 11294.0742,\n",
      "           7229.2144,  1579.2388,  5599.9678,  4493.9458,  3405.7288,\n",
      "           1931.6001,  1067.9584,   970.6166,  2517.7271,  2410.6274,\n",
      "           7499.2354,  3635.8857, 16598.4492,  3982.0981,  2288.1011,\n",
      "          17144.6953,  4480.3931,  4551.3467, 12322.3916,  5160.8096,\n",
      "           6221.9316,  3214.7427,  4193.5342,  8952.8027,  1409.3657,\n",
      "           8316.4395,  2192.5459,  2086.7153,  2399.6995,  2320.4575,\n",
      "           2725.2822, 13375.3652,  3284.8149,  1082.4154, 11235.6807],\n",
      "         [15455.3633,  1775.9574,  5832.9492, 14458.3262, 19654.3145,\n",
      "           7427.9702,  9633.5312,  3385.2466,  9442.2881,  4226.2363,\n",
      "           3598.5706,  5290.5073, 16913.6035,  2036.8564,  3870.2546,\n",
      "          11768.3311,  2866.7983,  1425.4746,  8402.9199, 10236.0488,\n",
      "           3400.6470,  1874.4369,  2386.9780,  4813.5278,  1493.6166,\n",
      "           3098.0039,  8769.5332, 23667.2773,  8029.4937,  4335.1382,\n",
      "          22782.1816,  3156.0571,  9260.5352,  5174.0137,  5466.1836,\n",
      "           1552.6533,  2436.9360,  1485.0293,  3006.8599,  3424.9136,\n",
      "           2689.8628,  6144.7280,  2994.6968,  3830.2852,  1415.4916,\n",
      "           3896.7026, 23618.5117,  3280.8188,  2933.9214,  3884.1196],\n",
      "         [14908.4277,  1882.0630,  6387.5264, 16338.4395, 21204.5449,\n",
      "           8413.3594,  7903.8496,  5859.3101, 14131.7715,  3007.7227,\n",
      "           7060.5127,  6695.3535, 16919.1562,  4255.3306,  7039.7241,\n",
      "          10846.9746,  1648.7251,  3418.6401,  5863.2954,  9506.8340,\n",
      "           3829.5308,  2463.2661,  2388.6665,  4905.5952,  1912.1655,\n",
      "           5691.1206,  8221.0234, 25335.7422,  5988.6704,  4817.6587,\n",
      "          24752.4902,  4370.3784, 10500.2930,  9081.6260,  6381.0620,\n",
      "           2588.0825,  3653.9624,  1680.9780,  4607.1270,  2277.0708,\n",
      "           2334.4910,  7133.3867,  3895.9312,  2768.6699,  1870.9663,\n",
      "           6213.1270, 21889.6191,  3850.3784,  3124.0786,  5480.0996],\n",
      "         [ 3276.7131,  8617.3652,  8869.7158, 12358.1348, 13513.6465,\n",
      "           4930.1172,  1341.9628, 19986.4766, 38924.5352,  3254.9531,\n",
      "          25990.7148,  6424.0566,  8273.4912, 21019.7812, 27705.6797,\n",
      "           2696.7612,  3941.8179, 17876.4688,  1185.0327,  1760.5725,\n",
      "           7350.7153,  6375.2983,  4345.3237,  5686.8540, 10660.3857,\n",
      "          20877.4121,  2734.9453, 14905.6797,   676.1929,  4926.4995,\n",
      "          17123.7090, 15669.9082,  6252.4219, 29527.0547,  9467.3145,\n",
      "          17553.4062, 13181.3730, 11556.0078, 23436.0742,  2918.9097,\n",
      "          16925.9668,  4842.7539,  7545.6455,  3185.0254, 10379.0986,\n",
      "          11946.7051,  4253.8770, 10992.9531,  4291.9038, 26156.9023],\n",
      "         [19288.6289,  4197.7856, 10076.7148, 15556.6074, 21646.5566,\n",
      "           9827.7012, 14238.0234,  2328.4048,  4937.8232,  7505.7109,\n",
      "           1572.1506,  6793.0151, 20606.5488,   652.4385,  2223.8523,\n",
      "          16345.7354,  5633.4976,   585.1367, 14045.5098, 14218.7949,\n",
      "           6708.8833,  3198.3511,  4865.9546,  8841.4395,  3260.7085,\n",
      "           1130.6016, 14107.9668, 25307.6133, 12764.2559,  6032.7710,\n",
      "          23998.8535,  4564.8931, 11263.5566,  2265.3655,  7424.2305,\n",
      "           2315.0107,  3912.5044,  4135.4961,  2548.2134,  6212.5620,\n",
      "           4497.4624,  8469.7637,  4180.9272,  6859.6680,  3101.3843,\n",
      "           3354.3159, 29965.7695,  6050.9712,  5172.6714,  1935.4945],\n",
      "         [ 4202.7832,  9801.7012,  9423.3174, 14562.9629, 15374.8418,\n",
      "           6386.6875,  1950.0800, 22602.2266, 42932.8984,  4266.2812,\n",
      "          29041.7578,  8178.5723,  9296.2100, 23799.8613, 30532.0781,\n",
      "           3391.5425,  5065.7437, 20472.9414,  1115.0327,  2449.5022,\n",
      "           8191.7466,  8133.1499,  5619.6089,  6299.9517, 12078.0771,\n",
      "          23998.4434,  2802.8516, 16949.3984,  1094.6147,  6659.6011,\n",
      "          19368.0918, 17170.9238,  7921.2109, 32948.5430, 11013.7598,\n",
      "          19398.0312, 14729.8965, 12624.8008, 25724.9062,  4066.3433,\n",
      "          18347.3145,  6292.2773,  9434.6221,  4290.5840, 11859.5049,\n",
      "          14507.0410,  4154.6973, 12002.5234,  5684.7046, 29130.4727],\n",
      "         [ 3438.3303,  6814.3887,  8486.5830, 11690.4238, 13498.5605,\n",
      "           4171.5156,  1357.0956, 16602.1641, 34198.4219,  2164.6211,\n",
      "          22414.8711,  5112.5215,  8731.8818, 17680.7539, 23954.5195,\n",
      "           2979.0269,  2538.4624, 14927.9883,  1186.7280,  1455.1428,\n",
      "           6082.3794,  4599.9155,  3010.1558,  4880.4868,  8494.7529,\n",
      "          17647.1309,  2737.9688, 14942.3906,   597.6694,  3696.0933,\n",
      "          16968.3105, 13068.7988,  5532.3125, 25611.1172,  8511.4160,\n",
      "          14778.0898, 10812.8105,  9775.5664, 19963.1348,  1747.6206,\n",
      "          14554.4980,  3917.0625,  5944.1494,  2043.0839,  8223.3721,\n",
      "           9502.8379,  5620.2598,  9145.0508,  3034.1929, 22405.7891],\n",
      "         [32066.8223,  6214.1685, 14525.5605, 29146.6094, 36779.7773,\n",
      "          20620.5156, 20804.8828,  5529.1958,  8191.4414, 11148.3975,\n",
      "           4544.8433, 16888.0996, 33080.7031,  3038.9263,  2948.3062,\n",
      "          25335.7402,  8875.7188,  3787.2026, 17700.0254, 23800.8672,\n",
      "          10306.3438,  9345.3701, 10329.3721, 13093.1230,  5003.0601,\n",
      "           5865.2300, 21154.6582, 43262.5469, 18472.0371, 14353.1416,\n",
      "          40491.3008,  5470.2539, 23467.8340,  5730.0991, 15029.6230,\n",
      "           1775.9907,  5987.0923,  4345.5464,  1322.7104, 10899.8965,\n",
      "           1816.1478, 18460.8711, 11027.4648, 10985.4932,  5088.2026,\n",
      "          12361.9746, 42885.4297,  7539.4712, 11897.0439,  1899.3651]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 17, 50])\n"
     ]
    }
   ],
   "source": [
    "F_i, G_j = loss(Z1, Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "84fd1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    " = [F_i, G_j]\n",
    "dual_sol = \n",
    "for i in range(len(dual_sol)):\n",
    "    dual_sol[i] = dual_sol[i].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ddbc1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lava import train_with_corrupt_flag, get_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b6ce14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = get_indices(loaders['train'])\n",
    "trained_with_flag = train_with_corrupt_flag(loaders['train'], shuffle_ind, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "42a4eb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspected: 10, found: 6 detection rate: 0.40 baseline: 1.8\n",
      "inspected: 20, found: 8 detection rate: 0.53 baseline: 3.6\n",
      "inspected: 30, found: 11 detection rate: 0.73 baseline: 5.4\n",
      "inspected: 40, found: 14 detection rate: 0.93 baseline: 7.2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of poisoned images:  15  out of 10000.\n",
      "last index of poison 47\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "calibrated_gradient = lava.compute_values_and_visualize(dual_sol, trained_with_flag, training_size, portion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
